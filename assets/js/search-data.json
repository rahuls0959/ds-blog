{
  
    
        "post0": {
            "title": "Recommendation Systems - Content Based Filtering (Part 2)",
            "content": "Content Based Filtering . This recommendation systems works by finding similarities between the items. If a user has liked or wishlisted some items in the past, this would try to find similar items and recommend to the user. . Content-based filtering is also used in Google PageRank algorithm to recommend the relevant webpages basis search keyworks. This is used along with citation model (reference of webpage in other webpages) and behavioral model (the activity on the webpage) to arrive at the final results. . We see this type of recommendation in work while searching for items in various apps. In Netflix, we can see some sort of weightage to content based filtering in the section &#39;Because you watched xxx&#39; (Read the article - https://qz.com/1059434/netflix-finally-explains-how-its-because-you-watched-recommendation-tool-works/) . . Loading the Data . Let&#39;s load our CSV file which we have saved in the Part-1 of the blog! . import pandas as pd import numpy as np import warnings warnings.filterwarnings(&quot;ignore&quot;) data=pd.read_csv(&#39;movies_database.csv&#39;) . Let&#39;s look at first few rows of our data! . data = data[[&#39;movie_title&#39;,&#39;overview&#39;,&#39;cast&#39;,&#39;genres&#39;,&#39;keywords&#39;,&#39;director&#39;]] data.head() . movie_title overview cast genres keywords director . 0 | Avatar | In the 22nd century, a paraplegic Marine is di... | [&#39;Sam Worthington&#39;, &#39;Zoe Saldana&#39;, &#39;Sigourney ... | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Fantasy&#39;] | [&#39;culture clash&#39;, &#39;future&#39;, &#39;space war&#39;] | James Cameron | . 1 | Pirates of the Caribbean: At World&#39;s End | Captain Barbossa, long believed to be dead, ha... | [&#39;Johnny Depp&#39;, &#39;Orlando Bloom&#39;, &#39;Keira Knight... | [&#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Action&#39;] | [&#39;ocean&#39;, &#39;drug abuse&#39;, &#39;exotic island&#39;] | Gore Verbinski | . 2 | Spectre | A cryptic message from Bond’s past sends him o... | [&#39;Daniel Craig&#39;, &#39;Christoph Waltz&#39;, &#39;Léa Seydo... | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Crime&#39;] | [&#39;spy&#39;, &#39;based on novel&#39;, &#39;secret agent&#39;] | Sam Mendes | . 3 | The Dark Knight Rises | Following the death of District Attorney Harve... | [&#39;Christian Bale&#39;, &#39;Michael Caine&#39;, &#39;Gary Oldm... | [&#39;Action&#39;, &#39;Crime&#39;, &#39;Drama&#39;] | [&#39;dc comics&#39;, &#39;crime fighter&#39;, &#39;terrorist&#39;] | Christopher Nolan | . 4 | John Carter | John Carter is a war-weary, former military ca... | [&#39;Taylor Kitsch&#39;, &#39;Lynn Collins&#39;, &#39;Samantha Mo... | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Science Fiction&#39;] | [&#39;based on novel&#39;, &#39;mars&#39;, &#39;medallion&#39;] | Andrew Stanton | . We can find the similarity scores between movies based on various metadata. We will first build a model looking at movie plot summaries given in the &#39;overview&#39; column and then refine our recommendations by including actor, director, genre, etc. . Plot Description based Recommendation . Creating a TF-IDF Vectorizer . We first need to convert each overview into its word vector. Next, we will have the find the Term Frequency - Inverse Document Frequency (TF-IDF) vector for each overview. . The TF-IDF algorithm is used to weight a word in each document and assign the importance of the word based on the following two factors: . Term Frequency (TF): The number of times the word appears in the document (in our case, the movie plot description) | Inverse Document Frequency (IDF): The number of times the word appears in the corpus, representing how significant the term is in the whole corpus (in our case, corpus of movie plot descriptions) | . The below formula is used for TFIDF calculation: . In Python, &#39;scikit-learn&#39; library has a pre-built TF-IDF vectorizer that calculates the TF-IDF score for each document’s description, word-by-word. . Let&#39;s now implement a TFIDF matrix for our data! (Link - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) . #Import TfIdfVectorizer from scikit-learn from sklearn.feature_extraction.text import TfidfVectorizer #Define a TF-IDF Vectorizer Object. Remove all english stop words such as &#39;the&#39;, &#39;a&#39;, &#39;an&#39; tfidf = TfidfVectorizer(stop_words=&#39;english&#39;) #Replace NaN with an empty string data[&#39;overview&#39;] = data[&#39;overview&#39;].fillna(&#39;&#39;) #Construct the required TF-IDF matrix by fitting and transforming the data tfidf_matrix = tfidf.fit_transform(data[&#39;overview&#39;]) #Output the shape of tfidf_matrix tfidf_matrix.shape . (4803, 20978) . We can see that over 20000 unique words are used to describe 4803 movies in our dataset. Let&#39;s see how the TFIDF matrix looks like! . #Convert TFIDF matrix to Pandas Dataframe if you want to see the word frequencies. doc_term_matrix = tfidf_matrix.todense() df = pd.DataFrame(doc_term_matrix, columns=tfidf.get_feature_names(), index=data.overview) df.to_csv(&#39;movies_database_tfidf.csv&#39;, index=True) . df.head() . 00 000 007 07am 10 100 1000 101 108 10th ... zuckerberg zula zuzu zyklon æon éloigne émigré été única über . overview . In the 22nd century, a paraplegic Marine is dispatched to the moon Pandora on a unique mission, but becomes torn between following orders and protecting an alien civilization. | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . Captain Barbossa, long believed to be dead, has come back to life and is headed to the edge of the Earth with Will Turner and Elizabeth Swann. But nothing is quite as it seems. | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . A cryptic message from Bond’s past sends him on a trail to uncover a sinister organization. While M battles political forces to keep the secret service alive, Bond peels back the layers of deceit to reveal the terrible truth behind SPECTRE. | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . Following the death of District Attorney Harvey Dent, Batman assumes responsibility for Dent&#39;s crimes to protect the late attorney&#39;s reputation and is subsequently hunted by the Gotham City Police Department. Eight years later, Batman encounters the mysterious Selina Kyle and the villainous Bane, a new terrorist leader who overwhelms Gotham&#39;s finest. The Dark Knight resurfaces to protect a city that has branded him an enemy. | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . John Carter is a war-weary, former military captain who&#39;s inexplicably transported to the mysterious and exotic planet of Barsoom (Mars) and reluctantly becomes embroiled in an epic conflict. It&#39;s a world on the brink of collapse, and Carter rediscovers his humanity when he realizes the survival of Barsoom and its people rests in his hands. | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 rows × 20978 columns . Computing Similarity Score using Cosine Similarity . With this matrix in hand, we can now compute a similarity score. We will be using the cosine similarity to calculate a numeric quantity that denotes the similarity between two movies. . Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together. The smaller the angle, higher the cosine similarity. . . Reference - https://www.machinelearningplus.com/nlp/cosine-similarity/ . # Compute Cosine Similarity from sklearn.metrics.pairwise import cosine_similarity cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix) print(cosine_sim) . [[1. 0. 0. ... 0. 0. 0. ] [0. 1. 0. ... 0.02160533 0. 0. ] [0. 0. 1. ... 0.01488159 0. 0. ] ... [0. 0.02160533 0.01488159 ... 1. 0.01609091 0.00701914] [0. 0. 0. ... 0.01609091 1. 0.01171696] [0. 0. 0. ... 0.00701914 0.01171696 1. ]] . #Let&#39;s create a dataframe of the similarity matrix with rows and columns as movie titles sim = pd.DataFrame(cosine_sim, columns=data.movie_title, index=data.movie_title) sim.head() . movie_title Avatar Pirates of the Caribbean: At World&#39;s End Spectre The Dark Knight Rises John Carter Spider-Man 3 Tangled Avengers: Age of Ultron Harry Potter and the Half-Blood Prince Batman v Superman: Dawn of Justice ... On The Downlow Sanctuary: Quite a Conundrum Bang Primer Cavite El Mariachi Newlyweds Signed, Sealed, Delivered Shanghai Calling My Date with Drew . movie_title . Avatar | 1.000000 | 0.000000 | 0.0 | 0.024995 | 0.000000 | 0.030353 | 0.000000 | 0.037581 | 0.000000 | 0.000000 | ... | 0.000000 | 0.0 | 0.029175 | 0.042176 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | . Pirates of the Caribbean: At World&#39;s End | 0.000000 | 1.000000 | 0.0 | 0.000000 | 0.033369 | 0.000000 | 0.000000 | 0.022676 | 0.000000 | 0.000000 | ... | 0.000000 | 0.0 | 0.006895 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.021605 | 0.000000 | 0.000000 | . Spectre | 0.000000 | 0.000000 | 1.0 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.030949 | 0.024830 | 0.000000 | ... | 0.027695 | 0.0 | 0.000000 | 0.000000 | 0.017768 | 0.0 | 0.0 | 0.014882 | 0.000000 | 0.000000 | . The Dark Knight Rises | 0.024995 | 0.000000 | 0.0 | 1.000000 | 0.010433 | 0.005145 | 0.012601 | 0.026954 | 0.020652 | 0.133740 | ... | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.033864 | 0.042752 | 0.022692 | . John Carter | 0.000000 | 0.033369 | 0.0 | 0.010433 | 1.000000 | 0.000000 | 0.009339 | 0.037407 | 0.000000 | 0.017148 | ... | 0.012730 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.006126 | 0.000000 | 0.000000 | . 5 rows × 4803 columns . We have now computed the similarity score of each movie with all the other movies based on plot description. Please note, similarity of the movie with itself is 1 and this can be seen in the diagonal in the above matrix. . Implementing the Recommendation System . Let&#39;s implement a recommendation system where we can input a movie title and the model returns the top 10 movies similar to the movie. . # Create a column of movie titles indices = pd.Series(data.index, index=data[&#39;movie_title&#39;]).drop_duplicates() print(indices) . movie_title Avatar 0 Pirates of the Caribbean: At World&#39;s End 1 Spectre 2 The Dark Knight Rises 3 John Carter 4 ... El Mariachi 4798 Newlyweds 4799 Signed, Sealed, Delivered 4800 Shanghai Calling 4801 My Date with Drew 4802 Length: 4803, dtype: int64 . # Function that takes in movie title as input and outputs most similar movies def get_recommendations(title, cosine_sim=cosine_sim): # Get the index of the movie that matches the title idx = indices[title] # Get the pairwise similarity scores of all movies with that movie sim_scores = list(enumerate(cosine_sim[idx])) # Sort the movies in descending order of similarity scores sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) # Get the scores of the 10 most similar movies ignoring the first one as it is itself movie sim_scores = sim_scores[1:11] # Get the movie indices movie_indices = [i[0] for i in sim_scores] # Return the top 10 most similar movies return data[&#39;movie_title&#39;].iloc[movie_indices] . get_recommendations(&#39;The Godfather&#39;) . 2731 The Godfather: Part II 1873 Blood Ties 867 The Godfather: Part III 3727 Easy Money 3623 Made 3125 Eulogy 3896 Sinister 4506 The Maid&#39;s Room 3783 Joe 2244 The Cold Light of Day Name: movie_title, dtype: object . We can see the model did a good job in finding the Godfather trilogy movies and other crime movies such as &#39;Blood Ties&#39; . However, it can be further improved by the following: . Including other features such as Director, Genre, etc. : People interested in &#39;The Godfather&#39; may be more interested in movies directed by Francis Ford Coppola. Let&#39;s try to include this information too in our model. | People might be interested in different genres based on the movie watched by other users. We can solve this using collaborative filtering, which will be discussed in Part 3 of this blog. | . Cast, Genres and Keywords Based Recommendation . Now we will build our model based on top 3 actors in the movie, director, top 3 genres of the movie and top 3 keywords of the movie. First, let&#39;s load our dataset. . #Let&#39;s load our data data.head() . movie_title overview cast genres keywords director . 0 | Avatar | In the 22nd century, a paraplegic Marine is di... | [&#39;Sam Worthington&#39;, &#39;Zoe Saldana&#39;, &#39;Sigourney ... | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Fantasy&#39;] | [&#39;culture clash&#39;, &#39;future&#39;, &#39;space war&#39;] | James Cameron | . 1 | Pirates of the Caribbean: At World&#39;s End | Captain Barbossa, long believed to be dead, ha... | [&#39;Johnny Depp&#39;, &#39;Orlando Bloom&#39;, &#39;Keira Knight... | [&#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Action&#39;] | [&#39;ocean&#39;, &#39;drug abuse&#39;, &#39;exotic island&#39;] | Gore Verbinski | . 2 | Spectre | A cryptic message from Bond’s past sends him o... | [&#39;Daniel Craig&#39;, &#39;Christoph Waltz&#39;, &#39;Léa Seydo... | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Crime&#39;] | [&#39;spy&#39;, &#39;based on novel&#39;, &#39;secret agent&#39;] | Sam Mendes | . 3 | The Dark Knight Rises | Following the death of District Attorney Harve... | [&#39;Christian Bale&#39;, &#39;Michael Caine&#39;, &#39;Gary Oldm... | [&#39;Action&#39;, &#39;Crime&#39;, &#39;Drama&#39;] | [&#39;dc comics&#39;, &#39;crime fighter&#39;, &#39;terrorist&#39;] | Christopher Nolan | . 4 | John Carter | John Carter is a war-weary, former military ca... | [&#39;Taylor Kitsch&#39;, &#39;Lynn Collins&#39;, &#39;Samantha Mo... | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Science Fiction&#39;] | [&#39;based on novel&#39;, &#39;mars&#39;, &#39;medallion&#39;] | Andrew Stanton | . Data Cleaning . Firstly, let&#39;s clean our data by converting all the text into lowercase and removing spaces in a single name. Example: Christian Bale would be converted to christianbale . # Function to convert all strings to lower case and strip names of spaces def clean_data(x): if isinstance(x, list): return [str.lower(i.replace(&quot; &quot;, &quot;&quot;)) for i in x] else: #Check if director exists. If not, return empty string if isinstance(x, str): return str.lower(x.replace(&quot; &quot;, &quot;&quot;)) else: return &#39;&#39; . # Apply clean_data function to your features. features = [&#39;cast&#39;, &#39;keywords&#39;, &#39;director&#39;, &#39;genres&#39;] for feature in features: data[feature] = data[feature].apply(clean_data) . Let&#39;s now combine all the feature data into a single string which combines all the metadata (such as actors, director, keywords and genres) to be feed into the count vectorizer. . def create_combined_features(x): return &#39; &#39;.join(x[&#39;keywords&#39;]) + &#39; &#39; + &#39; &#39;.join(x[&#39;cast&#39;]) + &#39; &#39; + x[&#39;director&#39;] + &#39; &#39; + &#39; &#39;.join(x[&#39;genres&#39;]) data[&#39;combined_features&#39;] = data.apply(create_combined_features, axis=1) . Creating a Count Vectorizer . Now, we have a combined features column in our data. We now apply the count vectorizer which creates a word vector of the entire corpus and provides the frequency of the each word in the document. . # Import CountVectorizer and create the count matrix from sklearn.feature_extraction.text import CountVectorizer count = CountVectorizer(stop_words=&#39;english&#39;) count_matrix = count.fit_transform(data[&#39;combined_features&#39;]) count_matrix.shape . (4803, 2469) . We can see that 2469 unique words are used to describe 4803 movies in our dataset. Let&#39;s see how the Count matrix looks like! . #Convert count matrix to Pandas Dataframe if you want to see the word frequencies. doc_term_matrix = count_matrix.todense() df = pd.DataFrame(doc_term_matrix, columns=count.get_feature_names(), index=data.combined_features) df.to_csv(&#39;movies_database_countmatrix.csv&#39;, index=True) . df.head() . aaronhann aaronschneider abelferrara abrams adambrooks adamcarolla adamgoldberg adamgreen adamjayepstein adammarcus ... zackward zakpenn zalbatmanglij zhangyimou zoranlisinac àlexpastor álexdelaiglesia émilegaudreault érictessier étiennefaure . combined_features . [ &#39; c u l t u r e c l a s h &#39; , &#39; f u t u r e &#39; , &#39; s p a c e w a r &#39; ] [ &#39; s a m w o r t h i n g t o n &#39; , &#39; z o e s a l d a n a &#39; , &#39; s i g o u r n e y w e a v e r &#39; ] jamescameron [ &#39; a c t i o n &#39; , &#39; a d v e n t u r e &#39; , &#39; f a n t a s y &#39; ] | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . [ &#39; o c e a n &#39; , &#39; d r u g a b u s e &#39; , &#39; e x o t i c i s l a n d &#39; ] [ &#39; j o h n n y d e p p &#39; , &#39; o r l a n d o b l o o m &#39; , &#39; k e i r a k n i g h t l e y &#39; ] goreverbinski [ &#39; a d v e n t u r e &#39; , &#39; f a n t a s y &#39; , &#39; a c t i o n &#39; ] | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . [ &#39; s p y &#39; , &#39; b a s e d o n n o v e l &#39; , &#39; s e c r e t a g e n t &#39; ] [ &#39; d a n i e l c r a i g &#39; , &#39; c h r i s t o p h w a l t z &#39; , &#39; l é a s e y d o u x &#39; ] sammendes [ &#39; a c t i o n &#39; , &#39; a d v e n t u r e &#39; , &#39; c r i m e &#39; ] | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . [ &#39; d c c o m i c s &#39; , &#39; c r i m e f i g h t e r &#39; , &#39; t e r r o r i s t &#39; ] [ &#39; c h r i s t i a n b a l e &#39; , &#39; m i c h a e l c a i n e &#39; , &#39; g a r y o l d m a n &#39; ] christophernolan [ &#39; a c t i o n &#39; , &#39; c r i m e &#39; , &#39; d r a m a &#39; ] | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . [ &#39; b a s e d o n n o v e l &#39; , &#39; m a r s &#39; , &#39; m e d a l l i o n &#39; ] [ &#39; t a y l o r k i t s c h &#39; , &#39; l y n n c o l l i n s &#39; , &#39; s a m a n t h a m o r t o n &#39; ] andrewstanton [ &#39; a c t i o n &#39; , &#39; a d v e n t u r e &#39; , &#39; s c i e n c e f i c t i o n &#39; ] | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 rows × 2469 columns . Computing Similarity Score using Cosine Similarity . # Compute the Cosine Similarity matrix based on the count_matrix from sklearn.metrics.pairwise import cosine_similarity cosine_sim2 = cosine_similarity(count_matrix, count_matrix) . #Let&#39;s create a dataframe of the similarity matrix with rows and columns as movie titles sim = pd.DataFrame(cosine_sim2, columns=data.movie_title, index=data.movie_title) sim.head() . movie_title Avatar Pirates of the Caribbean: At World&#39;s End Spectre The Dark Knight Rises John Carter Spider-Man 3 Tangled Avengers: Age of Ultron Harry Potter and the Half-Blood Prince Batman v Superman: Dawn of Justice ... On The Downlow Sanctuary: Quite a Conundrum Bang Primer Cavite El Mariachi Newlyweds Signed, Sealed, Delivered Shanghai Calling My Date with Drew . movie_title . Avatar | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . Pirates of the Caribbean: At World&#39;s End | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . Spectre | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . The Dark Knight Rises | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . John Carter | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 rows × 4803 columns . We have now computed the similarity score of each movie with all the other movies based on actors, director, keywords and genres. Please note, similarity of the movie with itself is 1 and this can be seen in the diagonal of the above matrix. . Implementing the Recommendation System . # Reset index of our main DataFrame and construct reverse mapping as before data = data.reset_index() indices = pd.Series(data.index, index=data[&#39;movie_title&#39;]) . get_recommendations(&#39;The Godfather&#39;, cosine_sim2) . 1018 The Cotton Club 1167 Dracula 1209 The Rainmaker 1525 Apocalypse Now 2333 Peggy Sue Got Married 2600 New York Stories 2731 The Godfather: Part II 3012 The Outsiders 3337 The Godfather 3401 Twixt Name: movie_title, dtype: object . We can see the model did a good job in finding movies similar to &#39;The Godfather&#39;. Most of the movies are directed by Francis Ford Coppola. Majority of movies theme is Crime/Thriller. . The recommender can be improved further by adding more features such as production_company such as DC or Marvel, release date, etc. . Endnotes . I hope this has helped to understand the implementation of content-based filtering using the dummy dataset of ~5000 English Movies. Feel free to play around with the code by opening in Colab or cloning the repo in github. . As we see, the content-based method only has to analyze the items and a single user’s profile for the recommendation, which makes the process less cumbersome. Content-based filtering would thus produce more reliable results with fewer users in the system. However, if the content doesn’t contain enough information to discriminate the items precisely, the recommendation itself risks being imprecise. This can be somewhat overcome with Collaborating Filtering which provides recommendations based similarities in the purchase behavior of users. We will discuss this method in last part of this blog series. . If you have any comments or suggestions please comment below or reach out to me at - Twitter or LinkedIn .",
            "url": "https://rahuls0959.github.io/ds-blog/python/recommendation%20system/relevancy/collaborative%20filtering/content-based%20filtering/demographic%20filtering/2020/05/03/_Recommendation-System_Part-2.html",
            "relUrl": "/python/recommendation%20system/relevancy/collaborative%20filtering/content-based%20filtering/demographic%20filtering/2020/05/03/_Recommendation-System_Part-2.html",
            "date": " • May 3, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Recommendation Systems - Demographic Filtering (Part 1)",
            "content": "Introduction . Recommendation systems have become central to most of the popular apps such as Netflix, Spotify, Flipkart, etc. These provide better recommendation to the user based on past search history or keyword searched by the user or similarity of the user to the other users&#39; behavior on the platform. Recommendation systems try to find the relevance of the content or item for the user by proving a scoring mechanism for each user-item pair. The targeted recommendations are used to drive more engagement on the platform which in turn would lead be more retention and revenue opportunity for these apps. . Types of Recommendation Systems . Demographic Filtering: This provides the same recommendation to all the audience based on popularity, rating or genre of the item. We can see this type of recommendations in sections such as &#39;Trending Now&#39;, &#39;Popular on Netflix&#39;, etc. This is one of the most simple form of filtering. This assumes that items which are more popular or critically acclaimed will have higher chances of being liked by the general audience. | . . Content Based Filtering: This suggests similar items based on a particular item. This uses the item&#39;s similarity with the previously liked or saved items of the user based on item&#39;s metadata or current search keywords used by user to find some item. This assumes that a user has higher chances to like similar items based on previous history of the user. Also, this helps in giving relevant recommendations to the user based on searched keywords. | . . Collaborating Filtering: This system matches persons with similar interests and provides recommendations based on this matching. Collaborative filters do not require item metadata like in content-based filtering. This kind of recommendations implementation can be seen in Amazon or Flipkart under &#39;Frequently Bought Together&#39; section. | . . Data Preparation . We have taken the data of ~5000 English Movies from Kaggle (https://www.kaggle.com/tmdb/tmdb-movie-metadata). With this data, we would try to understand each of three types of recommendation systems by developing Machine Learning models in Python. . The first dataset &#39;tmdb_5000_credits&#39; contains the following features: . movie_id - A unique identifier for each movie | cast - The name of lead and supporting actors | crew - The name of Director, Editor, Composer, Writer etc. | . The second dataset &#39;tmdb_5000_movies&#39; has the following features: . budget - The budget in which the movie was made | genre - The genre of the movie (Action, Comedy ,Thriller etc.) | homepage - A link to the homepage of the movie | id - This is infact the movie_id as in the first dataset | keywords - The keywords or tags related to the movie | original_language - The language in which the movie was made | original_title - The title of the movie before translation or adaptation | overview - A brief description of the movie | popularity - A numeric quantity specifying the movie popularity at the moment | production_companies - The production house of the movie | production_countries - The country in which it was produced | release_date - The date on which it was released | revenue - The worldwide revenue generated by the movie | runtime - The running time of the movie in minutes | status - &quot;Released&quot; or &quot;Rumored&quot; | tagline - Movie&#39;s tagline | title - Title of the movie | vote_average - average ratings the movie recieved | vote_count - the count of votes recieved | . import pandas as pd import numpy as np import warnings warnings.filterwarnings(&quot;ignore&quot;) df1=pd.read_csv(&#39;tmdb-movie-metadata/tmdb_5000_credits.csv&#39;) df2=pd.read_csv(&#39;tmdb-movie-metadata/tmdb_5000_movies.csv&#39;) . We can now join the two datasets on &#39;id&#39; column. . df1.columns = [&#39;id&#39;,&#39;movie_title&#39;,&#39;cast&#39;,&#39;crew&#39;] df2= df2.merge(df1,on=&#39;id&#39;) . Let&#39;s see how the combined data looks like! . df2.shape . (4803, 23) . The database contains 4803 movies and 23 features or variables. . df2.head(5) . budget genres homepage id keywords original_language original_title overview popularity production_companies ... runtime spoken_languages status tagline title vote_average vote_count movie_title cast crew . 0 | 237000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | http://www.avatarmovie.com/ | 19995 | [{&quot;id&quot;: 1463, &quot;name&quot;: &quot;culture clash&quot;}, {&quot;id&quot;:... | en | Avatar | In the 22nd century, a paraplegic Marine is di... | 150.437577 | [{&quot;name&quot;: &quot;Ingenious Film Partners&quot;, &quot;id&quot;: 289... | ... | 162.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}, {&quot;iso... | Released | Enter the World of Pandora. | Avatar | 7.2 | 11800 | Avatar | [{&quot;cast_id&quot;: 242, &quot;character&quot;: &quot;Jake Sully&quot;, &quot;... | [{&quot;credit_id&quot;: &quot;52fe48009251416c750aca23&quot;, &quot;de... | . 1 | 300000000 | [{&quot;id&quot;: 12, &quot;name&quot;: &quot;Adventure&quot;}, {&quot;id&quot;: 14, &quot;... | http://disney.go.com/disneypictures/pirates/ | 285 | [{&quot;id&quot;: 270, &quot;name&quot;: &quot;ocean&quot;}, {&quot;id&quot;: 726, &quot;na... | en | Pirates of the Caribbean: At World&#39;s End | Captain Barbossa, long believed to be dead, ha... | 139.082615 | [{&quot;name&quot;: &quot;Walt Disney Pictures&quot;, &quot;id&quot;: 2}, {&quot;... | ... | 169.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}] | Released | At the end of the world, the adventure begins. | Pirates of the Caribbean: At World&#39;s End | 6.9 | 4500 | Pirates of the Caribbean: At World&#39;s End | [{&quot;cast_id&quot;: 4, &quot;character&quot;: &quot;Captain Jack Spa... | [{&quot;credit_id&quot;: &quot;52fe4232c3a36847f800b579&quot;, &quot;de... | . 2 | 245000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | http://www.sonypictures.com/movies/spectre/ | 206647 | [{&quot;id&quot;: 470, &quot;name&quot;: &quot;spy&quot;}, {&quot;id&quot;: 818, &quot;name... | en | Spectre | A cryptic message from Bond’s past sends him o... | 107.376788 | [{&quot;name&quot;: &quot;Columbia Pictures&quot;, &quot;id&quot;: 5}, {&quot;nam... | ... | 148.0 | [{&quot;iso_639_1&quot;: &quot;fr&quot;, &quot;name&quot;: &quot;Fran u00e7ais&quot;},... | Released | A Plan No One Escapes | Spectre | 6.3 | 4466 | Spectre | [{&quot;cast_id&quot;: 1, &quot;character&quot;: &quot;James Bond&quot;, &quot;cr... | [{&quot;credit_id&quot;: &quot;54805967c3a36829b5002c41&quot;, &quot;de... | . 3 | 250000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 80, &quot;nam... | http://www.thedarkknightrises.com/ | 49026 | [{&quot;id&quot;: 849, &quot;name&quot;: &quot;dc comics&quot;}, {&quot;id&quot;: 853,... | en | The Dark Knight Rises | Following the death of District Attorney Harve... | 112.312950 | [{&quot;name&quot;: &quot;Legendary Pictures&quot;, &quot;id&quot;: 923}, {&quot;... | ... | 165.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}] | Released | The Legend Ends | The Dark Knight Rises | 7.6 | 9106 | The Dark Knight Rises | [{&quot;cast_id&quot;: 2, &quot;character&quot;: &quot;Bruce Wayne / Ba... | [{&quot;credit_id&quot;: &quot;52fe4781c3a36847f81398c3&quot;, &quot;de... | . 4 | 260000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | http://movies.disney.com/john-carter | 49529 | [{&quot;id&quot;: 818, &quot;name&quot;: &quot;based on novel&quot;}, {&quot;id&quot;:... | en | John Carter | John Carter is a war-weary, former military ca... | 43.926995 | [{&quot;name&quot;: &quot;Walt Disney Pictures&quot;, &quot;id&quot;: 2}] | ... | 132.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}] | Released | Lost in our world, found in another. | John Carter | 6.1 | 2124 | John Carter | [{&quot;cast_id&quot;: 5, &quot;character&quot;: &quot;John Carter&quot;, &quot;c... | [{&quot;credit_id&quot;: &quot;52fe479ac3a36847f813eaa3&quot;, &quot;de... | . 5 rows × 23 columns . Let&#39;s keep only the most important features from the combined dataset - title, cast, crew, genres, keywords, vote_count, vote_average, popularity . data = df2[[&#39;movie_title&#39;, &#39;cast&#39;,&#39;crew&#39;,&#39;genres&#39;,&#39;keywords&#39;,&#39;overview&#39;,&#39;vote_count&#39;,&#39;vote_average&#39;,&#39;popularity&#39;]] data.head() . movie_title cast crew genres keywords overview vote_count vote_average popularity . 0 | Avatar | [{&quot;cast_id&quot;: 242, &quot;character&quot;: &quot;Jake Sully&quot;, &quot;... | [{&quot;credit_id&quot;: &quot;52fe48009251416c750aca23&quot;, &quot;de... | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | [{&quot;id&quot;: 1463, &quot;name&quot;: &quot;culture clash&quot;}, {&quot;id&quot;:... | In the 22nd century, a paraplegic Marine is di... | 11800 | 7.2 | 150.437577 | . 1 | Pirates of the Caribbean: At World&#39;s End | [{&quot;cast_id&quot;: 4, &quot;character&quot;: &quot;Captain Jack Spa... | [{&quot;credit_id&quot;: &quot;52fe4232c3a36847f800b579&quot;, &quot;de... | [{&quot;id&quot;: 12, &quot;name&quot;: &quot;Adventure&quot;}, {&quot;id&quot;: 14, &quot;... | [{&quot;id&quot;: 270, &quot;name&quot;: &quot;ocean&quot;}, {&quot;id&quot;: 726, &quot;na... | Captain Barbossa, long believed to be dead, ha... | 4500 | 6.9 | 139.082615 | . 2 | Spectre | [{&quot;cast_id&quot;: 1, &quot;character&quot;: &quot;James Bond&quot;, &quot;cr... | [{&quot;credit_id&quot;: &quot;54805967c3a36829b5002c41&quot;, &quot;de... | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | [{&quot;id&quot;: 470, &quot;name&quot;: &quot;spy&quot;}, {&quot;id&quot;: 818, &quot;name... | A cryptic message from Bond’s past sends him o... | 4466 | 6.3 | 107.376788 | . 3 | The Dark Knight Rises | [{&quot;cast_id&quot;: 2, &quot;character&quot;: &quot;Bruce Wayne / Ba... | [{&quot;credit_id&quot;: &quot;52fe4781c3a36847f81398c3&quot;, &quot;de... | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 80, &quot;nam... | [{&quot;id&quot;: 849, &quot;name&quot;: &quot;dc comics&quot;}, {&quot;id&quot;: 853,... | Following the death of District Attorney Harve... | 9106 | 7.6 | 112.312950 | . 4 | John Carter | [{&quot;cast_id&quot;: 5, &quot;character&quot;: &quot;John Carter&quot;, &quot;c... | [{&quot;credit_id&quot;: &quot;52fe479ac3a36847f813eaa3&quot;, &quot;de... | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | [{&quot;id&quot;: 818, &quot;name&quot;: &quot;based on novel&quot;}, {&quot;id&quot;:... | John Carter is a war-weary, former military ca... | 2124 | 6.1 | 43.926995 | . As we can see from the above data, cast, crew, genres and keywords are lists of JSON objects: JSON objects are surrounded by curly braces {}. JSON objects are written in key/value pairs. Keys must be strings, and values must be a valid JSON data type (string, number, object, array, boolean or null). Keys and values are separated by a colon. Each key/value pair is separated by a comma. . We will now extract the values of the first three actor names from the feature &#39;cast&#39; using key &#39;name&#39;, director from feature &#39;crew&#39; using key &#39;job&#39; = Director and key &#39;name&#39;, first three movie genre from feature &#39;genres&#39; using key &#39;name&#39; and first three keywords related to movie from feature &#39;keywords&#39; using key &#39;name&#39;. . ## Extract cast, crew, keywords and genre # Parse the stringified features into their corresponding python objects from ast import literal_eval features = [&#39;cast&#39;, &#39;crew&#39;, &#39;keywords&#39;, &#39;genres&#39;] for feature in features: data[feature] = data[feature].apply(literal_eval) . # Get the first three names from the cast,keywords and genre features. def get_list(x): if isinstance(x, list): names = [i[&#39;name&#39;] for i in x] #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list. if len(names) &gt; 3: names = names[:3] return names #Return empty list in case of missing/malformed data return [] # Get the director&#39;s name from the crew feature. If director is not listed, return NaN def get_director(x): for i in x: if i[&#39;job&#39;] == &#39;Director&#39;: return i[&#39;name&#39;] return np.nan . # Define new director, cast, genres and keywords features that are in a suitable form. data[&#39;director&#39;] = data[&#39;crew&#39;].apply(get_director) features = [&#39;cast&#39;, &#39;keywords&#39;, &#39;genres&#39;] for feature in features: data[feature] = data[feature].apply(get_list) . Let&#39;s look at how the data looks like now. . data.head() . movie_title cast crew genres keywords overview vote_count vote_average popularity director . 0 | Avatar | [Sam Worthington, Zoe Saldana, Sigourney Weaver] | [{&#39;credit_id&#39;: &#39;52fe48009251416c750aca23&#39;, &#39;de... | [Action, Adventure, Fantasy] | [culture clash, future, space war] | In the 22nd century, a paraplegic Marine is di... | 11800 | 7.2 | 150.437577 | James Cameron | . 1 | Pirates of the Caribbean: At World&#39;s End | [Johnny Depp, Orlando Bloom, Keira Knightley] | [{&#39;credit_id&#39;: &#39;52fe4232c3a36847f800b579&#39;, &#39;de... | [Adventure, Fantasy, Action] | [ocean, drug abuse, exotic island] | Captain Barbossa, long believed to be dead, ha... | 4500 | 6.9 | 139.082615 | Gore Verbinski | . 2 | Spectre | [Daniel Craig, Christoph Waltz, Léa Seydoux] | [{&#39;credit_id&#39;: &#39;54805967c3a36829b5002c41&#39;, &#39;de... | [Action, Adventure, Crime] | [spy, based on novel, secret agent] | A cryptic message from Bond’s past sends him o... | 4466 | 6.3 | 107.376788 | Sam Mendes | . 3 | The Dark Knight Rises | [Christian Bale, Michael Caine, Gary Oldman] | [{&#39;credit_id&#39;: &#39;52fe4781c3a36847f81398c3&#39;, &#39;de... | [Action, Crime, Drama] | [dc comics, crime fighter, terrorist] | Following the death of District Attorney Harve... | 9106 | 7.6 | 112.312950 | Christopher Nolan | . 4 | John Carter | [Taylor Kitsch, Lynn Collins, Samantha Morton] | [{&#39;credit_id&#39;: &#39;52fe479ac3a36847f813eaa3&#39;, &#39;de... | [Action, Adventure, Science Fiction] | [based on novel, mars, medallion] | John Carter is a war-weary, former military ca... | 2124 | 6.1 | 43.926995 | Andrew Stanton | . #Let&#39;s store this in a CSV for further use data.to_csv(&#39;movies_database.csv&#39;, index=False) . Demographic Filtering . Let&#39;s start with demographic filtering. We would need a metric to score the items and rank them accordingly. Every platform or app has their own way of evaluating this metric. Platforms can take the reliable data sources to pull out the information on items (such as ratings, votes etc.), calculate the score for each item, rank them in descending order and show the top items to the users. . Recommendation based on highest ratings calculated using total votes . For the above movie database, we can use the average rating provided by the voters to rank, however this has a major flaw in terms of skewness due to movies with very low number of votes. For example, movie with 8.5 rating with only 3 votes may not be better than movie with 7.8 rating with 100 votes. . In order to go around this, I have taken the weighted rating approach followed by IMDB to rate the movies in this tutorial. . . where, . v is the number of votes for the movie; | m is the minimum votes required to be listed in the chart; | R is the average rating of the movie; And | C is the mean vote across the whole report | . We already have values of v(vote_count) and R(vote_average). C can be calculated from the data. . Let&#39;s calculate the mean rating across the whole database of 4803 movies. . C= data[&#39;vote_average&#39;].mean() C . 6.092171559442011 . So, the mean rating for all the movies is approx 6 on a scale of 10. . The next step is to determine an appropriate value for m, the minimum votes required to be listed in the chart. Let&#39;s assume the cutoff is 70th percentile. In other words, for a movie to feature in the charts, it must have more votes than at least 70% of the movies in the list. . m= data[&#39;vote_count&#39;].quantile(0.7) m . 581.0 . Hence, for a movie to feature, it must have minimum of 581 votes. . Let&#39;s filter out the movies with this criteria. We can then calculate the weighted rating for the qualified movies using the formula mentioned above . q_movies = data.copy().loc[data[&#39;vote_count&#39;] &gt;= m] q_movies.shape . (1442, 10) . We see that there are 1442 movies which qualify to be in this list. . Now, we need to calculate our metric for each qualified movie. To do this, we will define a function, weighted_rating() and define a new feature score, of which we&#39;ll calculate the value by applying this function to our DataFrame of qualified movies; . def weighted_rating(x, m=m, C=C): v = x[&#39;vote_count&#39;] R = x[&#39;vote_average&#39;] # Calculation based on the IMDB formula return (v/(v+m) * R) + (m/(m+v) * C) . # Define a new feature &#39;score&#39; and calculate its value with `weighted_rating()` q_movies[&#39;score&#39;] = q_movies.apply(weighted_rating, axis=1) . Let&#39;s look at the top 10 highest rated movies! . #Sort movies based on score calculated above q_movies = q_movies.sort_values(&#39;score&#39;, ascending=False) #Print the top 10 movies q_movies[[&#39;movie_title&#39;, &#39;vote_count&#39;, &#39;vote_average&#39;, &#39;score&#39;]].head(10) . movie_title vote_count vote_average score . 1881 | The Shawshank Redemption | 8205 | 8.5 | 8.340775 | . 3337 | The Godfather | 5893 | 8.4 | 8.192887 | . 662 | Fight Club | 9413 | 8.3 | 8.171648 | . 3232 | Pulp Fiction | 8428 | 8.3 | 8.157615 | . 65 | The Dark Knight | 12002 | 8.2 | 8.102674 | . 809 | Forrest Gump | 7927 | 8.2 | 8.056059 | . 1818 | Schindler&#39;s List | 4329 | 8.3 | 8.038748 | . 3865 | Whiplash | 4254 | 8.3 | 8.034695 | . 96 | Inception | 13752 | 8.1 | 8.018611 | . 1990 | The Empire Strikes Back | 5879 | 8.2 | 8.010426 | . #Let&#39;s create the plot of top 10 movies based on weighted score import matplotlib.pyplot as plt plt.figure(figsize=(12,8)) plt.barh(q_movies[&#39;movie_title&#39;].head(10),q_movies[&#39;score&#39;].head(10), align=&#39;center&#39;,color=&#39;skyblue&#39;) plt.gca().invert_yaxis() plt.xlabel(&quot;Weighted Score&quot;, weight=&#39;bold&#39;) plt.title(&quot;Best Rated Movies&quot;,weight=&#39;bold&#39;) . Text(0.5, 1.0, &#39;Best Rated Movies&#39;) . We can see the classics such as &#39;The GodFather&#39;, &#39;The Shawshank Redemption&#39;, etc. in the above list. These are the best movies of all times and our recommendation system might work in case we want to show the greatest hits of all time based on votes. . Recommendation based on maximum popularity at the moment . In case we want to show the movies currently popular among the app users, we would need the popularity score for each movie, rank them in descending order and show the movies with highest popularity score to the users. . For the above movies database, popularity score has been already calculated and available in the feature &#39;popularity&#39;. Each app build their popularity ratings differently. In the data provided, it has been built using the following information: . Number of votes for the day | Number of views for the day | Number of users who marked it as a &quot;favourite&quot; for the day | Number of users who added it to their &quot;watchlist&quot; for the day | Release date | Number of total votes | Previous days score | . Details in the link - https://developers.themoviedb.org/3/getting-started/popularity . Let&#39;s look at the top 10 movies based on popularity! . pop= data.sort_values(&#39;popularity&#39;, ascending=False) import matplotlib.pyplot as plt plt.figure(figsize=(12,8)) plt.barh(pop[&#39;movie_title&#39;].head(10),pop[&#39;popularity&#39;].head(10), align=&#39;center&#39;,color=&#39;skyblue&#39;) plt.gca().invert_yaxis() plt.xlabel(&quot;Popularity Score&quot;, weight=&#39;bold&#39;) plt.title(&quot;Most Popular Movies&quot;,weight=&#39;bold&#39;) . Text(0.5, 1.0, &#39;Most Popular Movies&#39;) . The above lists has classics as well as movies popular among the masses. The above recommendations logics can be experimented and the best recommendation engine based on superior performance (eg. one with highest engagement of app users) can be deployed. . Endnotes . I hope this has helped in developing a basic understanding of what recommendation systems are, how they work, some of the different types and implementation of demographic filtering using the dummy dataset of ~5000 English Movies. Feel free to play around with the code by opening in Colab or cloning the repo in github. . In the next two blogs, I will work on other two types of recommendation systems - Content-based Filtering and Collaborative Filtering. . If you have any comments or suggestions please comment below or reach out to me at - Twitter or LinkedIn .",
            "url": "https://rahuls0959.github.io/ds-blog/python/recommendation%20system/relevancy/collaborative%20filtering/content-based%20filtering/demographic%20filtering/2020/05/03/_Recommendation-System_Part-1.html",
            "relUrl": "/python/recommendation%20system/relevancy/collaborative%20filtering/content-based%20filtering/demographic%20filtering/2020/05/03/_Recommendation-System_Part-1.html",
            "date": " • May 3, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Topic Modeling of Product Reviews on Amazon Scraped using Selenium in Python",
            "content": "Introduction . The blog covers the step-by-step process to scrap product reviews from Amazon webpage and analysing main topics from the extracted data. We will scrap 1000 reviews from the Amazon for Apple iPhone 11 64GB. With this data, we will convert each review doc into bag-of words for applying the topic modeling algorithm. We will be using Latent Dirichlet Allocation (LDA) algorithm in this tutorial. The main python libraries used are: . selenium: Selenium is a portable framework for testing web applications. We will be using this to interact with the browser and open URLs (https://pypi.org/project/selenium/) | gensim: Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning (https://pypi.org/project/gensim/) | . Web Scraping . Web scraping is a technique for extracting information from the internet automatically using a software that simulates human web surfing. Web scraping helps us extract large volumes of data about customers, products, people, stock markets, etc. It is usually difficult to get this kind of information on a large scale using traditional data collection methods. We can utilize the data collected from a website such as e-commerce portal, social media channels to understand customer behaviors and sentiments, buying patterns, and brand attribute associations which are critical insights for any business. . The first and foremost thing while scraping a website is to understand the structure of the website. We will be scraping the reviews for Apple iPhone 11 64GB on Amazon.in website. We will scrape 1000 reviews from different users across multiple pages. We will scrape user name, date of review and review and export it into a .csv file for any further analysis. . Import Packages . Instll selenium package (if not already worked with before) using command &#39;!pip install selenium&#39; | Import webdriver from selenium in the notebook which we use to open an instance of Chrome browser | The executable file for launching Chrome &#39;chromedriver.exe&#39; should be in the same folder as the notebook | . #Importing packages from selenium import webdriver import pandas as pd . Script for Scraping . The below code opens the new chrome browser window and open our website with the url link provided. By the way, chrome knows that you are accessing it through an automated software! . driver = webdriver.Chrome(&#39;chromedriver.exe&#39;) url = &#39;https://www.amazon.in/Apple-iPhone-11-64GB-White/product-reviews/B07XVMCLP7/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&amp;reviewerType=all_reviews&amp;pageNumber1&#39; driver.get(url) . Woha! We just opened an url from python notebook. . . We will inspect 3 items (user id, date and comment) on our web page and understand how we can extract them. . Xpath for User id: Inspecting the userid, we can see the highlighted text represents the XML code for user id.The XML path (XPath)for the userid is shown below: . //*[@id=&quot;customer_review-RBOIMRTKIYBBR&quot;]/div[1]/a/div[2]/span . | . . There is an interesting thing to note here that the XML path contains a review id, which uniquely denotes each review on the website. This will be very helpful as we try to recursively scrape multiple comments. . Xpath for Date &amp; review: Similarily, we will find the XPaths for date and review. | Selenium has a function called “find_elements_by_xpath”. We will pass our XPath into this function and get a selenium element. Once we have the element, we can extract the text inside our XPath using the ‘text’ function. | We will recursively run the code for different review id and extract user id, date and review for each review id. Also, we will recursively go to next pages by simply changing the page numbers in the url to extract more comments until we get the desired number of comments. | . driver = webdriver.Chrome(&#39;chromedriver.exe&#39;) #Creating empty data frame to store user_id, dates and comments from ~5K users. data = pd.DataFrame(columns = [&#39;date&#39;,&#39;username&#39;,&#39;review&#39;]) j = 1 while (j&lt;=130): # Running while loop only till we get 1K reviews if (len(data)&lt;1000): url = &#39;https://www.amazon.in/Apple-iPhone-11-64GB-White/product-reviews/B07XVMCLP7/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&amp;reviewerType=all_reviews&amp;pageNumber=&#39; + str(j) driver.get(url) ids = driver.find_elements_by_xpath(&quot;//*[contains(@id,&#39;customer_review-&#39;)]&quot;) review_ids = [] for i in ids: review_ids.append(i.get_attribute(&#39;id&#39;)) for x in review_ids: #Extract dates from for each user on a page date_element = driver.find_elements_by_xpath(&#39;//*[@id=&quot;&#39; + x +&#39;&quot;]/span&#39;)[0] date = date_element.text #Extract user ids from each user on a page username_element = driver.find_elements_by_xpath(&#39;//*[@id=&quot;&#39; + x +&#39;&quot;]/div[1]/a/div[2]/span&#39;)[0] username = username_element.text #Extract Message for each user on a page review_element = driver.find_elements_by_xpath(&#39;//*[@id=&quot;&#39; + x +&#39;&quot;]/div[4]&#39;)[0] review = review_element.text #Adding date, userid and comment for each user in a dataframe data.loc[len(data)] = [date,username,review] j=j+1 else: break . Data Cleaning . We perform few data cleaning operations such as replacing line breaks with a space and copy the data into .csv file which can be used for further analysis. | . import copy data = copy.deepcopy(data) def remove_space(s): return s.replace(&quot; n&quot;,&quot; &quot;) data[&#39;review&#39;] = data[&#39;review&#39;].apply(remove_space) data.to_csv(&#39;amazon_reviews.csv&#39;, header=True, sep=&#39;,&#39;) . data = pd.read_csv(&#39;amazon_reviews.csv&#39;,index_col=[0]) data . date username review . 0 | Reviewed in India on 20 October 2019 | Suman Biswas | May be my first negative review about the prod... | . 1 | Reviewed in India on 17 September 2019 | Kaushik Bajaj | It&#39;s very expensive but the quality you get is... | . 2 | Reviewed in India on 29 September 2019 | Sunny Kumar | The iPhone design is good and the camera quali... | . 3 | Reviewed in India on 30 September 2019 | shanu Kumar | Awesome Phone. Nice upgrade from iPhone 6s to ... | . 4 | Reviewed in India on 14 October 2019 | Amazon Customer | My Phone is Producing Too Much Heat Even Didn’... | . ... | ... | ... | ... | . 995 | Reviewed in India on 4 March 2020 | Md.Imran | Rt | . 996 | Reviewed in India on 1 March 2020 | Amazon Customer | ❤️ | . 997 | Reviewed in India on 9 March 2020 | Chirag Patel | Ok | . 998 | Reviewed in India on 11 March 2020 | chintu | Excellent | . 999 | Reviewed in India on 8 March 2020 | Amazon Customer | Excellent | . 1000 rows × 3 columns . Since the goal of further analysis is to perform topic modeling, we will solely focus on the review text, and drop other metadata columns i.e. date and user name. | . # Remove the columns data = data.drop(columns=[&#39;date&#39;, &#39;username&#39;], axis=1) # Print out the data data . review . 0 | May be my first negative review about the prod... | . 1 | It&#39;s very expensive but the quality you get is... | . 2 | The iPhone design is good and the camera quali... | . 3 | Awesome Phone. Nice upgrade from iPhone 6s to ... | . 4 | My Phone is Producing Too Much Heat Even Didn’... | . ... | ... | . 995 | Rt | . 996 | ❤️ | . 997 | Ok | . 998 | Excellent | . 999 | Excellent | . 1000 rows × 1 columns . Topic Modeling using LDA . Topic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic. LDA is a generative probabilistic model that assumes each topic is a mixture over an underlying set of words, and each document is a mixture of over a set of topic probabilities. . Illustration of LDA input/output workflow (Credit: http://chdoig.github.io/pytexas2015-topic-modeling/#/3/4) . . Data Pre-processing . We will preprocess the review data using gensim library. Few of the actions performed by preprocess_string as follows: . Tokenization: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation. | All stopwords are removed. | Words are lemmatized: words in third person are changed to first person and verbs in past and future tenses are changed into present. | Words are stemmed: words are reduced to their root form. | . Please see below the output after pre-processing one of the reviews. . import gensim from gensim.parsing.preprocessing import preprocess_string # print unprocessed text print(data.review[1]) # print processed text print(preprocess_string(data.review[1])) . It&#39;s very expensive but the quality you get is osum [&#39;expens&#39;, &#39;qualiti&#39;, &#39;osum&#39;] . processed_data = data[&#39;review&#39;].map(preprocess_string) processed_data . 0 [neg, review, product, amazon, elat, receiv, i... 1 [expens, qualiti, osum] 2 [iphon, design, good, camera, qualiti, awesom,... 3 [awesom, phone, nice, upgrad, iphon, iphon, lo... 4 [phone, produc, heat, didn’t, sim, half, hour,... ... 995 [] 996 [] 997 [] 998 [excel] 999 [excel] Name: review, Length: 1000, dtype: object . Preparign Document-Term-Matrix . Gensim requires that tokens be converted to a dictionary. In this instance a dictionary is a mapping between words and their integer IDs. We then create a Document-Term-Matrix where we use Bag-of-Words approach returning the vector of word and its frequency (number of occurences in the document) for each document. . # Importing Gensim import gensim from gensim import corpora # Creating the term dictionary of our list of documents (corpus), where every unique term is assigned an index. dictionary = corpora.Dictionary(processed_data) # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. doc_term_matrix = [dictionary.doc2bow(doc) for doc in processed_data] . Running LDA Model . We will now run the LDA Model. The number of topics you give is largely a guess/arbitrary. The model assumes the document contains that many topics. However, finding the number of topics explaining the data is a optimisation problem and can be found by &#39;Coherence Model&#39;. . Here, we have used number of topics = 3 . #RUN THE MODEL # Creating the object for LDA model using gensim library Lda = gensim.models.ldamodel.LdaModel # Running and Trainign LDA model on the document term matrix. TOPIC_CNT= 3 ldamodel = Lda(doc_term_matrix, num_topics=TOPIC_CNT, id2word = dictionary, passes=50) . We can then see the weights of top 20 words in each topic, which can help us to explain the topic. . #Results topics= ldamodel.print_topics(num_topics=TOPIC_CNT, num_words=20) topics . [(0, &#39;0.050*&#34;phone&#34; + 0.031*&#34;iphon&#34; + 0.019*&#34;best&#34; + 0.015*&#34;amazon&#34; + 0.015*&#34;nice&#34; + 0.014*&#34;great&#34; + 0.013*&#34;camera&#34; + 0.012*&#34;time&#34; + 0.012*&#34;appl&#34; + 0.010*&#34;charger&#34; + 0.009*&#34;product&#34; + 0.008*&#34;bui&#34; + 0.008*&#34;charg&#34; + 0.007*&#34;qualiti&#34; + 0.007*&#34;us&#34; + 0.006*&#34;good&#34; + 0.006*&#34;deliveri&#34; + 0.006*&#34;perfect&#34; + 0.006*&#34;look&#34; + 0.006*&#34;batteri&#34;&#39;), (1, &#39;0.032*&#34;phone&#34; + 0.026*&#34;awesom&#34; + 0.024*&#34;iphon&#34; + 0.015*&#34;love&#34; + 0.011*&#34;great&#34; + 0.010*&#34;io&#34; + 0.010*&#34;dai&#34; + 0.010*&#34;camera&#34; + 0.009*&#34;android&#34; + 0.008*&#34;displai&#34; + 0.008*&#34;it’&#34; + 0.008*&#34;batteri&#34; + 0.007*&#34;screen&#34; + 0.007*&#34;appl&#34; + 0.007*&#34;review&#34; + 0.006*&#34;pro&#34; + 0.006*&#34;best&#34; + 0.006*&#34;experi&#34; + 0.006*&#34;feel&#34; + 0.006*&#34;word&#34;&#39;), (2, &#39;0.066*&#34;good&#34; + 0.041*&#34;batteri&#34; + 0.036*&#34;camera&#34; + 0.035*&#34;phone&#34; + 0.032*&#34;product&#34; + 0.025*&#34;life&#34; + 0.023*&#34;qualiti&#34; + 0.023*&#34;appl&#34; + 0.018*&#34;iphon&#34; + 0.018*&#34;best&#34; + 0.014*&#34;awesom&#34; + 0.014*&#34;bui&#34; + 0.014*&#34;amaz&#34; + 0.013*&#34;monei&#34; + 0.011*&#34;excel&#34; + 0.011*&#34;valu&#34; + 0.011*&#34;perform&#34; + 0.010*&#34;mobil&#34; + 0.009*&#34;great&#34; + 0.009*&#34;worth&#34;&#39;)] . Extracting Topics . We can identify the follow topics emerging out of reviews of Amazon iPhone 11 64GB: . Topic #1: There seems to discussion of heat/ charging issue with the product. | Topic #2: The discussion on iPhone&#39;s features such as camera, display, battery. | Topic #3: iPhone being value for money and discussion on Amazon delivery service. | . word_dict = {}; for i in range(TOPIC_CNT): words = ldamodel.show_topic(i, topn = 20) word_dict[&#39;Topic #&#39; + &#39;{:2d}&#39;.format(i+1)] = [i[0] for i in words] pd.DataFrame(word_dict) . Topic # 1 Topic # 2 Topic # 3 . 0 | phone | phone | good | . 1 | iphon | awesom | batteri | . 2 | best | iphon | camera | . 3 | amazon | love | phone | . 4 | nice | great | product | . 5 | great | io | life | . 6 | camera | dai | qualiti | . 7 | time | camera | appl | . 8 | appl | android | iphon | . 9 | charger | displai | best | . 10 | product | it’ | awesom | . 11 | bui | batteri | bui | . 12 | charg | screen | amaz | . 13 | qualiti | appl | monei | . 14 | us | review | excel | . 15 | good | pro | valu | . 16 | deliveri | best | perform | . 17 | perfect | experi | mobil | . 18 | look | feel | great | . 19 | batteri | word | worth | . The below code provide the % of topic a document is about. This helps to find the dominant topic in each review. . doc_to_topic = [] for i in range(len(doc_term_matrix)): top_topics = ldamodel.get_document_topics(doc_term_matrix[i], minimum_probability=0.0) topic_vec = [top_topics[j][1] for j in range(TOPIC_CNT)] doc_to_topic.append(topic_vec) . #Dataframe of topic document_topics = pd.DataFrame(doc_to_topic) document_topics = document_topics.rename(columns=lambda x: x + 1) document_topics.columns = document_topics.columns.astype(str) document_topics = document_topics.rename(columns=lambda x: &#39;Topic #&#39; + x) . #Dataframe of review and topics data_new = pd.concat([data,document_topics],axis=1,join=&#39;inner&#39;) data_new.head() . review Topic #1 Topic #2 Topic #3 . 0 | May be my first negative review about the prod... | 0.990397 | 0.004789 | 0.004815 | . 1 | It&#39;s very expensive but the quality you get is... | 0.084920 | 0.089119 | 0.825960 | . 2 | The iPhone design is good and the camera quali... | 0.989838 | 0.004876 | 0.005286 | . 3 | Awesome Phone. Nice upgrade from iPhone 6s to ... | 0.455081 | 0.503810 | 0.041108 | . 4 | My Phone is Producing Too Much Heat Even Didn’... | 0.978437 | 0.010337 | 0.011225 | . Endnotes . I hope this blog helps in understanding how powerful Topic Modeling is in understanding unstructured textual data. Feel free to play around with the code by opening in Colab or cloning the repo in github. . If you have any comments or suggestions please comment below or reach out to me at - Twitter or LinkedIn .",
            "url": "https://rahuls0959.github.io/ds-blog/python/text%20analytics/web%20scraping/topic%20modeling/selenium/gensim/nlp/2020/04/21/Topic_Modeling_Amazon_Reviews_new.html",
            "relUrl": "/python/text%20analytics/web%20scraping/topic%20modeling/selenium/gensim/nlp/2020/04/21/Topic_Modeling_Amazon_Reviews_new.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Basic Text Analytics Tutorial on Sentiment Analysis of Tweets using Python",
            "content": "Introduction . This blog covers the basics of Text Analytics. We will be using Natural Language Processing and Machine Learning alogorithm to analyse the tweets on the Twitter platform.We will analyse the sentiments of 5000 tweets with certain hashtags and label them as Positive, Negative or Neutral. With this data, we will build a ML model using Naive Bayes Classifier which can be deployed to predict the sentiment of new tweets.We will be using the following python libraries for this analysis: . tweepy: Library for accessing the Twitter API | pandas: Library to perform various operations on the data | nltk: Library to perform Natural Language Processing for English text | sklearn: Library to perform machine learning algorithms | . Imports . tweepy: http://docs.tweepy.org/en/latest/ | pandas: https://pandas.pydata.org/docs/user_guide/index.html | . import tweepy as tw #library for accessing Twitter API import pandas as pd #data analysis API import json . Scrapping Tweets from Twitter using Tweepy . You have to first create a Twitter Developer account to gain credentials for Tweepy. This does require that you have a Twitter account. The application will ask various questions about what sort of work you want to do. Don’t fret, these details don’t have to be extensive, and the process is relatively easy. Link- https://developer.twitter.com/en | . After finishing the application, the approval process is relatively quick and shouldn’t take longer than a couple of days. Upon being approved you will need to log in and set up a dev environment in the developer dashboard and view that app’s details to retrieve your developer credentials as shown in the below picture. Unless you specifically have requested access to the other API’s offered, you will now be able to use the standard Tweepy API. | . Before getting started on Tweepy you will have to authorize that you have the credentials to utilize its API. The code snippet given is how one authorizes themself. | The search parameters I focused on are q(text query), lang(language of tweets), since(starting date o tweets), include_rts(whether to include retweets or not), items(count of tweets to be extracted). In the below code, I scrape the 5000 of tweets since 1st April 2020 that have following text &quot;#coronavirus, #COVID19, #CoronavirusOutbreak&quot;. | If you want to further customize your search you can view the rest of the search parameters available in the api.search method here- https://tweepy.readthedocs.io/en/latest/api.html#API.search | One of the advantages of querying with Tweepy is the amount of information contained in the tweet object. I have queried only tweet txt in the below code. If you’re interested in grabbing other information you can view the full list of information available in Tweepy’s tweet object here- https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object | . #Twitter App Auth consumer_key = &#39;41zvYrOHdiIKgSq7Xf5tbTyrp&#39; consumer_secret = &#39;3rOqNnBRjjqkEpGfQcmyD9kh6WgGALmTmiI6IJUgILee0Z0Uad&#39; access_key = &#39;1224090842106757120-gD25mu7R2pNTCQCozu5o9SCSpE8XHG&#39; access_secret = &#39;D63jeYIxtByfzbMvwr2SSXycTSyR4Hdm1xCxcAt3mNfY5&#39; # Initialize API auth = tw.OAuthHandler(consumer_key, consumer_secret) auth.set_access_token(access_key, access_secret) api = tw.API(auth, wait_on_rate_limit=True) # Search terms search_words = [&quot;#coronavirus&quot;, &quot;#COVID19&quot;, &quot;#CoronavirusOutbreak&quot;] date_since = &quot;2020-04-01&quot; # Collect tweets tweets = tw.Cursor(api.search, q=search_words, lang=&quot;en&quot;, since=date_since, tweet_mode=&#39;extended&#39;, include_rts=True).items(5000) tweets_arr = [] # Iterate and print tweets for tweet in tweets: tweets_arr.append(tweet.full_text) print(&quot;Done&quot;) #Creating data frame of tweets df_tweets = pd.DataFrame(tweets_arr) df_tweets . Done . 0 . 0 | RT @chidambara09: #Coronavirus: n n#European P... | . 1 | China denies cover up, but abruptly raises COV... | . 2 | RT @chidambara09: #Coronavirus: n n#European P... | . 3 | RT @pcraindia: With necessary precautions and ... | . 4 | RT @chidambara09: #Coronavirus: n n#European P... | . ... | ... | . 4995 | Vietnam sets up &#39;Rice ATMs&#39; to provide free ri... | . 4996 | Covid19 austerity: New Zealand PM sets example... | . 4997 | RT @admediainsider: &quot;This phase has made India... | . 4998 | RT @pennewstweet: Masks are mandatory in Singa... | . 4999 | RT @QuickTake: Young people are falling seriou... | . 5000 rows × 1 columns . Sentiment Analysis of the scrapped tweets . Import nltk libraries for doing sentiment analysis: https://www.nltk.org/ | . # NLP libraries import nltk from nltk.tokenize import sent_tokenize #sentence tokenization:break text into sentences from nltk.tokenize import word_tokenize #word tokenization:break sentences into words from nltk.corpus import stopwords #removal of stop words from nltk.stem import PorterStemmer #lexicon Normalisation/Stemming: retain only root form of the word . Install required packages in nltk with inbuilt sentiment analyser. VADER belongs to a type of sentiment analysis that is based on lexicons of sentiment-related words. In this approach, each of the words in the lexicon is rated as to whether it is positive or negative, and in many cases, how positive or negative. | . #Using NLTK package to conduct sentiment analysis nltk.download(&#39;vader_lexicon&#39;) . [nltk_data] Downloading package vader_lexicon to [nltk_data] C: Users 61920959 AppData Roaming nltk_data... [nltk_data] Package vader_lexicon is already up-to-date! . True . The polarity scores for each tweet are calculated using Sentiment Intensity Analyser. | The Positive, Negative and Neutral scores represent the proportion of text that falls in these categories. Hence all these should add up to 1. | The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1(most extreme negative) and +1 (most extreme positive). | . #Sentiment Scores from nltk.sentiment.vader import SentimentIntensityAnalyzer sid = SentimentIntensityAnalyzer() scores = [] for tweet in tweets_arr: score = sid.polarity_scores(tweet) scores.append(score) #Dataframe for sentiment scores df_sentiments = pd.DataFrame(scores) df_sentiments dataset = pd.concat([df_tweets, df_sentiments], axis=1, join=&#39;inner&#39;) dataset . 0 neg neu pos compound . 0 | RT @chidambara09: #Coronavirus: n n#European P... | 0.000 | 1.000 | 0.000 | 0.0000 | . 1 | China denies cover up, but abruptly raises COV... | 0.257 | 0.743 | 0.000 | -0.8047 | . 2 | RT @chidambara09: #Coronavirus: n n#European P... | 0.000 | 1.000 | 0.000 | 0.0000 | . 3 | RT @pcraindia: With necessary precautions and ... | 0.115 | 0.885 | 0.000 | -0.3818 | . 4 | RT @chidambara09: #Coronavirus: n n#European P... | 0.000 | 1.000 | 0.000 | 0.0000 | . ... | ... | ... | ... | ... | ... | . 4995 | Vietnam sets up &#39;Rice ATMs&#39; to provide free ri... | 0.000 | 0.883 | 0.117 | 0.5106 | . 4996 | Covid19 austerity: New Zealand PM sets example... | 0.059 | 0.941 | 0.000 | -0.1280 | . 4997 | RT @admediainsider: &quot;This phase has made India... | 0.174 | 0.826 | 0.000 | -0.4588 | . 4998 | RT @pennewstweet: Masks are mandatory in Singa... | 0.000 | 0.909 | 0.091 | 0.0772 | . 4999 | RT @QuickTake: Young people are falling seriou... | 0.243 | 0.757 | 0.000 | -0.6249 | . 5000 rows × 5 columns . The overall sentiment of the tweet is computed using the following: positive sentiment : (compound score &gt;= 0.05) | neutral sentiment : (compound score &gt; -0.05) and (compound score &lt; 0.05) | negative sentiment : (compound score &lt;= -0.05) | . | As we can see, sentiment of around 50% of the tweets is neutral, 27% negative and 23% positive | . # Generate overall_sentiment using pandas overall_sentiment = [] for value in dataset[&quot;compound&quot;]: if value &gt;= 0.05: overall_sentiment.append(&quot;Positive&quot;) elif value &lt;= -0.05: overall_sentiment.append(&quot;Negative&quot;) else: overall_sentiment.append(&quot;Neutral&quot;) dataset[&quot;overall_sentiment&quot;] = overall_sentiment #dropping the scores columns data = dataset.drop(columns ={&quot;neg&quot;,&quot;pos&quot;,&quot;neu&quot;,&quot;compound&quot;}) # changing column name with rename() data = data.rename(columns = {0: &quot;text&quot;}) print(data) data.groupby(&#39;overall_sentiment&#39;).size() . text overall_sentiment 0 RT @chidambara09: #Coronavirus: n n#European P... Neutral 1 China denies cover up, but abruptly raises COV... Negative 2 RT @chidambara09: #Coronavirus: n n#European P... Neutral 3 RT @pcraindia: With necessary precautions and ... Negative 4 RT @chidambara09: #Coronavirus: n n#European P... Neutral ... ... ... 4995 Vietnam sets up &#39;Rice ATMs&#39; to provide free ri... Positive 4996 Covid19 austerity: New Zealand PM sets example... Negative 4997 RT @admediainsider: &#34;This phase has made India... Negative 4998 RT @pennewstweet: Masks are mandatory in Singa... Positive 4999 RT @QuickTake: Young people are falling seriou... Negative [5000 rows x 2 columns] . overall_sentiment Negative 1356 Neutral 2475 Positive 1169 dtype: int64 . Wordcloud of the Tweet Data . Wordcloud is a powerful visualisation tool to understand what are the main words in the content or what most people are tweeting about. In the below code we have removed the following words from appearing in wordcloud: . Words starting with @ (tweet author&#39;s name) | Words starting with # (we have extracted tweets with hashtags related to coronavirus) | Words such as RT (which is coming in case it is a re-tweet) | Few interesting observations from the word clouds: . Positive wordcloud: United States,deaths reported, new cases - United States is mostly mentioned in tweets with overall positive tone. | Negative wordcloud: Europe, seriously ill, young people - Tweets are showing the illness caused by virus and young people might be most affected by this disease. Europe is mostly mentioned in tweets with overall negative tone. | Neutral wordcloud: new case, total deaths, total confirmed - These might be general tweets providing coronaviris stats. | #visualizing word clouds for positive and negative tweets from wordcloud import WordCloud,STOPWORDS import matplotlib.pyplot as plt %matplotlib inline data_pos = data[ data[&#39;overall_sentiment&#39;] == &#39;Positive&#39;] data_pos = data_pos[&#39;text&#39;] data_neg = data[ data[&#39;overall_sentiment&#39;] == &#39;Negative&#39;] data_neg = data_neg[&#39;text&#39;] data_neu = data[ data[&#39;overall_sentiment&#39;] == &#39;Neutral&#39;] data_neu = data_neu[&#39;text&#39;] def wordcloud_draw(data, color = &#39;black&#39;): words = &#39; &#39;.join(data) cleaned_word = &quot; &quot;.join([word for word in words.split() if &#39;http&#39; not in word and not word.startswith(&#39;@&#39;) and not word.startswith(&#39;#&#39;) and word != &#39;RT&#39; ]) wordcloud = WordCloud(stopwords=STOPWORDS, background_color=color, width=2500, height=2000 ).generate(cleaned_word) plt.figure(1,figsize=(9, 9)) plt.imshow(wordcloud) plt.axis(&#39;off&#39;) plt.show() print(&quot;Positive words&quot;) wordcloud_draw(data_pos,&#39;white&#39;) print(&quot;Negative words&quot;) wordcloud_draw(data_neg) print(&quot;Neutral words&quot;) wordcloud_draw(data_neu,&#39;white&#39;) . Positive words . Negative words . Neutral words . Naive Bayes Classification Model . Import sklearn library for building the classification model: https://scikit-learn.org/stable/ | . #import machine learning libraries import time from sklearn.pipeline import Pipeline from sklearn.multiclass import OneVsRestClassifier from sklearn import model_selection, naive_bayes, metrics, svm from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfTransformer . Split the 5000 tweet data into training (70%) and validation sets (30%) | . #Splitting the data in train and test split t1 = time.time() X_train, X_test, y_train, y_test = train_test_split(data[&#39;text&#39;], data[&#39;overall_sentiment&#39;], test_size=0.3,random_state = 0) t2= time.time() . We will be using CountVectorizer for performing the following data preprocessing tasks: Representing the tweet text as bag-of-words for feature extraction | Text preprocessing, tokenizing and filtering of stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors. | | . The output would be the list of words/ tokens that appear in the tweet corpus and number of ocuurences of words in each tweet. . However, this would only give the occurences of word in a document which might be not be an ideal metric since larger tweets will have more words. We will solve this by normalising using TF(Term Frequency) - IDF (Inverse Document Frequency) metric explained in this image. TfidfTransform performs this function in Python. | . count_vect = CountVectorizer(lowercase=True,stop_words=&quot;english&quot;,min_df=10) count_vect.fit(X_train) X_train_counts = count_vect.transform(X_train) X_test_counts = count_vect.transform(X_test) # Create the tf-idf representation using the bag-of-words matrix tfidf_transformer = TfidfTransformer(norm=None) tfidf_transformer.fit(X_train_counts) X_train_tfid =tfidf_transformer.transform(X_train_counts) X_test_tfid = tfidf_transformer.transform(X_test_counts) . X_train_counts.shape . (3500, 624) . X_test_counts.shape . (1500, 624) . We will be building classification model using Naive Bayes algorithm. It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature (a big assumption indeed, that is why model is called Naive). We will be using multinomial naive bayes since there are more than 2 classes (positive, negative, neutral). . | Explained simply, naive bayes algoorithm works by calculating the probabilities using following formula and giving output as the class having highest probability. . | Our model accuracy on training data is 79% and 76% on test data (which is quite good). . | . #Applying Naive Bayes Classifier from sklearn.naive_bayes import MultinomialNB nb = MultinomialNB() #fit data to NB model nb.fit(X_train_tfid, y_train) # train naive bayes on count print(&quot;Train Accuracy: &quot;, round(nb.score(X_train_tfid,y_train), 3)) print(&quot;Test Accuracy: &quot;, round(nb.score(X_test_tfid,y_test), 3)) . Train Accuracy: 0.793 Test Accuracy: 0.76 . Using this model, we can predict the sentiment of any new tweet. As given in example below, the tone of the tweet is negative and model performed a well job in predicting the same. Voila! | . #predicting the sentiment of a new tweet docs_new = [&#39;Balancing working from home and shouldering the bulk of domestic tasks leaves many women stretched to capacity, meaning less quality time with their families and for themselves.&#39;] X_new_counts = count_vect.transform(docs_new) X_new_tfidf = tfidf_transformer.transform(X_new_counts) clf = nb.fit(X_train_tfid, y_train) predicted = clf.predict(X_new_tfidf) print(predicted) . [&#39;Neutral&#39;] . If you have any comments or suggestions please comment below or reach out to me at - Twitter or LinkedIn .",
            "url": "https://rahuls0959.github.io/ds-blog/python/text%20analytics/sentiment%20analysis/wordcloud/tweepy/nlp/sklearn/2020/04/18/twitter_sentiment_analysis.html",
            "relUrl": "/python/text%20analytics/sentiment%20analysis/wordcloud/tweepy/nlp/sklearn/2020/04/18/twitter_sentiment_analysis.html",
            "date": " • Apr 18, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there! I am Rahul. I have completed my engineering from IIT Kanpur in 2015 and MBA from ISB in 2020. I believe in the power of data analytics in solving challenging business problems. . . If you want to chat about Tech, Products, or Analytics feel free to reach out to me on Twitter or Linkedin .",
          "url": "https://rahuls0959.github.io/ds-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rahuls0959.github.io/ds-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}