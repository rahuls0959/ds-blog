{
  
    
        "post0": {
            "title": "Basic Text Analytics Tutorial of Tweet Sentiment Analysis",
            "content": "Introduction . This blog covers the basics of Text Analytics. We will be using Natural Language Processing and Machine Learning alogorithm to analyse the tweets on the Twitter platform.We will analyse the sentiments of 5000 tweets with certain hashtags and label them as Positive, Negative or Neutral. With this data, we will build a ML model using Naive Bayes Classifier which can be deployed to predict the sentiment of new tweets.We will be using the following python libraries for this analysis: . tweepy: Library for accessing the Twitter API | pandas: Library to perform various operations on the data | nltk: Library to perform Natural Language Processing for English text | sklearn: Library to perform machine learning algorithms | . Imports . tweepy: http://docs.tweepy.org/en/latest/ | pandas: https://pandas.pydata.org/docs/user_guide/index.html | . import tweepy as tw #library for accessing Twitter API import pandas as pd #data analysis API import json . Scrapping Tweets from Twitter using Tweepy . You have to first create a Twitter Developer account to gain credentials for Tweepy. This does require that you have a Twitter account. The application will ask various questions about what sort of work you want to do. Don’t fret, these details don’t have to be extensive, and the process is relatively easy. Link- https://developer.twitter.com/en | . After finishing the application, the approval process is relatively quick and shouldn’t take longer than a couple of days. Upon being approved you will need to log in and set up a dev environment in the developer dashboard and view that app’s details to retrieve your developer credentials as shown in the below picture. Unless you specifically have requested access to the other API’s offered, you will now be able to use the standard Tweepy API. | . Before getting started on Tweepy you will have to authorize that you have the credentials to utilize its API. The code snippet given is how one authorizes themself. | The search parameters I focused on are q(text query), lang(language of tweets), since(starting date o tweets), include_rts(whether to include retweets or not), items(count of tweets to be extracted). In the below code, I scrape the 5000 of tweets since 1st April 2020 that have following text &quot;#coronavirus, #COVID19, #CoronavirusOutbreak&quot;. | If you want to further customize your search you can view the rest of the search parameters available in the api.search method here- https://tweepy.readthedocs.io/en/latest/api.html#API.search | One of the advantages of querying with Tweepy is the amount of information contained in the tweet object. I have queried only tweet txt in the below code. If you’re interested in grabbing other information you can view the full list of information available in Tweepy’s tweet object here- https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object | . #Twitter App Auth consumer_key = &#39;41zvYrOHdiIKgSq7Xf5tbTyrp&#39; consumer_secret = &#39;3rOqNnBRjjqkEpGfQcmyD9kh6WgGALmTmiI6IJUgILee0Z0Uad&#39; access_key = &#39;1224090842106757120-gD25mu7R2pNTCQCozu5o9SCSpE8XHG&#39; access_secret = &#39;D63jeYIxtByfzbMvwr2SSXycTSyR4Hdm1xCxcAt3mNfY5&#39; # Initialize API auth = tw.OAuthHandler(consumer_key, consumer_secret) auth.set_access_token(access_key, access_secret) api = tw.API(auth, wait_on_rate_limit=True) # Search terms search_words = [&quot;#coronavirus&quot;, &quot;#COVID19&quot;, &quot;#CoronavirusOutbreak&quot;] date_since = &quot;2020-04-01&quot; # Collect tweets tweets = tw.Cursor(api.search, q=search_words, lang=&quot;en&quot;, since=date_since, tweet_mode=&#39;extended&#39;, include_rts=True).items(5000) tweets_arr = [] # Iterate and print tweets for tweet in tweets: tweets_arr.append(tweet.full_text) print(&quot;Done&quot;) #Creating data frame of tweets df_tweets = pd.DataFrame(tweets_arr) df_tweets . Done . 0 . 0 | RT @chidambara09: #Coronavirus: n n#European P... | . 1 | China denies cover up, but abruptly raises COV... | . 2 | RT @chidambara09: #Coronavirus: n n#European P... | . 3 | RT @pcraindia: With necessary precautions and ... | . 4 | RT @chidambara09: #Coronavirus: n n#European P... | . ... | ... | . 4995 | Vietnam sets up &#39;Rice ATMs&#39; to provide free ri... | . 4996 | Covid19 austerity: New Zealand PM sets example... | . 4997 | RT @admediainsider: &quot;This phase has made India... | . 4998 | RT @pennewstweet: Masks are mandatory in Singa... | . 4999 | RT @QuickTake: Young people are falling seriou... | . 5000 rows × 1 columns . Sentiment Analysis of the scrapped tweets . Import nltk libraries for doing sentiment analysis: https://www.nltk.org/ | . # NLP libraries import nltk from nltk.tokenize import sent_tokenize #sentence tokenization:break text into sentences from nltk.tokenize import word_tokenize #word tokenization:break sentences into words from nltk.corpus import stopwords #removal of stop words from nltk.stem import PorterStemmer #lexicon Normalisation/Stemming: retain only root form of the word . Install required packages in nltk with inbuilt sentiment analyser. VADER belongs to a type of sentiment analysis that is based on lexicons of sentiment-related words. In this approach, each of the words in the lexicon is rated as to whether it is positive or negative, and in many cases, how positive or negative. | . #Using NLTK package to conduct sentiment analysis nltk.download(&#39;vader_lexicon&#39;) . [nltk_data] Downloading package vader_lexicon to [nltk_data] C: Users 61920959 AppData Roaming nltk_data... [nltk_data] Package vader_lexicon is already up-to-date! . True . The polarity scores for each tweet are calculated using Sentiment Intensity Analyser. | The Positive, Negative and Neutral scores represent the proportion of text that falls in these categories. Hence all these should add up to 1. | The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1(most extreme negative) and +1 (most extreme positive). | . #Sentiment Scores from nltk.sentiment.vader import SentimentIntensityAnalyzer sid = SentimentIntensityAnalyzer() scores = [] for tweet in tweets_arr: score = sid.polarity_scores(tweet) scores.append(score) #Dataframe for sentiment scores df_sentiments = pd.DataFrame(scores) df_sentiments dataset = pd.concat([df_tweets, df_sentiments], axis=1, join=&#39;inner&#39;) dataset . 0 neg neu pos compound . 0 | RT @chidambara09: #Coronavirus: n n#European P... | 0.000 | 1.000 | 0.000 | 0.0000 | . 1 | China denies cover up, but abruptly raises COV... | 0.257 | 0.743 | 0.000 | -0.8047 | . 2 | RT @chidambara09: #Coronavirus: n n#European P... | 0.000 | 1.000 | 0.000 | 0.0000 | . 3 | RT @pcraindia: With necessary precautions and ... | 0.115 | 0.885 | 0.000 | -0.3818 | . 4 | RT @chidambara09: #Coronavirus: n n#European P... | 0.000 | 1.000 | 0.000 | 0.0000 | . ... | ... | ... | ... | ... | ... | . 4995 | Vietnam sets up &#39;Rice ATMs&#39; to provide free ri... | 0.000 | 0.883 | 0.117 | 0.5106 | . 4996 | Covid19 austerity: New Zealand PM sets example... | 0.059 | 0.941 | 0.000 | -0.1280 | . 4997 | RT @admediainsider: &quot;This phase has made India... | 0.174 | 0.826 | 0.000 | -0.4588 | . 4998 | RT @pennewstweet: Masks are mandatory in Singa... | 0.000 | 0.909 | 0.091 | 0.0772 | . 4999 | RT @QuickTake: Young people are falling seriou... | 0.243 | 0.757 | 0.000 | -0.6249 | . 5000 rows × 5 columns . The overall sentiment of the tweet is computed using the following: positive sentiment : (compound score &gt;= 0.05) | neutral sentiment : (compound score &gt; -0.05) and (compound score &lt; 0.05) | negative sentiment : (compound score &lt;= -0.05) | . | As we can see, sentiment of around 50% of the tweets is neutral, 27% negative and 23% positive | . # Generate overall_sentiment using pandas overall_sentiment = [] for value in dataset[&quot;compound&quot;]: if value &gt;= 0.05: overall_sentiment.append(&quot;Positive&quot;) elif value &lt;= -0.05: overall_sentiment.append(&quot;Negative&quot;) else: overall_sentiment.append(&quot;Neutral&quot;) dataset[&quot;overall_sentiment&quot;] = overall_sentiment #dropping the scores columns data = dataset.drop(columns ={&quot;neg&quot;,&quot;pos&quot;,&quot;neu&quot;,&quot;compound&quot;}) # changing column name with rename() data = data.rename(columns = {0: &quot;text&quot;}) print(data) data.groupby(&#39;overall_sentiment&#39;).size() . text overall_sentiment 0 RT @chidambara09: #Coronavirus: n n#European P... Neutral 1 China denies cover up, but abruptly raises COV... Negative 2 RT @chidambara09: #Coronavirus: n n#European P... Neutral 3 RT @pcraindia: With necessary precautions and ... Negative 4 RT @chidambara09: #Coronavirus: n n#European P... Neutral ... ... ... 4995 Vietnam sets up &#39;Rice ATMs&#39; to provide free ri... Positive 4996 Covid19 austerity: New Zealand PM sets example... Negative 4997 RT @admediainsider: &#34;This phase has made India... Negative 4998 RT @pennewstweet: Masks are mandatory in Singa... Positive 4999 RT @QuickTake: Young people are falling seriou... Negative [5000 rows x 2 columns] . overall_sentiment Negative 1356 Neutral 2475 Positive 1169 dtype: int64 . Wordcloud of the Tweet Data . Wordcloud is a powerful visualisation tool to understand what are the main words in the content or what most people are tweeting about. In the below code we have removed the following words from appearing in wordcloud: . Words starting with @ (tweet author&#39;s name) | Words starting with # (we have extracted tweets with hashtags related to coronavirus) | Words such as RT (which is coming in case it is a re-tweet) | Few interesting observations from the word clouds: . Positive wordcloud: United States,deaths reported, new cases - United States is mostly mentioned in tweets with overall positive tone. | Negative wordcloud: Europe, seriously ill, young people - Tweets are showing the illness caused by virus and young people might be most affected by this disease. Europe is mostly mentioned in tweets with overall negative tone. | Neutral wordcloud: new case, total deaths, total confirmed - These might be general tweets providing coronaviris stats. | #visualizing word clouds for positive and negative tweets from wordcloud import WordCloud,STOPWORDS import matplotlib.pyplot as plt %matplotlib inline data_pos = data[ data[&#39;overall_sentiment&#39;] == &#39;Positive&#39;] data_pos = data_pos[&#39;text&#39;] data_neg = data[ data[&#39;overall_sentiment&#39;] == &#39;Negative&#39;] data_neg = data_neg[&#39;text&#39;] data_neu = data[ data[&#39;overall_sentiment&#39;] == &#39;Neutral&#39;] data_neu = data_neu[&#39;text&#39;] def wordcloud_draw(data, color = &#39;black&#39;): words = &#39; &#39;.join(data) cleaned_word = &quot; &quot;.join([word for word in words.split() if &#39;http&#39; not in word and not word.startswith(&#39;@&#39;) and not word.startswith(&#39;#&#39;) and word != &#39;RT&#39; ]) wordcloud = WordCloud(stopwords=STOPWORDS, background_color=color, width=2500, height=2000 ).generate(cleaned_word) plt.figure(1,figsize=(9, 9)) plt.imshow(wordcloud) plt.axis(&#39;off&#39;) plt.show() print(&quot;Positive words&quot;) wordcloud_draw(data_pos,&#39;white&#39;) print(&quot;Negative words&quot;) wordcloud_draw(data_neg) print(&quot;Neutral words&quot;) wordcloud_draw(data_neu,&#39;white&#39;) . Positive words . Negative words . Neutral words . Naive Bayes Classification Model . Import sklearn library for building the classification model: https://scikit-learn.org/stable/ | . #import machine learning libraries import time from sklearn.pipeline import Pipeline from sklearn.multiclass import OneVsRestClassifier from sklearn import model_selection, naive_bayes, metrics, svm from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfTransformer . Split the 5000 tweet data into training (70%) and validation sets (30%) | . #Splitting the data in train and test split t1 = time.time() X_train, X_test, y_train, y_test = train_test_split(data[&#39;text&#39;], data[&#39;overall_sentiment&#39;], test_size=0.3,random_state = 0) t2= time.time() . We will be using CountVectorizer for performing the following data preprocessing tasks: Representing the tweet text as bag-of-words for feature extraction | Text preprocessing, tokenizing and filtering of stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors. | | . The output would be the list of words/ tokens that appear in the tweet corpus and number of ocuurences of words in each tweet. . However, this would only give the occurences of word in a document which might be not be an ideal metric since larger tweets will have more words. We will solve this by normalising using TF(Term Frequency) - IDF (Inverse Document Frequency) metric explained in this image. TfidfTransform performs this function in Python. | . count_vect = CountVectorizer(lowercase=True,stop_words=&quot;english&quot;,min_df=10) count_vect.fit(X_train) X_train_counts = count_vect.transform(X_train) X_test_counts = count_vect.transform(X_test) # Create the tf-idf representation using the bag-of-words matrix tfidf_transformer = TfidfTransformer(norm=None) tfidf_transformer.fit(X_train_counts) X_train_tfid =tfidf_transformer.transform(X_train_counts) X_test_tfid = tfidf_transformer.transform(X_test_counts) . X_train_counts.shape . (3500, 624) . X_test_counts.shape . (1500, 624) . We will be building classification model using Naive Bayes algorithm. It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature (a big assumption indeed, that is why model is called Naive). We will be using multinomial naive bayes since there are more than 2 classes (positive, negative, neutral). . | Explained simply, naive bayes algoorithm works by calculating the probabilities using following formula and giving output as the class having highest probability. . | Our model accuracy on training data is 79% and 76% on test data (which is quite good). . | . #Applying Naive Bayes Classifier from sklearn.naive_bayes import MultinomialNB nb = MultinomialNB() #fit data to NB model nb.fit(X_train_tfid, y_train) # train naive bayes on count print(&quot;Train Accuracy: &quot;, round(nb.score(X_train_tfid,y_train), 3)) print(&quot;Test Accuracy: &quot;, round(nb.score(X_test_tfid,y_test), 3)) . Train Accuracy: 0.793 Test Accuracy: 0.76 . Using this model, we can predict the sentiment of any new tweet. As given in example below, the tone of the tweet is negative and model performed a well job in predicting the same. Voila! | . #predicting the sentiment of a new tweet docs_new = [&#39;Balancing working from home and shouldering the bulk of domestic tasks leaves many women stretched to capacity, meaning less quality time with their families and for themselves.&#39;] X_new_counts = count_vect.transform(docs_new) X_new_tfidf = tfidf_transformer.transform(X_new_counts) clf = nb.fit(X_train_tfid, y_train) predicted = clf.predict(X_new_tfidf) print(predicted) . [&#39;Neutral&#39;] . If you have any comments or suggestions please comment below or reach out to me at - Twitter or LinkedIn .",
            "url": "https://rahuls0959.github.io/ds-blog/python/text%20analytics/sentiment%20analysis/wordcloud/tweepy/nlp/sklearn/2020/04/18/twittersentimentanalysis.html",
            "relUrl": "/python/text%20analytics/sentiment%20analysis/wordcloud/tweepy/nlp/sklearn/2020/04/18/twittersentimentanalysis.html",
            "date": " • Apr 18, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://rahuls0959.github.io/ds-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://rahuls0959.github.io/ds-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://rahuls0959.github.io/ds-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rahuls0959.github.io/ds-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}