{
  
    
        "post0": {
            "title": "Title",
            "content": "import tweepy as tw #library for accessing Twitter API import pandas as pd #data analysis API import json . #Twitter App Auth consumer_key = &#39;41zvYrOHdiIKgSq7Xf5tbTyrp&#39; consumer_secret = &#39;3rOqNnBRjjqkEpGfQcmyD9kh6WgGALmTmiI6IJUgILee0Z0Uad&#39; access_key = &#39;1224090842106757120-gD25mu7R2pNTCQCozu5o9SCSpE8XHG&#39; access_secret = &#39;D63jeYIxtByfzbMvwr2SSXycTSyR4Hdm1xCxcAt3mNfY5&#39; . # Initialize API auth = tw.OAuthHandler(consumer_key, consumer_secret) auth.set_access_token(access_key, access_secret) api = tw.API(auth, wait_on_rate_limit=True) . # Search terms search_words = [&quot;#coronavirus&quot;, &quot;#COVID19&quot;, &quot;#CoronavirusOutbreak&quot;] date_since = &quot;2020-04-01&quot; . # Collect tweets tweets = tw.Cursor(api.search, q=search_words, lang=&quot;en&quot;, since=date_since, tweet_mode=&#39;extended&#39;, include_rts=True).items(5000) tweets_arr = [] . # Iterate and print tweets for tweet in tweets: tweets_arr.append(tweet.full_text) print(&quot;Done&quot;) . Done . #Creating data frame of tweets df_tweets = pd.DataFrame(tweets_arr) df_tweets . 0 . 0 | RT @MaverakisLab: CORONAVIRUS WE NEED A BETTER... | . 1 | RT @QuickTake: Young people are falling seriou... | . 2 | Three more COVID-19 patients have been dischar... | . 3 | People Tell Us How Going on Video Chat First D... | . 4 | #Journey of the #Soul by Sam Oputa #ThursdayTh... | . ... | ... | . 4995 | https://t.co/maknYetj5h How is this possible? ... | . 4996 | RT @kr3at: US #CoronaVirus: 589,015 cases and ... | . 4997 | RT @kr3at: #CoronaVirus Outside of China - 1,8... | . 4998 | RT @kr3at: Poland reports 153 new cases and 12... | . 4999 | RT @kr3at: USA reports 57 new cases and 778 ne... | . 5000 rows × 1 columns . # NLP libraries import nltk from nltk.tokenize import sent_tokenize #sentence tokenization:break text into sentences from nltk.tokenize import word_tokenize #word tokenization:break sentences into words from nltk.corpus import stopwords #removal of stop words from nltk.stem import PorterStemmer #lexicon Normalisation/Stemming: retain only root form of the word . #Using NLTK package to conduct sentiment analysis nltk.download(&#39;vader_lexicon&#39;) . [nltk_data] Downloading package vader_lexicon to [nltk_data] C: Users 61920959 AppData Roaming nltk_data... [nltk_data] Package vader_lexicon is already up-to-date! . True . from nltk.sentiment.vader import SentimentIntensityAnalyzer sid = SentimentIntensityAnalyzer() scores = [] for tweet in tweets_arr: score = sid.polarity_scores(tweet) scores.append(score) . #Dataframe for sentiment scores df_sentiments = pd.DataFrame(scores) df_sentiments . neg neu pos compound . 0 | 0.000 | 0.712 | 0.288 | 0.5622 | . 1 | 0.243 | 0.757 | 0.000 | -0.6249 | . 2 | 0.000 | 0.952 | 0.048 | 0.0772 | . 3 | 0.000 | 1.000 | 0.000 | 0.0000 | . 4 | 0.000 | 1.000 | 0.000 | 0.0000 | . ... | ... | ... | ... | ... | . 4995 | 0.000 | 1.000 | 0.000 | 0.0000 | . 4996 | 0.000 | 0.891 | 0.109 | 0.4215 | . 4997 | 0.000 | 1.000 | 0.000 | 0.0000 | . 4998 | 0.000 | 1.000 | 0.000 | 0.0000 | . 4999 | 0.000 | 1.000 | 0.000 | 0.0000 | . 5000 rows × 4 columns . dataset = pd.concat([df_tweets, df_sentiments], axis=1, join=&#39;inner&#39;) dataset . 0 neg neu pos compound . 0 | RT @MaverakisLab: CORONAVIRUS WE NEED A BETTER... | 0.000 | 0.712 | 0.288 | 0.5622 | . 1 | RT @QuickTake: Young people are falling seriou... | 0.243 | 0.757 | 0.000 | -0.6249 | . 2 | Three more COVID-19 patients have been dischar... | 0.000 | 0.952 | 0.048 | 0.0772 | . 3 | People Tell Us How Going on Video Chat First D... | 0.000 | 1.000 | 0.000 | 0.0000 | . 4 | #Journey of the #Soul by Sam Oputa #ThursdayTh... | 0.000 | 1.000 | 0.000 | 0.0000 | . ... | ... | ... | ... | ... | ... | . 4995 | https://t.co/maknYetj5h How is this possible? ... | 0.000 | 1.000 | 0.000 | 0.0000 | . 4996 | RT @kr3at: US #CoronaVirus: 589,015 cases and ... | 0.000 | 0.891 | 0.109 | 0.4215 | . 4997 | RT @kr3at: #CoronaVirus Outside of China - 1,8... | 0.000 | 1.000 | 0.000 | 0.0000 | . 4998 | RT @kr3at: Poland reports 153 new cases and 12... | 0.000 | 1.000 | 0.000 | 0.0000 | . 4999 | RT @kr3at: USA reports 57 new cases and 778 ne... | 0.000 | 1.000 | 0.000 | 0.0000 | . 5000 rows × 5 columns . # Generate overall_sentiment using pandas overall_sentiment = [] for value in dataset[&quot;compound&quot;]: if value &gt; 0: overall_sentiment.append(&quot;Positive&quot;) elif value &lt; 0: overall_sentiment.append(&quot;Negative&quot;) else: overall_sentiment.append(&quot;Neutral&quot;) dataset[&quot;overall_sentiment&quot;] = overall_sentiment print(dataset) . 0 neg neu pos 0 RT @MaverakisLab: CORONAVIRUS WE NEED A BETTER... 0.000 0.712 0.288 1 RT @QuickTake: Young people are falling seriou... 0.243 0.757 0.000 2 Three more COVID-19 patients have been dischar... 0.000 0.952 0.048 3 People Tell Us How Going on Video Chat First D... 0.000 1.000 0.000 4 #Journey of the #Soul by Sam Oputa #ThursdayTh... 0.000 1.000 0.000 ... ... ... ... ... 4995 https://t.co/maknYetj5h How is this possible? ... 0.000 1.000 0.000 4996 RT @kr3at: US #CoronaVirus: 589,015 cases and ... 0.000 0.891 0.109 4997 RT @kr3at: #CoronaVirus Outside of China - 1,8... 0.000 1.000 0.000 4998 RT @kr3at: Poland reports 153 new cases and 12... 0.000 1.000 0.000 4999 RT @kr3at: USA reports 57 new cases and 778 ne... 0.000 1.000 0.000 compound overall_sentiment 0 0.5622 Positive 1 -0.6249 Negative 2 0.0772 Positive 3 0.0000 Neutral 4 0.0000 Neutral ... ... ... 4995 0.0000 Neutral 4996 0.4215 Positive 4997 0.0000 Neutral 4998 0.0000 Neutral 4999 0.0000 Neutral [5000 rows x 6 columns] . data = dataset.drop(columns ={&quot;neg&quot;,&quot;pos&quot;,&quot;neu&quot;,&quot;compound&quot;}) . # changing cols with rename() data = data.rename(columns = {0: &quot;text&quot;}) print(data) . text overall_sentiment 0 RT @MaverakisLab: CORONAVIRUS WE NEED A BETTER... Positive 1 RT @QuickTake: Young people are falling seriou... Negative 2 Three more COVID-19 patients have been dischar... Positive 3 People Tell Us How Going on Video Chat First D... Neutral 4 #Journey of the #Soul by Sam Oputa #ThursdayTh... Neutral ... ... ... 4995 https://t.co/maknYetj5h How is this possible? ... Neutral 4996 RT @kr3at: US #CoronaVirus: 589,015 cases and ... Positive 4997 RT @kr3at: #CoronaVirus Outside of China - 1,8... Neutral 4998 RT @kr3at: Poland reports 153 new cases and 12... Neutral 4999 RT @kr3at: USA reports 57 new cases and 778 ne... Neutral [5000 rows x 2 columns] . #visualizing word clouds for positive and negative tweets from wordcloud import WordCloud,STOPWORDS import matplotlib.pyplot as plt %matplotlib inline data_pos = data[ data[&#39;overall_sentiment&#39;] == &#39;Positive&#39;] data_pos = data_pos[&#39;text&#39;] data_neg = data[ data[&#39;overall_sentiment&#39;] == &#39;Negative&#39;] data_neg = data_neg[&#39;text&#39;] data_neu = data[ data[&#39;overall_sentiment&#39;] == &#39;Neutral&#39;] data_neu = data_neu[&#39;text&#39;] def wordcloud_draw(data, color = &#39;black&#39;): words = &#39; &#39;.join(data) cleaned_word = &quot; &quot;.join([word for word in words.split() if &#39;http&#39; not in word and not word.startswith(&#39;@&#39;) and not word.startswith(&#39;#&#39;) and word != &#39;RT&#39; ]) wordcloud = WordCloud(stopwords=STOPWORDS, background_color=color, width=2500, height=2000 ).generate(cleaned_word) plt.figure(1,figsize=(9, 9)) plt.imshow(wordcloud) plt.axis(&#39;off&#39;) plt.show() print(&quot;Positive words&quot;) wordcloud_draw(data_pos,&#39;white&#39;) print(&quot;Negative words&quot;) wordcloud_draw(data_neg) print(&quot;Neutral words&quot;) wordcloud_draw(data_neu) . Positive words . Negative words . Neutral words . #import machine learning libraries import time from sklearn.pipeline import Pipeline from sklearn.multiclass import OneVsRestClassifier from sklearn import model_selection, naive_bayes, metrics, svm from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfTransformer . #Splitting the data in train and test split t1 = time.time() X_train, X_test, y_train, y_test = train_test_split(data[&#39;text&#39;], data[&#39;overall_sentiment&#39;], test_size=0.3,random_state = 0) t2= time.time() print(round(t2-t1, 2),&quot; secs&quot;) . 0.05 secs . # Represent the review text as a bag-of-words # Text preprocessing, tokenizing and filtering of stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors count_vect = CountVectorizer(lowercase=True,stop_words=&quot;english&quot;,min_df=10) count_vect.fit(X_train) X_train_counts = count_vect.transform(X_train) X_test_counts = count_vect.transform(X_test) # Create the tf-idf representation using the bag-of-words matrix tfidf_transformer = TfidfTransformer(norm=None) tfidf_transformer.fit(X_train_counts) X_train_tfid =tfidf_transformer.transform(X_train_counts) X_test_tfid = tfidf_transformer.transform(X_test_counts) . X_train_counts.shape . (3500, 673) . X_test_counts.shape . (1500, 673) . #Applying Naive Bayes Classifier from sklearn.naive_bayes import MultinomialNB nb = MultinomialNB() #fit data to NB model nb.fit(X_train_tfid, y_train) # train naive bayes on count print(&quot;Train Accuracy: &quot;, round(nb.score(X_train_tfid,y_train), 3)) print(&quot;Test Accuracy: &quot;, round(nb.score(X_test_tfid,y_test), 3)) . Train Accuracy: 0.79 Test Accuracy: 0.761 . #predicting the sentiment of a new tweet docs_new = [&#39;Balancing working from home and shouldering the bulk of domestic tasks leaves many women stretched to capacity, meaning less quality time with their families and for themselves.&#39;] X_new_counts = count_vect.transform(docs_new) X_new_tfidf = tfidf_transformer.transform(X_new_counts) clf = nb.fit(X_train_tfid, y_train) predicted = clf.predict(X_new_tfidf) print(predicted) . [&#39;Negative&#39;] .",
            "url": "https://rahuls0959.github.io/ds-blog/2020/04/17/twitter.html",
            "relUrl": "/2020/04/17/twitter.html",
            "date": " • Apr 17, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "import tweepy as tw #library for accessing Twitter API import pandas as pd #data analysis API import json . Introduction . How to do twitter Analysis? . #Twitter App Auth consumer_key = &#39;41zvYrOHdiIKgSq7Xf5tbTyrp&#39; consumer_secret = &#39;3rOqNnBRjjqkEpGfQcmyD9kh6WgGALmTmiI6IJUgILee0Z0Uad&#39; access_key = &#39;1224090842106757120-gD25mu7R2pNTCQCozu5o9SCSpE8XHG&#39; access_secret = &#39;D63jeYIxtByfzbMvwr2SSXycTSyR4Hdm1xCxcAt3mNfY5&#39; . # Initialize API auth = tw.OAuthHandler(consumer_key, consumer_secret) auth.set_access_token(access_key, access_secret) api = tw.API(auth, wait_on_rate_limit=True) . # Search terms search_words = [&quot;#coronavirus&quot;, &quot;#COVID19&quot;, &quot;#CoronavirusOutbreak&quot;] date_since = &quot;2020-04-01&quot; . # Collect tweets tweets = tw.Cursor(api.search, q=search_words, lang=&quot;en&quot;, since=date_since, tweet_mode=&#39;extended&#39;, include_rts=True).items(5000) tweets_arr = [] . # Iterate and print tweets for tweet in tweets: tweets_arr.append(tweet.full_text) print(&quot;Done&quot;) . Done . #Creating data frame of tweets df_tweets = pd.DataFrame(tweets_arr) df_tweets . 0 . 0 | RT @MaverakisLab: CORONAVIRUS WE NEED A BETTER... | . 1 | RT @QuickTake: Young people are falling seriou... | . 2 | Three more COVID-19 patients have been dischar... | . 3 | People Tell Us How Going on Video Chat First D... | . 4 | #Journey of the #Soul by Sam Oputa #ThursdayTh... | . ... | ... | . 4995 | https://t.co/maknYetj5h How is this possible? ... | . 4996 | RT @kr3at: US #CoronaVirus: 589,015 cases and ... | . 4997 | RT @kr3at: #CoronaVirus Outside of China - 1,8... | . 4998 | RT @kr3at: Poland reports 153 new cases and 12... | . 4999 | RT @kr3at: USA reports 57 new cases and 778 ne... | . 5000 rows × 1 columns . # NLP libraries import nltk from nltk.tokenize import sent_tokenize #sentence tokenization:break text into sentences from nltk.tokenize import word_tokenize #word tokenization:break sentences into words from nltk.corpus import stopwords #removal of stop words from nltk.stem import PorterStemmer #lexicon Normalisation/Stemming: retain only root form of the word . #Using NLTK package to conduct sentiment analysis nltk.download(&#39;vader_lexicon&#39;) . [nltk_data] Downloading package vader_lexicon to [nltk_data] C: Users 61920959 AppData Roaming nltk_data... [nltk_data] Package vader_lexicon is already up-to-date! . True . from nltk.sentiment.vader import SentimentIntensityAnalyzer sid = SentimentIntensityAnalyzer() scores = [] for tweet in tweets_arr: score = sid.polarity_scores(tweet) scores.append(score) . #Dataframe for sentiment scores df_sentiments = pd.DataFrame(scores) df_sentiments . neg neu pos compound . 0 | 0.000 | 0.712 | 0.288 | 0.5622 | . 1 | 0.243 | 0.757 | 0.000 | -0.6249 | . 2 | 0.000 | 0.952 | 0.048 | 0.0772 | . 3 | 0.000 | 1.000 | 0.000 | 0.0000 | . 4 | 0.000 | 1.000 | 0.000 | 0.0000 | . ... | ... | ... | ... | ... | . 4995 | 0.000 | 1.000 | 0.000 | 0.0000 | . 4996 | 0.000 | 0.891 | 0.109 | 0.4215 | . 4997 | 0.000 | 1.000 | 0.000 | 0.0000 | . 4998 | 0.000 | 1.000 | 0.000 | 0.0000 | . 4999 | 0.000 | 1.000 | 0.000 | 0.0000 | . 5000 rows × 4 columns . dataset = pd.concat([df_tweets, df_sentiments], axis=1, join=&#39;inner&#39;) dataset . 0 neg neu pos compound . 0 | RT @MaverakisLab: CORONAVIRUS WE NEED A BETTER... | 0.000 | 0.712 | 0.288 | 0.5622 | . 1 | RT @QuickTake: Young people are falling seriou... | 0.243 | 0.757 | 0.000 | -0.6249 | . 2 | Three more COVID-19 patients have been dischar... | 0.000 | 0.952 | 0.048 | 0.0772 | . 3 | People Tell Us How Going on Video Chat First D... | 0.000 | 1.000 | 0.000 | 0.0000 | . 4 | #Journey of the #Soul by Sam Oputa #ThursdayTh... | 0.000 | 1.000 | 0.000 | 0.0000 | . ... | ... | ... | ... | ... | ... | . 4995 | https://t.co/maknYetj5h How is this possible? ... | 0.000 | 1.000 | 0.000 | 0.0000 | . 4996 | RT @kr3at: US #CoronaVirus: 589,015 cases and ... | 0.000 | 0.891 | 0.109 | 0.4215 | . 4997 | RT @kr3at: #CoronaVirus Outside of China - 1,8... | 0.000 | 1.000 | 0.000 | 0.0000 | . 4998 | RT @kr3at: Poland reports 153 new cases and 12... | 0.000 | 1.000 | 0.000 | 0.0000 | . 4999 | RT @kr3at: USA reports 57 new cases and 778 ne... | 0.000 | 1.000 | 0.000 | 0.0000 | . 5000 rows × 5 columns . # Generate overall_sentiment using pandas overall_sentiment = [] for value in dataset[&quot;compound&quot;]: if value &gt; 0: overall_sentiment.append(&quot;Positive&quot;) elif value &lt; 0: overall_sentiment.append(&quot;Negative&quot;) else: overall_sentiment.append(&quot;Neutral&quot;) dataset[&quot;overall_sentiment&quot;] = overall_sentiment print(dataset) . 0 neg neu pos 0 RT @MaverakisLab: CORONAVIRUS WE NEED A BETTER... 0.000 0.712 0.288 1 RT @QuickTake: Young people are falling seriou... 0.243 0.757 0.000 2 Three more COVID-19 patients have been dischar... 0.000 0.952 0.048 3 People Tell Us How Going on Video Chat First D... 0.000 1.000 0.000 4 #Journey of the #Soul by Sam Oputa #ThursdayTh... 0.000 1.000 0.000 ... ... ... ... ... 4995 https://t.co/maknYetj5h How is this possible? ... 0.000 1.000 0.000 4996 RT @kr3at: US #CoronaVirus: 589,015 cases and ... 0.000 0.891 0.109 4997 RT @kr3at: #CoronaVirus Outside of China - 1,8... 0.000 1.000 0.000 4998 RT @kr3at: Poland reports 153 new cases and 12... 0.000 1.000 0.000 4999 RT @kr3at: USA reports 57 new cases and 778 ne... 0.000 1.000 0.000 compound overall_sentiment 0 0.5622 Positive 1 -0.6249 Negative 2 0.0772 Positive 3 0.0000 Neutral 4 0.0000 Neutral ... ... ... 4995 0.0000 Neutral 4996 0.4215 Positive 4997 0.0000 Neutral 4998 0.0000 Neutral 4999 0.0000 Neutral [5000 rows x 6 columns] . data = dataset.drop(columns ={&quot;neg&quot;,&quot;pos&quot;,&quot;neu&quot;,&quot;compound&quot;}) . # changing cols with rename() data = data.rename(columns = {0: &quot;text&quot;}) print(data) . text overall_sentiment 0 RT @MaverakisLab: CORONAVIRUS WE NEED A BETTER... Positive 1 RT @QuickTake: Young people are falling seriou... Negative 2 Three more COVID-19 patients have been dischar... Positive 3 People Tell Us How Going on Video Chat First D... Neutral 4 #Journey of the #Soul by Sam Oputa #ThursdayTh... Neutral ... ... ... 4995 https://t.co/maknYetj5h How is this possible? ... Neutral 4996 RT @kr3at: US #CoronaVirus: 589,015 cases and ... Positive 4997 RT @kr3at: #CoronaVirus Outside of China - 1,8... Neutral 4998 RT @kr3at: Poland reports 153 new cases and 12... Neutral 4999 RT @kr3at: USA reports 57 new cases and 778 ne... Neutral [5000 rows x 2 columns] . #visualizing word clouds for positive and negative tweets from wordcloud import WordCloud,STOPWORDS import matplotlib.pyplot as plt %matplotlib inline data_pos = data[ data[&#39;overall_sentiment&#39;] == &#39;Positive&#39;] data_pos = data_pos[&#39;text&#39;] data_neg = data[ data[&#39;overall_sentiment&#39;] == &#39;Negative&#39;] data_neg = data_neg[&#39;text&#39;] data_neu = data[ data[&#39;overall_sentiment&#39;] == &#39;Neutral&#39;] data_neu = data_neu[&#39;text&#39;] def wordcloud_draw(data, color = &#39;black&#39;): words = &#39; &#39;.join(data) cleaned_word = &quot; &quot;.join([word for word in words.split() if &#39;http&#39; not in word and not word.startswith(&#39;@&#39;) and not word.startswith(&#39;#&#39;) and word != &#39;RT&#39; ]) wordcloud = WordCloud(stopwords=STOPWORDS, background_color=color, width=2500, height=2000 ).generate(cleaned_word) plt.figure(1,figsize=(9, 9)) plt.imshow(wordcloud) plt.axis(&#39;off&#39;) plt.show() print(&quot;Positive words&quot;) wordcloud_draw(data_pos,&#39;white&#39;) print(&quot;Negative words&quot;) wordcloud_draw(data_neg) print(&quot;Neutral words&quot;) wordcloud_draw(data_neu) . Positive words . Negative words . Neutral words . #import machine learning libraries import time from sklearn.pipeline import Pipeline from sklearn.multiclass import OneVsRestClassifier from sklearn import model_selection, naive_bayes, metrics, svm from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfTransformer . #Splitting the data in train and test split t1 = time.time() X_train, X_test, y_train, y_test = train_test_split(data[&#39;text&#39;], data[&#39;overall_sentiment&#39;], test_size=0.3,random_state = 0) t2= time.time() print(round(t2-t1, 2),&quot; secs&quot;) . 0.05 secs . # Represent the review text as a bag-of-words # Text preprocessing, tokenizing and filtering of stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors count_vect = CountVectorizer(lowercase=True,stop_words=&quot;english&quot;,min_df=10) count_vect.fit(X_train) X_train_counts = count_vect.transform(X_train) X_test_counts = count_vect.transform(X_test) # Create the tf-idf representation using the bag-of-words matrix tfidf_transformer = TfidfTransformer(norm=None) tfidf_transformer.fit(X_train_counts) X_train_tfid =tfidf_transformer.transform(X_train_counts) X_test_tfid = tfidf_transformer.transform(X_test_counts) . X_train_counts.shape . (3500, 673) . X_test_counts.shape . (1500, 673) . #Applying Naive Bayes Classifier from sklearn.naive_bayes import MultinomialNB nb = MultinomialNB() #fit data to NB model nb.fit(X_train_tfid, y_train) # train naive bayes on count print(&quot;Train Accuracy: &quot;, round(nb.score(X_train_tfid,y_train), 3)) print(&quot;Test Accuracy: &quot;, round(nb.score(X_test_tfid,y_test), 3)) . Train Accuracy: 0.79 Test Accuracy: 0.761 . #predicting the sentiment of a new tweet docs_new = [&#39;Balancing working from home and shouldering the bulk of domestic tasks leaves many women stretched to capacity, meaning less quality time with their families and for themselves.&#39;] X_new_counts = count_vect.transform(docs_new) X_new_tfidf = tfidf_transformer.transform(X_new_counts) clf = nb.fit(X_train_tfid, y_train) predicted = clf.predict(X_new_tfidf) print(predicted) . [&#39;Negative&#39;] .",
            "url": "https://rahuls0959.github.io/ds-blog/2020/04/17/Twitter_Sentiment_Analysis.html",
            "relUrl": "/2020/04/17/Twitter_Sentiment_Analysis.html",
            "date": " • Apr 17, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://rahuls0959.github.io/ds-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://rahuls0959.github.io/ds-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://rahuls0959.github.io/ds-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rahuls0959.github.io/ds-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}