{
  
    
        "post0": {
            "title": "Recommendation Systems - KNN Item-Based Collaborating Filtering (Part 3)",
            "content": "Collaborating filtering . Collaborative filtering approach builds a model from a user’s past behaviors (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that the user may have an interest in. . . Item-Based vs User-Based Collaborative Filtering . User based collaborating filtering uses the patterns of users similar to me to recommend a product (users like me also looked at these other items). . Item based collaborative filtering uses the patterns of users who browsed the same item as me to recommend me a product (users who looked at my item also looked at these other items). . Item-based approach is usually prefered than user-based approach. User-based approach is often harder to scale because of the dynamic nature of users, whereas items usually don&#39;t change much, so item-based approach often can be computed offline. . KNN Model . Collaborative Filtering models are developed using machine learning algorithms to predict a user’s rating of unrated items. There are several techniques for modeling such as K-Nearest Neighbors (KNN), Matrix Factorization, Deep Learning Models, etc. In this blog, we will be using KNN model. . The k-nearest neighbors (KNN) algorithm doesn’t make any assumptions on the underlying data distribution, but it relies on item feature similarity. When a KNN makes a prediction about an item (say, movie), it will calculate the &#39;distance&#39; between the target movie and every other movie in its database. It then ranks its distances and returns the top &#39;k&#39; nearest neighbor movies (smallest distance measure) as the most similar movie recommendations. . KNN is widely used algorithm in classification models where an item is classified to one of the classes based on majority class in the k-nearest neighbour items. We have to parametrize the value of &#39;k&#39; in the model. With increasing &#39;k&#39;, the accuracy first increases (due to over-fitting at lower &#39;k&#39; values) and then decreases at large values of &#39;k&#39;. The optimum values of &#39;k&#39; usually lies between 3-10. (Reference - https://machinelearningmastery.com/k-nearest-neighbors-for-machine-learning/) . In this blog, we will be developing an item-based KNN collaborating filtering model. . Data Pre-Processing . We have taken the data of Movies from Kaggle - https://www.kaggle.com/grouplens/movielens-latest-small which contains over 20 Mn+ ratings of movies by users. Let&#39;s explore the dataset. . import pandas as pd import numpy as np import warnings warnings.filterwarnings(&quot;ignore&quot;) df1=pd.read_csv(&#39;movielens-20m-dataset/movie.csv&#39;) df2=pd.read_csv(&#39;movielens-20m-dataset/rating.csv&#39;) . The &#39;movie&#39; file contains movie id, title of the movie and genres. Let&#39;s see first few rows of the dataset. . df1.head() . movieId title genres . 0 | 1 | Toy Story (1995) | Adventure|Animation|Children|Comedy|Fantasy | . 1 | 2 | Jumanji (1995) | Adventure|Children|Fantasy | . 2 | 3 | Grumpier Old Men (1995) | Comedy|Romance | . 3 | 4 | Waiting to Exhale (1995) | Comedy|Drama|Romance | . 4 | 5 | Father of the Bride Part II (1995) | Comedy | . The &#39;rating&#39; file contains ratings given by users to the movies. Let&#39;s see first few rows of the dataset. . df2.head() . userId movieId rating timestamp . 0 | 1 | 2 | 3.5 | 2005-04-02 23:53:47 | . 1 | 1 | 29 | 3.5 | 2005-04-02 23:31:16 | . 2 | 1 | 32 | 3.5 | 2005-04-02 23:33:39 | . 3 | 1 | 47 | 3.5 | 2005-04-02 23:32:07 | . 4 | 1 | 50 | 3.5 | 2005-04-02 23:29:40 | . Let&#39;s combine these two files on movieId column! . data = df2.merge(df1,on=&#39;movieId&#39;) data.head() . userId movieId rating timestamp title genres . 0 | 1 | 2 | 3.5 | 2005-04-02 23:53:47 | Jumanji (1995) | Adventure|Children|Fantasy | . 1 | 5 | 2 | 3.0 | 1996-12-25 15:26:09 | Jumanji (1995) | Adventure|Children|Fantasy | . 2 | 13 | 2 | 3.0 | 1996-11-27 08:19:02 | Jumanji (1995) | Adventure|Children|Fantasy | . 3 | 29 | 2 | 3.0 | 1996-06-23 20:36:14 | Jumanji (1995) | Adventure|Children|Fantasy | . 4 | 34 | 2 | 3.0 | 1996-10-28 13:29:44 | Jumanji (1995) | Adventure|Children|Fantasy | . Let&#39;s group the movies and find the total rating count for each movie. This will help in understanding the distribution of rating across movies. . df3 = (df2.groupby(by=[&#39;movieId&#39;])[&#39;rating&#39;].count().reset_index().rename(columns={&#39;rating&#39;:&#39;totalRatingCount_movies&#39;}) [[&#39;movieId&#39;,&#39;totalRatingCount_movies&#39;]]) df3.tail() . movieId totalRatingCount_movies . 26739 | 131254 | 1 | . 26740 | 131256 | 1 | . 26741 | 131258 | 1 | . 26742 | 131260 | 1 | . 26743 | 131262 | 1 | . We can see that there are lot of movies with only single rating also. In order for statistical significance, we should take the movies only with some minimum threshold of ratings. Let&#39;s check the statistical measures of the totalRatingCount field. . df3[&#39;totalRatingCount_movies&#39;].describe() . count 26744.000000 mean 747.841123 std 3085.818268 min 1.000000 25% 3.000000 50% 18.000000 75% 205.000000 max 67310.000000 Name: totalRatingCount_movies, dtype: float64 . About 25% of the movies received 205 or more ratings. Because we have so many movies in our data, we will limit it to the top 25% for statistical significance, and this will give us ~6700 movies from total 26744 unique movies. Also, intuitively, the recommendations should be of the popular movies with some threshold of reviews. . df4 = (df2.groupby(by=[&#39;userId&#39;])[&#39;rating&#39;].count().reset_index().rename(columns={&#39;rating&#39;:&#39;totalRatingCount_users&#39;}) [[&#39;userId&#39;,&#39;totalRatingCount_users&#39;]]) df4.tail() . userId totalRatingCount_users . 138488 | 138489 | 38 | . 138489 | 138490 | 151 | . 138490 | 138491 | 22 | . 138491 | 138492 | 82 | . 138492 | 138493 | 373 | . df4[&#39;totalRatingCount_users&#39;].describe() . count 138493.000000 mean 144.413530 std 230.267257 min 20.000000 25% 35.000000 50% 68.000000 75% 155.000000 max 9254.000000 Name: totalRatingCount_users, dtype: float64 . About 25% of the users gave less than 35 movie ratings. Because we have so many users in our data, we will limit it to the top users with total movie ratings greater than or equal to 50 for statistical significance, and this will give us nearly 80,000 users from total 1,38,493 users. Also, intuitively, it would be better to consider only movie reviews from avid watchers. . Let&#39;s add the totalRatingCount_movies and totalRatingCount_users to our combined data! . #Let&#39;s combine data with totalRatingCount data1 = data.merge(df3,on=&#39;movieId&#39;) data2 = data1.merge(df4,on=&#39;userId&#39;) data2.head() . userId movieId rating timestamp title genres totalRatingCount_movies totalRatingCount_users . 0 | 1 | 2 | 3.5 | 2005-04-02 23:53:47 | Jumanji (1995) | Adventure|Children|Fantasy | 22243 | 175 | . 1 | 1 | 29 | 3.5 | 2005-04-02 23:31:16 | City of Lost Children, The (Cité des enfants p... | Adventure|Drama|Fantasy|Mystery|Sci-Fi | 8520 | 175 | . 2 | 1 | 32 | 3.5 | 2005-04-02 23:33:39 | Twelve Monkeys (a.k.a. 12 Monkeys) (1995) | Mystery|Sci-Fi|Thriller | 44980 | 175 | . 3 | 1 | 47 | 3.5 | 2005-04-02 23:32:07 | Seven (a.k.a. Se7en) (1995) | Mystery|Thriller | 43249 | 175 | . 4 | 1 | 50 | 3.5 | 2005-04-02 23:29:40 | Usual Suspects, The (1995) | Crime|Mystery|Thriller | 47006 | 175 | . Let&#39;s keep only the movies with totalRatingCount_movies and total_ratingCount_users greather than the chosen threshold and store in popular movies dataframe! . #Let&#39;s keep only the movies with totalRatingCount &gt;=205 popular_movies1 = data2.query(&#39;totalRatingCount_movies &gt;= 205&#39;) popular_movies = popular_movies1.query (&#39;totalRatingCount_users &gt;= 50&#39;) popular_movies.head() . userId movieId rating timestamp title genres totalRatingCount_movies totalRatingCount_users . 0 | 1 | 2 | 3.5 | 2005-04-02 23:53:47 | Jumanji (1995) | Adventure|Children|Fantasy | 22243 | 175 | . 1 | 1 | 29 | 3.5 | 2005-04-02 23:31:16 | City of Lost Children, The (Cité des enfants p... | Adventure|Drama|Fantasy|Mystery|Sci-Fi | 8520 | 175 | . 2 | 1 | 32 | 3.5 | 2005-04-02 23:33:39 | Twelve Monkeys (a.k.a. 12 Monkeys) (1995) | Mystery|Sci-Fi|Thriller | 44980 | 175 | . 3 | 1 | 47 | 3.5 | 2005-04-02 23:32:07 | Seven (a.k.a. Se7en) (1995) | Mystery|Thriller | 43249 | 175 | . 4 | 1 | 50 | 3.5 | 2005-04-02 23:29:40 | Usual Suspects, The (1995) | Crime|Mystery|Thriller | 47006 | 175 | . KNN Modeling . Reshaping the Data . For K-Nearest Neighbors, we want the data to be in an array, where each row is a movie and each column is a different user. To reshape the dataframe, we&#39;ll pivot the dataframe to the wide format with movies as rows and users as columns. Then we&#39;ll fill the missing observations with 0s since we&#39;re going to be performing linear algebra operations (calculating distances between vectors). Finally, we transform the values of the dataframe into a scipy sparse matrix for more efficient calculations. . Let&#39;s create a pivot of the data with movie ids as rows and user ids as columns! . #pivot ratings into movie features movie_user_rating_pivot = popular_movies.pivot( index=&#39;movieId&#39;, columns=&#39;userId&#39;, values=&#39;rating&#39; ).fillna(0) # create mapper from movie title to index movie_to_idx = { movie: i for i, movie in enumerate(list(df1.set_index(&#39;movieId&#39;).loc[movie_user_rating_pivot.index].title)) } . movie_user_rating_pivot . userId 1 2 3 5 7 8 11 13 14 16 ... 138474 138475 138477 138483 138484 138486 138487 138490 138492 138493 . movieId . 1 | 0.0 | 0.0 | 4.0 | 0.0 | 0.0 | 4.0 | 4.5 | 4.0 | 4.5 | 3.0 | ... | 5.0 | 0.0 | 3.0 | 4.0 | 0.0 | 5.0 | 0.0 | 0.0 | 0.0 | 3.5 | . 2 | 3.5 | 0.0 | 0.0 | 3.0 | 0.0 | 0.0 | 0.0 | 3.0 | 0.0 | 0.0 | ... | 4.0 | 0.0 | 0.0 | 3.0 | 3.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.0 | . 3 | 0.0 | 4.0 | 0.0 | 0.0 | 3.0 | 5.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 4.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 4.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 116797 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 116823 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 117176 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 118696 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 119141 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 6690 rows × 85307 columns . Thus our data contains movie ratings of 6690 movies by 85307 users. . We can see that there are lot of fields with rating as &#39;0&#39;. This means that users have not provided ratings for that movie. It makes sense since there will be large number of ratings for most popular movies and few ratings for less popular or recently released movies. . However, working with sparse matrices causes space and time complexity. The solution to representing and working with sparse matrices is to use an alternate data structure to represent the sparse data. The zero values can be ignored and only the data or non-zero values in the sparse matrix need to be stored or acted upon. More on this in the link - https://machinelearningmastery.com/sparse-matrices-for-machine-learning/ . from scipy.sparse import csr_matrix # convert dataframe of movie features to scipy sparse matrix movie_user_rating_matrix = csr_matrix(movie_user_rating_pivot.values) . Fitting the Model . Time to implement the model. We&#39;ll initialize the NearestNeighbors class as model_knn and fit our sparse matrix (movie_user_rating_matrix) to the instance. By specifying the metric = cosine, the model will measure similarity between movie vectors by using cosine similarity (We have explained cosine similarity in details in part 2 of this blog on content-based filtering). . More on nearest neighbours algorithm can be found on the link - https://scikit-learn.org/stable/modules/neighbors.html . # import libraries from sklearn.neighbors import NearestNeighbors # define model model_knn = NearestNeighbors(metric=&#39;cosine&#39;, algorithm=&#39;brute&#39;, n_neighbors=20, n_jobs=-1) # fit model_knn.fit(movie_user_rating_matrix) . NearestNeighbors(algorithm=&#39;brute&#39;, leaf_size=30, metric=&#39;cosine&#39;, metric_params=None, n_jobs=-1, n_neighbors=20, p=2, radius=1.0) . from fuzzywuzzy import fuzz def fuzzy_matching(mapper, fav_movie, verbose=True): &quot;&quot;&quot; return the closest match via fuzzy ratio. If no match found, return None Parameters - mapper: dict, map movie title name to index of the movie in data fav_movie: str, name of user input movie verbose: bool, print log if True Return index of the closest match &quot;&quot;&quot; match_tuple = [] # get match for title, idx in mapper.items(): ratio = fuzz.ratio(title.lower(), fav_movie.lower()) if ratio &gt;= 60: match_tuple.append((title, idx, ratio)) # sort match_tuple = sorted(match_tuple, key=lambda x: x[2])[::-1] if not match_tuple: print(&#39;Oops! No match is found&#39;) return if verbose: print(&#39;Found possible matches in our database: {0} n&#39;.format([x[0] for x in match_tuple])) return match_tuple[0][1] . def make_recommendation(model_knn, data, mapper, fav_movie, n_recommendations): &quot;&quot;&quot; return top n similar movie recommendations based on user&#39;s input movie Parameters - model_knn: sklearn model, knn model data: movie-user matrix mapper: dict, map movie title name to index of the movie in data fav_movie: str, name of user input movie n_recommendations: int, top n recommendations Return list of top n similar movie recommendations &quot;&quot;&quot; # fit model_knn.fit(data) # get input movie index print(&#39;You have input movie:&#39;, fav_movie) idx = fuzzy_matching(mapper, fav_movie, verbose=True) # inference print(&#39;Recommendation system start to make inference&#39;) print(&#39;...... n&#39;) distances, indices = model_knn.kneighbors(data[idx], n_neighbors=n_recommendations+1) # get list of raw idx of recommendations raw_recommends = sorted(list(zip(indices.squeeze().tolist(), distances.squeeze().tolist())), key=lambda x: x[1])[:0:-1] # get reverse mapper reverse_mapper = {v: k for k, v in mapper.items()} # print recommendations print(&#39;Recommendations for {}:&#39;.format(fav_movie)) for i, (idx, dist) in enumerate(raw_recommends): print(&#39;{0}: {1}, with distance of {2}&#39;.format(i+1, reverse_mapper[idx], dist)) . my_favorite = &#39;The Dark Knight Rises&#39; make_recommendation( model_knn=model_knn, data=movie_user_rating_matrix, fav_movie=my_favorite, mapper=movie_to_idx, n_recommendations=10) . You have input movie: The Dark Knight Rises Found possible matches in our database: [&#39;Dark Knight Rises, The (2012)&#39;] Recommendation system start to make inference ...... Recommendations for The Dark Knight Rises: 1: Sherlock Holmes: A Game of Shadows (2011), with distance of 0.5127881899481235 2: Amazing Spider-Man, The (2012), with distance of 0.49290540194859955 3: Hunger Games, The (2012), with distance of 0.4854909300858494 4: Hobbit: An Unexpected Journey, The (2012), with distance of 0.47720058748608674 5: Looper (2012), with distance of 0.47505264255188184 6: X-Men: First Class (2011), with distance of 0.4742094475174321 7: Inception (2010), with distance of 0.46427478653960397 8: Skyfall (2012), with distance of 0.44506859529221166 9: Django Unchained (2012), with distance of 0.43321771381173113 10: Avengers, The (2012), with distance of 0.3402240240654465 . This is interesting that KNN model recommends movies produced in very similar years. Also, we can see that cosine similarity is small. This is due to that our data has lot of zero values and has high sparsity, hence the distances starts to fall apart. . Please note that the movies usually predicted as recommendations are the ones that have the highest volume of ratings. That means that our network is being greatly influenced by heavily rated movies. In such a situation, a movie might be the best recommendation for ‘The Dark Knight Rises’ but could be overlooked by our model due to fewer ratings provided by users for said movie. . Endnotes . I hope this has helped in basic understanding of implementing collaborating filtering in Python. Feel free to play around with the code by opening in Colab or cloning the repo in github. . However, the model has quite a number of limitations such as in case the items don&#39;t have enough number of ratings or interactions or new users don&#39;t have enough interaction on the platform, the recommendations given by CF would be quite poor. Hence, it is rare that new movies would be recommendated by the model since they would have very few ratings. Most of the recommendation systems are hybrid of different filtering techniques such as content-based and collaborative filtering to address some of these concerns.Also, with increasing base of users and movies, the KNN model would face scalability issues. There are other techniques to solve these issues. . If you have any comments or suggestions please comment below or reach out to me at - Twitter or LinkedIn .",
            "url": "https://rahuls0959.github.io/ds-blog/python/recommendation%20system/relevancy/collaborative%20filtering/content-based%20filtering/demographic%20filtering/2020/06/06/_Recommendation-System_Part-3.html",
            "relUrl": "/python/recommendation%20system/relevancy/collaborative%20filtering/content-based%20filtering/demographic%20filtering/2020/06/06/_Recommendation-System_Part-3.html",
            "date": " • Jun 6, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Recommendation Systems - Content Based Filtering (Part 2)",
            "content": "Content Based Filtering . This recommendation systems works by finding similarities between the items. If a user has liked or wishlisted some items in the past, this would try to find similar items and recommend to the user. . Content-based filtering is also used in Google PageRank algorithm to recommend the relevant webpages basis search keyworks. This is used along with citation model (reference of webpage in other webpages) and behavioral model (the activity on the webpage) to arrive at the final results. . We see this type of recommendation in work while searching for items in various apps. In Netflix, we can see some sort of weightage to content based filtering in the section &#39;Because you watched xxx&#39; (Read the article - https://qz.com/1059434/netflix-finally-explains-how-its-because-you-watched-recommendation-tool-works/) . . Loading the Data . Let&#39;s load our CSV file which we have saved in the Part-1 of the blog! . import pandas as pd import numpy as np import warnings warnings.filterwarnings(&quot;ignore&quot;) data=pd.read_csv(&#39;movies_database.csv&#39;) . Let&#39;s look at first few rows of our data! . data = data[[&#39;movie_title&#39;,&#39;overview&#39;,&#39;cast&#39;,&#39;genres&#39;,&#39;keywords&#39;,&#39;director&#39;]] data.head() . movie_title overview cast genres keywords director . 0 | Avatar | In the 22nd century, a paraplegic Marine is di... | [&#39;Sam Worthington&#39;, &#39;Zoe Saldana&#39;, &#39;Sigourney ... | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Fantasy&#39;] | [&#39;culture clash&#39;, &#39;future&#39;, &#39;space war&#39;] | James Cameron | . 1 | Pirates of the Caribbean: At World&#39;s End | Captain Barbossa, long believed to be dead, ha... | [&#39;Johnny Depp&#39;, &#39;Orlando Bloom&#39;, &#39;Keira Knight... | [&#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Action&#39;] | [&#39;ocean&#39;, &#39;drug abuse&#39;, &#39;exotic island&#39;] | Gore Verbinski | . 2 | Spectre | A cryptic message from Bond’s past sends him o... | [&#39;Daniel Craig&#39;, &#39;Christoph Waltz&#39;, &#39;Léa Seydo... | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Crime&#39;] | [&#39;spy&#39;, &#39;based on novel&#39;, &#39;secret agent&#39;] | Sam Mendes | . 3 | The Dark Knight Rises | Following the death of District Attorney Harve... | [&#39;Christian Bale&#39;, &#39;Michael Caine&#39;, &#39;Gary Oldm... | [&#39;Action&#39;, &#39;Crime&#39;, &#39;Drama&#39;] | [&#39;dc comics&#39;, &#39;crime fighter&#39;, &#39;terrorist&#39;] | Christopher Nolan | . 4 | John Carter | John Carter is a war-weary, former military ca... | [&#39;Taylor Kitsch&#39;, &#39;Lynn Collins&#39;, &#39;Samantha Mo... | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Science Fiction&#39;] | [&#39;based on novel&#39;, &#39;mars&#39;, &#39;medallion&#39;] | Andrew Stanton | . We can find the similarity scores between movies based on various metadata. We will first build a model looking at movie plot summaries given in the &#39;overview&#39; column and then refine our recommendations by including actor, director, genre, etc. . Plot Description based Recommendation . Creating a TF-IDF Vectorizer . We first need to convert each overview into its word vector. Next, we will have the find the Term Frequency - Inverse Document Frequency (TF-IDF) vector for each overview. . The TF-IDF algorithm is used to weight a word in each document and assign the importance of the word based on the following two factors: . Term Frequency (TF): The number of times the word appears in the document (in our case, the movie plot description) | Inverse Document Frequency (IDF): The number of times the word appears in the corpus, representing how significant the term is in the whole corpus (in our case, corpus of movie plot descriptions) | . The below formula is used for TFIDF calculation: . In Python, &#39;scikit-learn&#39; library has a pre-built TF-IDF vectorizer that calculates the TF-IDF score for each document’s description, word-by-word. . Let&#39;s now implement a TFIDF matrix for our data! (Link - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) . #Import TfIdfVectorizer from scikit-learn from sklearn.feature_extraction.text import TfidfVectorizer #Define a TF-IDF Vectorizer Object. Remove all english stop words such as &#39;the&#39;, &#39;a&#39;, &#39;an&#39; tfidf = TfidfVectorizer(stop_words=&#39;english&#39;) #Replace NaN with an empty string data[&#39;overview&#39;] = data[&#39;overview&#39;].fillna(&#39;&#39;) #Construct the required TF-IDF matrix by fitting and transforming the data tfidf_matrix = tfidf.fit_transform(data[&#39;overview&#39;]) #Output the shape of tfidf_matrix tfidf_matrix.shape . (4803, 20978) . We can see that over 20000 unique words are used to describe 4803 movies in our dataset. Let&#39;s see how the TFIDF matrix looks like! . #Convert TFIDF matrix to Pandas Dataframe if you want to see the word frequencies. doc_term_matrix = tfidf_matrix.todense() df = pd.DataFrame(doc_term_matrix, columns=tfidf.get_feature_names(), index=data.overview) df.to_csv(&#39;movies_database_tfidf.csv&#39;, index=True) . df.head() . 00 000 007 07am 10 100 1000 101 108 10th ... zuckerberg zula zuzu zyklon æon éloigne émigré été única über . overview . In the 22nd century, a paraplegic Marine is dispatched to the moon Pandora on a unique mission, but becomes torn between following orders and protecting an alien civilization. | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . Captain Barbossa, long believed to be dead, has come back to life and is headed to the edge of the Earth with Will Turner and Elizabeth Swann. But nothing is quite as it seems. | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . A cryptic message from Bond’s past sends him on a trail to uncover a sinister organization. While M battles political forces to keep the secret service alive, Bond peels back the layers of deceit to reveal the terrible truth behind SPECTRE. | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . Following the death of District Attorney Harvey Dent, Batman assumes responsibility for Dent&#39;s crimes to protect the late attorney&#39;s reputation and is subsequently hunted by the Gotham City Police Department. Eight years later, Batman encounters the mysterious Selina Kyle and the villainous Bane, a new terrorist leader who overwhelms Gotham&#39;s finest. The Dark Knight resurfaces to protect a city that has branded him an enemy. | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . John Carter is a war-weary, former military captain who&#39;s inexplicably transported to the mysterious and exotic planet of Barsoom (Mars) and reluctantly becomes embroiled in an epic conflict. It&#39;s a world on the brink of collapse, and Carter rediscovers his humanity when he realizes the survival of Barsoom and its people rests in his hands. | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 rows × 20978 columns . Computing Similarity Score using Cosine Similarity . With this matrix in hand, we can now compute a similarity score. We will be using the cosine similarity to calculate a numeric quantity that denotes the similarity between two movies. . Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together. The smaller the angle, higher the cosine similarity. . . Reference - https://www.machinelearningplus.com/nlp/cosine-similarity/ . # Compute Cosine Similarity from sklearn.metrics.pairwise import cosine_similarity cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix) print(cosine_sim) . [[1. 0. 0. ... 0. 0. 0. ] [0. 1. 0. ... 0.02160533 0. 0. ] [0. 0. 1. ... 0.01488159 0. 0. ] ... [0. 0.02160533 0.01488159 ... 1. 0.01609091 0.00701914] [0. 0. 0. ... 0.01609091 1. 0.01171696] [0. 0. 0. ... 0.00701914 0.01171696 1. ]] . #Let&#39;s create a dataframe of the similarity matrix with rows and columns as movie titles sim = pd.DataFrame(cosine_sim, columns=data.movie_title, index=data.movie_title) sim.head() . movie_title Avatar Pirates of the Caribbean: At World&#39;s End Spectre The Dark Knight Rises John Carter Spider-Man 3 Tangled Avengers: Age of Ultron Harry Potter and the Half-Blood Prince Batman v Superman: Dawn of Justice ... On The Downlow Sanctuary: Quite a Conundrum Bang Primer Cavite El Mariachi Newlyweds Signed, Sealed, Delivered Shanghai Calling My Date with Drew . movie_title . Avatar | 1.000000 | 0.000000 | 0.0 | 0.024995 | 0.000000 | 0.030353 | 0.000000 | 0.037581 | 0.000000 | 0.000000 | ... | 0.000000 | 0.0 | 0.029175 | 0.042176 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | . Pirates of the Caribbean: At World&#39;s End | 0.000000 | 1.000000 | 0.0 | 0.000000 | 0.033369 | 0.000000 | 0.000000 | 0.022676 | 0.000000 | 0.000000 | ... | 0.000000 | 0.0 | 0.006895 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.021605 | 0.000000 | 0.000000 | . Spectre | 0.000000 | 0.000000 | 1.0 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.030949 | 0.024830 | 0.000000 | ... | 0.027695 | 0.0 | 0.000000 | 0.000000 | 0.017768 | 0.0 | 0.0 | 0.014882 | 0.000000 | 0.000000 | . The Dark Knight Rises | 0.024995 | 0.000000 | 0.0 | 1.000000 | 0.010433 | 0.005145 | 0.012601 | 0.026954 | 0.020652 | 0.133740 | ... | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.033864 | 0.042752 | 0.022692 | . John Carter | 0.000000 | 0.033369 | 0.0 | 0.010433 | 1.000000 | 0.000000 | 0.009339 | 0.037407 | 0.000000 | 0.017148 | ... | 0.012730 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.006126 | 0.000000 | 0.000000 | . 5 rows × 4803 columns . We have now computed the similarity score of each movie with all the other movies based on plot description. Please note, similarity of the movie with itself is 1 and this can be seen in the diagonal in the above matrix. . Implementing the Recommendation System . Let&#39;s implement a recommendation system where we can input a movie title and the model returns the top 10 movies similar to the movie. . # Create a column of movie titles indices = pd.Series(data.index, index=data[&#39;movie_title&#39;]).drop_duplicates() print(indices) . movie_title Avatar 0 Pirates of the Caribbean: At World&#39;s End 1 Spectre 2 The Dark Knight Rises 3 John Carter 4 ... El Mariachi 4798 Newlyweds 4799 Signed, Sealed, Delivered 4800 Shanghai Calling 4801 My Date with Drew 4802 Length: 4803, dtype: int64 . # Function that takes in movie title as input and outputs most similar movies def get_recommendations(title, cosine_sim=cosine_sim): # Get the index of the movie that matches the title idx = indices[title] # Get the pairwise similarity scores of all movies with that movie sim_scores = list(enumerate(cosine_sim[idx])) # Sort the movies in descending order of similarity scores sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) # Get the scores of the 10 most similar movies ignoring the first one as it is itself movie sim_scores = sim_scores[1:11] # Get the movie indices movie_indices = [i[0] for i in sim_scores] # Return the top 10 most similar movies return data[&#39;movie_title&#39;].iloc[movie_indices] . get_recommendations(&#39;The Godfather&#39;) . 2731 The Godfather: Part II 1873 Blood Ties 867 The Godfather: Part III 3727 Easy Money 3623 Made 3125 Eulogy 3896 Sinister 4506 The Maid&#39;s Room 3783 Joe 2244 The Cold Light of Day Name: movie_title, dtype: object . We can see the model did a good job in finding the Godfather trilogy movies and other crime movies such as &#39;Blood Ties&#39; . However, it can be further improved by the following: . Including other features such as Director, Genre, etc. : People interested in &#39;The Godfather&#39; may be more interested in movies directed by Francis Ford Coppola. Let&#39;s try to include this information too in our model. | People might be interested in different genres based on the movie watched by other users. We can solve this using collaborative filtering, which will be discussed in Part 3 of this blog. | . Cast, Genres and Keywords Based Recommendation . Now we will build our model based on top 3 actors in the movie, director, top 3 genres of the movie and top 3 keywords of the movie. First, let&#39;s load our dataset. . #Let&#39;s load our data data.head() . movie_title overview cast genres keywords director . 0 | Avatar | In the 22nd century, a paraplegic Marine is di... | [&#39;Sam Worthington&#39;, &#39;Zoe Saldana&#39;, &#39;Sigourney ... | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Fantasy&#39;] | [&#39;culture clash&#39;, &#39;future&#39;, &#39;space war&#39;] | James Cameron | . 1 | Pirates of the Caribbean: At World&#39;s End | Captain Barbossa, long believed to be dead, ha... | [&#39;Johnny Depp&#39;, &#39;Orlando Bloom&#39;, &#39;Keira Knight... | [&#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Action&#39;] | [&#39;ocean&#39;, &#39;drug abuse&#39;, &#39;exotic island&#39;] | Gore Verbinski | . 2 | Spectre | A cryptic message from Bond’s past sends him o... | [&#39;Daniel Craig&#39;, &#39;Christoph Waltz&#39;, &#39;Léa Seydo... | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Crime&#39;] | [&#39;spy&#39;, &#39;based on novel&#39;, &#39;secret agent&#39;] | Sam Mendes | . 3 | The Dark Knight Rises | Following the death of District Attorney Harve... | [&#39;Christian Bale&#39;, &#39;Michael Caine&#39;, &#39;Gary Oldm... | [&#39;Action&#39;, &#39;Crime&#39;, &#39;Drama&#39;] | [&#39;dc comics&#39;, &#39;crime fighter&#39;, &#39;terrorist&#39;] | Christopher Nolan | . 4 | John Carter | John Carter is a war-weary, former military ca... | [&#39;Taylor Kitsch&#39;, &#39;Lynn Collins&#39;, &#39;Samantha Mo... | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Science Fiction&#39;] | [&#39;based on novel&#39;, &#39;mars&#39;, &#39;medallion&#39;] | Andrew Stanton | . Data Cleaning . Firstly, let&#39;s clean our data by converting all the text into lowercase and removing spaces in a single name. Example: Christian Bale would be converted to christianbale . # Function to convert all strings to lower case and strip names of spaces def clean_data(x): if isinstance(x, list): return [str.lower(i.replace(&quot; &quot;, &quot;&quot;)) for i in x] else: #Check if director exists. If not, return empty string if isinstance(x, str): return str.lower(x.replace(&quot; &quot;, &quot;&quot;)) else: return &#39;&#39; . # Apply clean_data function to your features. features = [&#39;cast&#39;, &#39;keywords&#39;, &#39;director&#39;, &#39;genres&#39;] for feature in features: data[feature] = data[feature].apply(clean_data) . Let&#39;s now combine all the feature data into a single string which combines all the metadata (such as actors, director, keywords and genres) to be feed into the count vectorizer. . def create_combined_features(x): return &#39; &#39;.join(x[&#39;keywords&#39;]) + &#39; &#39; + &#39; &#39;.join(x[&#39;cast&#39;]) + &#39; &#39; + x[&#39;director&#39;] + &#39; &#39; + &#39; &#39;.join(x[&#39;genres&#39;]) data[&#39;combined_features&#39;] = data.apply(create_combined_features, axis=1) . Creating a Count Vectorizer . Now, we have a combined features column in our data. We now apply the count vectorizer which creates a word vector of the entire corpus and provides the frequency of the each word in the document. . # Import CountVectorizer and create the count matrix from sklearn.feature_extraction.text import CountVectorizer count = CountVectorizer(stop_words=&#39;english&#39;) count_matrix = count.fit_transform(data[&#39;combined_features&#39;]) count_matrix.shape . (4803, 2469) . We can see that 2469 unique words are used to describe 4803 movies in our dataset. Let&#39;s see how the Count matrix looks like! . #Convert count matrix to Pandas Dataframe if you want to see the word frequencies. doc_term_matrix = count_matrix.todense() df = pd.DataFrame(doc_term_matrix, columns=count.get_feature_names(), index=data.combined_features) df.to_csv(&#39;movies_database_countmatrix.csv&#39;, index=True) . df.head() . aaronhann aaronschneider abelferrara abrams adambrooks adamcarolla adamgoldberg adamgreen adamjayepstein adammarcus ... zackward zakpenn zalbatmanglij zhangyimou zoranlisinac àlexpastor álexdelaiglesia émilegaudreault érictessier étiennefaure . combined_features . [ &#39; c u l t u r e c l a s h &#39; , &#39; f u t u r e &#39; , &#39; s p a c e w a r &#39; ] [ &#39; s a m w o r t h i n g t o n &#39; , &#39; z o e s a l d a n a &#39; , &#39; s i g o u r n e y w e a v e r &#39; ] jamescameron [ &#39; a c t i o n &#39; , &#39; a d v e n t u r e &#39; , &#39; f a n t a s y &#39; ] | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . [ &#39; o c e a n &#39; , &#39; d r u g a b u s e &#39; , &#39; e x o t i c i s l a n d &#39; ] [ &#39; j o h n n y d e p p &#39; , &#39; o r l a n d o b l o o m &#39; , &#39; k e i r a k n i g h t l e y &#39; ] goreverbinski [ &#39; a d v e n t u r e &#39; , &#39; f a n t a s y &#39; , &#39; a c t i o n &#39; ] | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . [ &#39; s p y &#39; , &#39; b a s e d o n n o v e l &#39; , &#39; s e c r e t a g e n t &#39; ] [ &#39; d a n i e l c r a i g &#39; , &#39; c h r i s t o p h w a l t z &#39; , &#39; l é a s e y d o u x &#39; ] sammendes [ &#39; a c t i o n &#39; , &#39; a d v e n t u r e &#39; , &#39; c r i m e &#39; ] | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . [ &#39; d c c o m i c s &#39; , &#39; c r i m e f i g h t e r &#39; , &#39; t e r r o r i s t &#39; ] [ &#39; c h r i s t i a n b a l e &#39; , &#39; m i c h a e l c a i n e &#39; , &#39; g a r y o l d m a n &#39; ] christophernolan [ &#39; a c t i o n &#39; , &#39; c r i m e &#39; , &#39; d r a m a &#39; ] | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . [ &#39; b a s e d o n n o v e l &#39; , &#39; m a r s &#39; , &#39; m e d a l l i o n &#39; ] [ &#39; t a y l o r k i t s c h &#39; , &#39; l y n n c o l l i n s &#39; , &#39; s a m a n t h a m o r t o n &#39; ] andrewstanton [ &#39; a c t i o n &#39; , &#39; a d v e n t u r e &#39; , &#39; s c i e n c e f i c t i o n &#39; ] | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 rows × 2469 columns . Computing Similarity Score using Cosine Similarity . # Compute the Cosine Similarity matrix based on the count_matrix from sklearn.metrics.pairwise import cosine_similarity cosine_sim2 = cosine_similarity(count_matrix, count_matrix) . #Let&#39;s create a dataframe of the similarity matrix with rows and columns as movie titles sim = pd.DataFrame(cosine_sim2, columns=data.movie_title, index=data.movie_title) sim.head() . movie_title Avatar Pirates of the Caribbean: At World&#39;s End Spectre The Dark Knight Rises John Carter Spider-Man 3 Tangled Avengers: Age of Ultron Harry Potter and the Half-Blood Prince Batman v Superman: Dawn of Justice ... On The Downlow Sanctuary: Quite a Conundrum Bang Primer Cavite El Mariachi Newlyweds Signed, Sealed, Delivered Shanghai Calling My Date with Drew . movie_title . Avatar | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . Pirates of the Caribbean: At World&#39;s End | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . Spectre | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . The Dark Knight Rises | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . John Carter | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 rows × 4803 columns . We have now computed the similarity score of each movie with all the other movies based on actors, director, keywords and genres. Please note, similarity of the movie with itself is 1 and this can be seen in the diagonal of the above matrix. . Implementing the Recommendation System . # Reset index of our main DataFrame and construct reverse mapping as before data = data.reset_index() indices = pd.Series(data.index, index=data[&#39;movie_title&#39;]) . get_recommendations(&#39;The Godfather&#39;, cosine_sim2) . 1018 The Cotton Club 1167 Dracula 1209 The Rainmaker 1525 Apocalypse Now 2333 Peggy Sue Got Married 2600 New York Stories 2731 The Godfather: Part II 3012 The Outsiders 3337 The Godfather 3401 Twixt Name: movie_title, dtype: object . We can see the model did a good job in finding movies similar to &#39;The Godfather&#39;. Most of the movies are directed by Francis Ford Coppola. Majority of movies theme is Crime/Thriller. . The recommender can be improved further by adding more features such as production_company such as DC or Marvel, release date, etc. . Endnotes . I hope this has helped to understand the implementation of content-based filtering using the dummy dataset of ~5000 English Movies. Feel free to play around with the code by opening in Colab or cloning the repo in github. . As we see, the content-based method only has to analyze the items and a single user’s profile for the recommendation, which makes the process less cumbersome. Content-based filtering would thus produce more reliable results with fewer users in the system. However, if the content doesn’t contain enough information to discriminate the items precisely, the recommendation itself risks being imprecise. This can be somewhat overcome with Collaborating Filtering which provides recommendations based similarities in the purchase behavior of users. We will discuss this method in last part of this blog series. . If you have any comments or suggestions please comment below or reach out to me at - Twitter or LinkedIn .",
            "url": "https://rahuls0959.github.io/ds-blog/python/recommendation%20system/relevancy/collaborative%20filtering/content-based%20filtering/demographic%20filtering/2020/06/06/_Recommendation-System_Part-2.html",
            "relUrl": "/python/recommendation%20system/relevancy/collaborative%20filtering/content-based%20filtering/demographic%20filtering/2020/06/06/_Recommendation-System_Part-2.html",
            "date": " • Jun 6, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Recommendation Systems - Demographic Filtering (Part 1)",
            "content": "Introduction . Recommendation systems have become central to most of the popular apps such as Netflix, Spotify, Flipkart, etc. These provide better recommendation to the user based on past search history or keyword searched by the user or similarity of the user to the other users&#39; behavior on the platform. Recommendation systems try to find the relevance of the content or item for the user by proving a scoring mechanism for each user-item pair. The targeted recommendations are used to drive more engagement on the platform which in turn would lead be more retention and revenue opportunity for these apps. . Types of Recommendation Systems . Demographic Filtering: This provides the same recommendation to all the audience based on popularity, rating or genre of the item. We can see this type of recommendations in sections such as &#39;Trending Now&#39;, &#39;Popular on Netflix&#39;, etc. This is one of the most simple form of filtering. This assumes that items which are more popular or critically acclaimed will have higher chances of being liked by the general audience. | . . Content Based Filtering: This suggests similar items based on a particular item. This uses the item&#39;s similarity with the previously liked or saved items of the user based on item&#39;s metadata or current search keywords used by user to find some item. This assumes that a user has higher chances to like similar items based on previous history of the user. Also, this helps in giving relevant recommendations to the user based on searched keywords. | . . Collaborating Filtering: This system matches persons with similar interests and provides recommendations based on this matching. Collaborative filters do not require item metadata like in content-based filtering. This kind of recommendations implementation can be seen in Amazon or Flipkart under &#39;Frequently Bought Together&#39; section. | . . Data Preparation . We have taken the data of ~5000 English Movies from Kaggle (https://www.kaggle.com/tmdb/tmdb-movie-metadata). With this data, we would try to understand each of three types of recommendation systems by developing Machine Learning models in Python. . The first dataset &#39;tmdb_5000_credits&#39; contains the following features: . movie_id - A unique identifier for each movie | cast - The name of lead and supporting actors | crew - The name of Director, Editor, Composer, Writer etc. | . The second dataset &#39;tmdb_5000_movies&#39; has the following features: . budget - The budget in which the movie was made | genre - The genre of the movie (Action, Comedy ,Thriller etc.) | homepage - A link to the homepage of the movie | id - This is infact the movie_id as in the first dataset | keywords - The keywords or tags related to the movie | original_language - The language in which the movie was made | original_title - The title of the movie before translation or adaptation | overview - A brief description of the movie | popularity - A numeric quantity specifying the movie popularity at the moment | production_companies - The production house of the movie | production_countries - The country in which it was produced | release_date - The date on which it was released | revenue - The worldwide revenue generated by the movie | runtime - The running time of the movie in minutes | status - &quot;Released&quot; or &quot;Rumored&quot; | tagline - Movie&#39;s tagline | title - Title of the movie | vote_average - average ratings the movie recieved | vote_count - the count of votes recieved | . import pandas as pd import numpy as np import warnings warnings.filterwarnings(&quot;ignore&quot;) df1=pd.read_csv(&#39;tmdb-movie-metadata/tmdb_5000_credits.csv&#39;) df2=pd.read_csv(&#39;tmdb-movie-metadata/tmdb_5000_movies.csv&#39;) . We can now join the two datasets on &#39;id&#39; column. . df1.columns = [&#39;id&#39;,&#39;movie_title&#39;,&#39;cast&#39;,&#39;crew&#39;] df2= df2.merge(df1,on=&#39;id&#39;) . Let&#39;s see how the combined data looks like! . df2.shape . (4803, 23) . The database contains 4803 movies and 23 features or variables. . df2.head(5) . budget genres homepage id keywords original_language original_title overview popularity production_companies ... runtime spoken_languages status tagline title vote_average vote_count movie_title cast crew . 0 | 237000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | http://www.avatarmovie.com/ | 19995 | [{&quot;id&quot;: 1463, &quot;name&quot;: &quot;culture clash&quot;}, {&quot;id&quot;:... | en | Avatar | In the 22nd century, a paraplegic Marine is di... | 150.437577 | [{&quot;name&quot;: &quot;Ingenious Film Partners&quot;, &quot;id&quot;: 289... | ... | 162.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}, {&quot;iso... | Released | Enter the World of Pandora. | Avatar | 7.2 | 11800 | Avatar | [{&quot;cast_id&quot;: 242, &quot;character&quot;: &quot;Jake Sully&quot;, &quot;... | [{&quot;credit_id&quot;: &quot;52fe48009251416c750aca23&quot;, &quot;de... | . 1 | 300000000 | [{&quot;id&quot;: 12, &quot;name&quot;: &quot;Adventure&quot;}, {&quot;id&quot;: 14, &quot;... | http://disney.go.com/disneypictures/pirates/ | 285 | [{&quot;id&quot;: 270, &quot;name&quot;: &quot;ocean&quot;}, {&quot;id&quot;: 726, &quot;na... | en | Pirates of the Caribbean: At World&#39;s End | Captain Barbossa, long believed to be dead, ha... | 139.082615 | [{&quot;name&quot;: &quot;Walt Disney Pictures&quot;, &quot;id&quot;: 2}, {&quot;... | ... | 169.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}] | Released | At the end of the world, the adventure begins. | Pirates of the Caribbean: At World&#39;s End | 6.9 | 4500 | Pirates of the Caribbean: At World&#39;s End | [{&quot;cast_id&quot;: 4, &quot;character&quot;: &quot;Captain Jack Spa... | [{&quot;credit_id&quot;: &quot;52fe4232c3a36847f800b579&quot;, &quot;de... | . 2 | 245000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | http://www.sonypictures.com/movies/spectre/ | 206647 | [{&quot;id&quot;: 470, &quot;name&quot;: &quot;spy&quot;}, {&quot;id&quot;: 818, &quot;name... | en | Spectre | A cryptic message from Bond’s past sends him o... | 107.376788 | [{&quot;name&quot;: &quot;Columbia Pictures&quot;, &quot;id&quot;: 5}, {&quot;nam... | ... | 148.0 | [{&quot;iso_639_1&quot;: &quot;fr&quot;, &quot;name&quot;: &quot;Fran u00e7ais&quot;},... | Released | A Plan No One Escapes | Spectre | 6.3 | 4466 | Spectre | [{&quot;cast_id&quot;: 1, &quot;character&quot;: &quot;James Bond&quot;, &quot;cr... | [{&quot;credit_id&quot;: &quot;54805967c3a36829b5002c41&quot;, &quot;de... | . 3 | 250000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 80, &quot;nam... | http://www.thedarkknightrises.com/ | 49026 | [{&quot;id&quot;: 849, &quot;name&quot;: &quot;dc comics&quot;}, {&quot;id&quot;: 853,... | en | The Dark Knight Rises | Following the death of District Attorney Harve... | 112.312950 | [{&quot;name&quot;: &quot;Legendary Pictures&quot;, &quot;id&quot;: 923}, {&quot;... | ... | 165.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}] | Released | The Legend Ends | The Dark Knight Rises | 7.6 | 9106 | The Dark Knight Rises | [{&quot;cast_id&quot;: 2, &quot;character&quot;: &quot;Bruce Wayne / Ba... | [{&quot;credit_id&quot;: &quot;52fe4781c3a36847f81398c3&quot;, &quot;de... | . 4 | 260000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | http://movies.disney.com/john-carter | 49529 | [{&quot;id&quot;: 818, &quot;name&quot;: &quot;based on novel&quot;}, {&quot;id&quot;:... | en | John Carter | John Carter is a war-weary, former military ca... | 43.926995 | [{&quot;name&quot;: &quot;Walt Disney Pictures&quot;, &quot;id&quot;: 2}] | ... | 132.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}] | Released | Lost in our world, found in another. | John Carter | 6.1 | 2124 | John Carter | [{&quot;cast_id&quot;: 5, &quot;character&quot;: &quot;John Carter&quot;, &quot;c... | [{&quot;credit_id&quot;: &quot;52fe479ac3a36847f813eaa3&quot;, &quot;de... | . 5 rows × 23 columns . Let&#39;s keep only the most important features from the combined dataset - title, cast, crew, genres, keywords, vote_count, vote_average, popularity . data = df2[[&#39;movie_title&#39;, &#39;cast&#39;,&#39;crew&#39;,&#39;genres&#39;,&#39;keywords&#39;,&#39;overview&#39;,&#39;vote_count&#39;,&#39;vote_average&#39;,&#39;popularity&#39;]] data.head() . movie_title cast crew genres keywords overview vote_count vote_average popularity . 0 | Avatar | [{&quot;cast_id&quot;: 242, &quot;character&quot;: &quot;Jake Sully&quot;, &quot;... | [{&quot;credit_id&quot;: &quot;52fe48009251416c750aca23&quot;, &quot;de... | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | [{&quot;id&quot;: 1463, &quot;name&quot;: &quot;culture clash&quot;}, {&quot;id&quot;:... | In the 22nd century, a paraplegic Marine is di... | 11800 | 7.2 | 150.437577 | . 1 | Pirates of the Caribbean: At World&#39;s End | [{&quot;cast_id&quot;: 4, &quot;character&quot;: &quot;Captain Jack Spa... | [{&quot;credit_id&quot;: &quot;52fe4232c3a36847f800b579&quot;, &quot;de... | [{&quot;id&quot;: 12, &quot;name&quot;: &quot;Adventure&quot;}, {&quot;id&quot;: 14, &quot;... | [{&quot;id&quot;: 270, &quot;name&quot;: &quot;ocean&quot;}, {&quot;id&quot;: 726, &quot;na... | Captain Barbossa, long believed to be dead, ha... | 4500 | 6.9 | 139.082615 | . 2 | Spectre | [{&quot;cast_id&quot;: 1, &quot;character&quot;: &quot;James Bond&quot;, &quot;cr... | [{&quot;credit_id&quot;: &quot;54805967c3a36829b5002c41&quot;, &quot;de... | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | [{&quot;id&quot;: 470, &quot;name&quot;: &quot;spy&quot;}, {&quot;id&quot;: 818, &quot;name... | A cryptic message from Bond’s past sends him o... | 4466 | 6.3 | 107.376788 | . 3 | The Dark Knight Rises | [{&quot;cast_id&quot;: 2, &quot;character&quot;: &quot;Bruce Wayne / Ba... | [{&quot;credit_id&quot;: &quot;52fe4781c3a36847f81398c3&quot;, &quot;de... | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 80, &quot;nam... | [{&quot;id&quot;: 849, &quot;name&quot;: &quot;dc comics&quot;}, {&quot;id&quot;: 853,... | Following the death of District Attorney Harve... | 9106 | 7.6 | 112.312950 | . 4 | John Carter | [{&quot;cast_id&quot;: 5, &quot;character&quot;: &quot;John Carter&quot;, &quot;c... | [{&quot;credit_id&quot;: &quot;52fe479ac3a36847f813eaa3&quot;, &quot;de... | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | [{&quot;id&quot;: 818, &quot;name&quot;: &quot;based on novel&quot;}, {&quot;id&quot;:... | John Carter is a war-weary, former military ca... | 2124 | 6.1 | 43.926995 | . As we can see from the above data, cast, crew, genres and keywords are lists of JSON objects: JSON objects are surrounded by curly braces {}. JSON objects are written in key/value pairs. Keys must be strings, and values must be a valid JSON data type (string, number, object, array, boolean or null). Keys and values are separated by a colon. Each key/value pair is separated by a comma. . We will now extract the values of the first three actor names from the feature &#39;cast&#39; using key &#39;name&#39;, director from feature &#39;crew&#39; using key &#39;job&#39; = Director and key &#39;name&#39;, first three movie genre from feature &#39;genres&#39; using key &#39;name&#39; and first three keywords related to movie from feature &#39;keywords&#39; using key &#39;name&#39;. . ## Extract cast, crew, keywords and genre # Parse the stringified features into their corresponding python objects from ast import literal_eval features = [&#39;cast&#39;, &#39;crew&#39;, &#39;keywords&#39;, &#39;genres&#39;] for feature in features: data[feature] = data[feature].apply(literal_eval) . # Get the first three names from the cast,keywords and genre features. def get_list(x): if isinstance(x, list): names = [i[&#39;name&#39;] for i in x] #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list. if len(names) &gt; 3: names = names[:3] return names #Return empty list in case of missing/malformed data return [] # Get the director&#39;s name from the crew feature. If director is not listed, return NaN def get_director(x): for i in x: if i[&#39;job&#39;] == &#39;Director&#39;: return i[&#39;name&#39;] return np.nan . # Define new director, cast, genres and keywords features that are in a suitable form. data[&#39;director&#39;] = data[&#39;crew&#39;].apply(get_director) features = [&#39;cast&#39;, &#39;keywords&#39;, &#39;genres&#39;] for feature in features: data[feature] = data[feature].apply(get_list) . Let&#39;s look at how the data looks like now. . data.head() . movie_title cast crew genres keywords overview vote_count vote_average popularity director . 0 | Avatar | [Sam Worthington, Zoe Saldana, Sigourney Weaver] | [{&#39;credit_id&#39;: &#39;52fe48009251416c750aca23&#39;, &#39;de... | [Action, Adventure, Fantasy] | [culture clash, future, space war] | In the 22nd century, a paraplegic Marine is di... | 11800 | 7.2 | 150.437577 | James Cameron | . 1 | Pirates of the Caribbean: At World&#39;s End | [Johnny Depp, Orlando Bloom, Keira Knightley] | [{&#39;credit_id&#39;: &#39;52fe4232c3a36847f800b579&#39;, &#39;de... | [Adventure, Fantasy, Action] | [ocean, drug abuse, exotic island] | Captain Barbossa, long believed to be dead, ha... | 4500 | 6.9 | 139.082615 | Gore Verbinski | . 2 | Spectre | [Daniel Craig, Christoph Waltz, Léa Seydoux] | [{&#39;credit_id&#39;: &#39;54805967c3a36829b5002c41&#39;, &#39;de... | [Action, Adventure, Crime] | [spy, based on novel, secret agent] | A cryptic message from Bond’s past sends him o... | 4466 | 6.3 | 107.376788 | Sam Mendes | . 3 | The Dark Knight Rises | [Christian Bale, Michael Caine, Gary Oldman] | [{&#39;credit_id&#39;: &#39;52fe4781c3a36847f81398c3&#39;, &#39;de... | [Action, Crime, Drama] | [dc comics, crime fighter, terrorist] | Following the death of District Attorney Harve... | 9106 | 7.6 | 112.312950 | Christopher Nolan | . 4 | John Carter | [Taylor Kitsch, Lynn Collins, Samantha Morton] | [{&#39;credit_id&#39;: &#39;52fe479ac3a36847f813eaa3&#39;, &#39;de... | [Action, Adventure, Science Fiction] | [based on novel, mars, medallion] | John Carter is a war-weary, former military ca... | 2124 | 6.1 | 43.926995 | Andrew Stanton | . #Let&#39;s store this in a CSV for further use data.to_csv(&#39;movies_database.csv&#39;, index=False) . Demographic Filtering . Let&#39;s start with demographic filtering. We would need a metric to score the items and rank them accordingly. Every platform or app has their own way of evaluating this metric. Platforms can take the reliable data sources to pull out the information on items (such as ratings, votes etc.), calculate the score for each item, rank them in descending order and show the top items to the users. . Recommendation based on highest ratings calculated using total votes . For the above movie database, we can use the average rating provided by the voters to rank, however this has a major flaw in terms of skewness due to movies with very low number of votes. For example, movie with 8.5 rating with only 3 votes may not be better than movie with 7.8 rating with 100 votes. . In order to go around this, I have taken the weighted rating approach followed by IMDB to rate the movies in this tutorial. . . where, . v is the number of votes for the movie; | m is the minimum votes required to be listed in the chart; | R is the average rating of the movie; And | C is the mean vote across the whole report | . We already have values of v(vote_count) and R(vote_average). C can be calculated from the data. . Let&#39;s calculate the mean rating across the whole database of 4803 movies. . C= data[&#39;vote_average&#39;].mean() C . 6.092171559442011 . So, the mean rating for all the movies is approx 6 on a scale of 10. . The next step is to determine an appropriate value for m, the minimum votes required to be listed in the chart. Let&#39;s assume the cutoff is 70th percentile. In other words, for a movie to feature in the charts, it must have more votes than at least 70% of the movies in the list. . m= data[&#39;vote_count&#39;].quantile(0.7) m . 581.0 . Hence, for a movie to feature, it must have minimum of 581 votes. . Let&#39;s filter out the movies with this criteria. We can then calculate the weighted rating for the qualified movies using the formula mentioned above . q_movies = data.copy().loc[data[&#39;vote_count&#39;] &gt;= m] q_movies.shape . (1442, 10) . We see that there are 1442 movies which qualify to be in this list. . Now, we need to calculate our metric for each qualified movie. To do this, we will define a function, weighted_rating() and define a new feature score, of which we&#39;ll calculate the value by applying this function to our DataFrame of qualified movies; . def weighted_rating(x, m=m, C=C): v = x[&#39;vote_count&#39;] R = x[&#39;vote_average&#39;] # Calculation based on the IMDB formula return (v/(v+m) * R) + (m/(m+v) * C) . # Define a new feature &#39;score&#39; and calculate its value with `weighted_rating()` q_movies[&#39;score&#39;] = q_movies.apply(weighted_rating, axis=1) . Let&#39;s look at the top 10 highest rated movies! . #Sort movies based on score calculated above q_movies = q_movies.sort_values(&#39;score&#39;, ascending=False) #Print the top 10 movies q_movies[[&#39;movie_title&#39;, &#39;vote_count&#39;, &#39;vote_average&#39;, &#39;score&#39;]].head(10) . movie_title vote_count vote_average score . 1881 | The Shawshank Redemption | 8205 | 8.5 | 8.340775 | . 3337 | The Godfather | 5893 | 8.4 | 8.192887 | . 662 | Fight Club | 9413 | 8.3 | 8.171648 | . 3232 | Pulp Fiction | 8428 | 8.3 | 8.157615 | . 65 | The Dark Knight | 12002 | 8.2 | 8.102674 | . 809 | Forrest Gump | 7927 | 8.2 | 8.056059 | . 1818 | Schindler&#39;s List | 4329 | 8.3 | 8.038748 | . 3865 | Whiplash | 4254 | 8.3 | 8.034695 | . 96 | Inception | 13752 | 8.1 | 8.018611 | . 1990 | The Empire Strikes Back | 5879 | 8.2 | 8.010426 | . #Let&#39;s create the plot of top 10 movies based on weighted score import matplotlib.pyplot as plt plt.figure(figsize=(12,8)) plt.barh(q_movies[&#39;movie_title&#39;].head(10),q_movies[&#39;score&#39;].head(10), align=&#39;center&#39;,color=&#39;skyblue&#39;) plt.gca().invert_yaxis() plt.xlabel(&quot;Weighted Score&quot;, weight=&#39;bold&#39;) plt.title(&quot;Best Rated Movies&quot;,weight=&#39;bold&#39;) . Text(0.5, 1.0, &#39;Best Rated Movies&#39;) . We can see the classics such as &#39;The GodFather&#39;, &#39;The Shawshank Redemption&#39;, etc. in the above list. These are the best movies of all times and our recommendation system might work in case we want to show the greatest hits of all time based on votes. . Recommendation based on maximum popularity at the moment . In case we want to show the movies currently popular among the app users, we would need the popularity score for each movie, rank them in descending order and show the movies with highest popularity score to the users. . For the above movies database, popularity score has been already calculated and available in the feature &#39;popularity&#39;. Each app build their popularity ratings differently. In the data provided, it has been built using the following information: . Number of votes for the day | Number of views for the day | Number of users who marked it as a &quot;favourite&quot; for the day | Number of users who added it to their &quot;watchlist&quot; for the day | Release date | Number of total votes | Previous days score | . Details in the link - https://developers.themoviedb.org/3/getting-started/popularity . Let&#39;s look at the top 10 movies based on popularity! . pop= data.sort_values(&#39;popularity&#39;, ascending=False) import matplotlib.pyplot as plt plt.figure(figsize=(12,8)) plt.barh(pop[&#39;movie_title&#39;].head(10),pop[&#39;popularity&#39;].head(10), align=&#39;center&#39;,color=&#39;skyblue&#39;) plt.gca().invert_yaxis() plt.xlabel(&quot;Popularity Score&quot;, weight=&#39;bold&#39;) plt.title(&quot;Most Popular Movies&quot;,weight=&#39;bold&#39;) . Text(0.5, 1.0, &#39;Most Popular Movies&#39;) . The above lists has classics as well as movies popular among the masses. The above recommendations logics can be experimented and the best recommendation engine based on superior performance (eg. one with highest engagement of app users) can be deployed. . Endnotes . I hope this has helped in developing a basic understanding of what recommendation systems are, how they work, some of the different types and implementation of demographic filtering using the dummy dataset of ~5000 English Movies. Feel free to play around with the code by opening in Colab or cloning the repo in github. . In the next two blogs, I will work on other two types of recommendation systems - Content-based Filtering and Collaborative Filtering. . If you have any comments or suggestions please comment below or reach out to me at - Twitter or LinkedIn .",
            "url": "https://rahuls0959.github.io/ds-blog/python/recommendation%20system/relevancy/collaborative%20filtering/content-based%20filtering/demographic%20filtering/2020/06/06/_Recommendation-System_Part-1.html",
            "relUrl": "/python/recommendation%20system/relevancy/collaborative%20filtering/content-based%20filtering/demographic%20filtering/2020/06/06/_Recommendation-System_Part-1.html",
            "date": " • Jun 6, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Customer Churn Analysis - Exploratory Data Analysis",
            "content": "Introduction . Retention is one of the most important criteria to understand the product-market fit for any product. Retention in broad sense means the customers who keep using your product. The customers who leave the platform are said to have churned. Understanding churn is extremely important in order to create streategies for delightful customer experience. Machine learning methods can be used to understand the customer behavioral data of product usage, whcih can be used to model the probability of churn by understanding the patterns of customers who have historically churned. . In this blog, we have a data from the telecom operator along with the tag whether the customer is retained or churned. It is very important to understand the data before any model is built on it. We will first do exploratory data analysis in this blog. In the next blog, we will develop classification models using Logistic Regression and Decision Tree algorithms. . Data Source . Data is sourced from the kaggle - https://www.kaggle.com/blastchar/telco-customer-churn. It contains the data for ~7000 telecom customers and have details such as demographics, plan details, usage and churn. Let&#39;s explore the dataset. . Import relevant python libraries in the notebook. . import pandas as pd import warnings warnings.filterwarnings(&quot;ignore&quot;) . Read the data CSV file from the local directory. . df_data = pd.read_csv(&#39;Telco-Customer-Churn.csv&#39;) df_data.head(5) . customerID gender SeniorCitizen Partner Dependents tenure PhoneService MultipleLines InternetService OnlineSecurity ... DeviceProtection TechSupport StreamingTV StreamingMovies Contract PaperlessBilling PaymentMethod MonthlyCharges TotalCharges Churn . 0 | 7590-VHVEG | Female | 0 | Yes | No | 1 | No | No phone service | DSL | No | ... | No | No | No | No | Month-to-month | Yes | Electronic check | 29.85 | 29.85 | No | . 1 | 5575-GNVDE | Male | 0 | No | No | 34 | Yes | No | DSL | Yes | ... | Yes | No | No | No | One year | No | Mailed check | 56.95 | 1889.5 | No | . 2 | 3668-QPYBK | Male | 0 | No | No | 2 | Yes | No | DSL | Yes | ... | No | No | No | No | Month-to-month | Yes | Mailed check | 53.85 | 108.15 | Yes | . 3 | 7795-CFOCW | Male | 0 | No | No | 45 | No | No phone service | DSL | Yes | ... | Yes | Yes | No | No | One year | No | Bank transfer (automatic) | 42.30 | 1840.75 | No | . 4 | 9237-HQITU | Female | 0 | No | No | 2 | Yes | No | Fiber optic | No | ... | No | No | No | No | Month-to-month | Yes | Electronic check | 70.70 | 151.65 | Yes | . 5 rows × 21 columns . As can seen by looking at few rows of the data, it has 21 columns - 1 column with unique identifier for Customer (customerID), 19 features (such as gender, MonthlyCharges, etc.) and 1 target feature (Churn). . Let&#39;s have a further look into the data types for each column. . df_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 7043 entries, 0 to 7042 Data columns (total 21 columns): customerID 7043 non-null object gender 7043 non-null object SeniorCitizen 7043 non-null int64 Partner 7043 non-null object Dependents 7043 non-null object tenure 7043 non-null int64 PhoneService 7043 non-null object MultipleLines 7043 non-null object InternetService 7043 non-null object OnlineSecurity 7043 non-null object OnlineBackup 7043 non-null object DeviceProtection 7043 non-null object TechSupport 7043 non-null object StreamingTV 7043 non-null object StreamingMovies 7043 non-null object Contract 7043 non-null object PaperlessBilling 7043 non-null object PaymentMethod 7043 non-null object MonthlyCharges 7043 non-null float64 TotalCharges 7043 non-null object Churn 7043 non-null object dtypes: float64(1), int64(2), object(18) memory usage: 1.1+ MB . As above, there is no missing fields in our data. Most of the features are categorical in the dataset. However, we see that the TotalCharges datatype is object or categorical and it need to be converted to numerical. . Data Cleaning . df_data = df_data.dropna(how=&#39;all&#39;) # remove samples with all missing values df_data = df_data[~df_data.duplicated()] # remove duplicates total_charges_filter = df_data.TotalCharges == &quot; &quot; df_data = df_data[~total_charges_filter] df_data.TotalCharges = pd.to_numeric(df_data.TotalCharges) #converting data type of Total charges to numeric . df_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 7032 entries, 0 to 7042 Data columns (total 21 columns): customerID 7032 non-null object gender 7032 non-null object SeniorCitizen 7032 non-null int64 Partner 7032 non-null object Dependents 7032 non-null object tenure 7032 non-null int64 PhoneService 7032 non-null object MultipleLines 7032 non-null object InternetService 7032 non-null object OnlineSecurity 7032 non-null object OnlineBackup 7032 non-null object DeviceProtection 7032 non-null object TechSupport 7032 non-null object StreamingTV 7032 non-null object StreamingMovies 7032 non-null object Contract 7032 non-null object PaperlessBilling 7032 non-null object PaymentMethod 7032 non-null object MonthlyCharges 7032 non-null float64 TotalCharges 7032 non-null float64 Churn 7032 non-null object dtypes: float64(2), int64(2), object(17) memory usage: 1.2+ MB . As can be seen above, we have 17 categorical features, three numerical features and one target feature i.e. Churn. Also, the data contains information about 7032 customers. . categorical_features = [ &#39;gender&#39;, &#39;SeniorCitizen&#39;, &#39;Partner&#39;, &#39;Dependents&#39;, &#39;PhoneService&#39;, &#39;MultipleLines&#39;, &#39;InternetService&#39;, &#39;OnlineSecurity&#39;, &#39;OnlineBackup&#39;, &#39;DeviceProtection&#39;, &#39;TechSupport&#39;, &#39;StreamingTV&#39;, &#39;StreamingMovies&#39;, &#39;Contract&#39;, &#39;PaperlessBilling&#39;, &#39;PaymentMethod&#39;, ] numerical_features = [ &#39;tenure&#39;, &#39;MonthlyCharges&#39;, &#39;TotalCharges&#39; ] target = &#39;Churn&#39; . Data Visualization . Python Libraries . Let&#39;s import the relevant Python data visualization libraries in the notebook. . matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html. It provides a nice MATLAB-like plotting framework to create data visualizations in Python. | Seaborn: https://seaborn.pydata.org/introduction.html. It is a library to make statistical graphics in Python. It is built on top of matplotlib and closely integrated with pandas data structures. | Gridspec: https://matplotlib.org/3.2.1/tutorials/intermediate/gridspec.html. It provides the function to create multiple grids and specifies subplots with number of rows and number of columns in a grid. | . import matplotlib.pyplot as plt import seaborn as sns import matplotlib.gridspec as gridspec . Target Variable . &quot;Churn&quot; is our target variable. It has the values whether customer churned or not (Yes, No). . Let&#39;s see how many customers have churned. . It is important to have significant proportion of customers having churned/ not churned for the classification model to perform better. In case the number is not significant for any class, more data needs to be taken or while splitting the data into training and validation sets, it needs to be ensured that sufficient number of customers are available for both classes in the split data. . def plot_target_dist(df): fig = plt.figure(figsize = (10, 5)) plt.subplot(121) plt.pie(df.Churn.value_counts(),labels = [&#39;No Churn&#39;, &#39;Churn&#39;], autopct = &#39;%.1f%%&#39;, radius = 1, textprops={&#39;fontsize&#39;: 10, &#39;fontweight&#39;: &#39;bold&#39;}) plt.title(&#39;Churn Outcome Pie Chart&#39;, fontsize = 15, fontweight = &#39;bold&#39;) plt.subplot(122) t = sns.countplot(df.Churn) t.set_xlabel(&#39;Churn&#39;, fontsize = 10, fontweight = &#39;bold&#39;) t.set_ylabel(&#39;Count&#39;, fontsize = 10, fontweight = &#39;bold&#39;) plt.title(&#39;Churn Outcome Distribution&#39;, fontsize = 15, fontweight = &#39;bold&#39;) plt.tight_layout() plot_target_dist(df_data) . The above pie chart and bar chart denotes the distribution of Churned and Not Churned Customers: . We have around 73% of customers (nearly 5000 customers) who are retained with the telecom company and still availing the company&#39;s services. Around 27% customers (nearly 2000 customers) have churned and are not using any services of the telecom company. The split between churned and not churned is significant with good number of customers present in both classes. . Numerical Features . There are three numerical features in the dataset: . Tenure - Number of months the customer has been with the company | MonthlyCharges - The monthly amount charged to the customer | TotalCharges - The total amount charged to the customer | Let us check their distribution using the histogram. . df_data[numerical_features].hist(bins=30, figsize=(15, 8)) plt.show() . From the above histograms, . Most of the customers fall in the tenure of less than 5 months or greather than 70 months with nearly uniform distribution in between these ranges. | Most of the customers Monthly Charges is extremely low. | Let&#39;s also look at how churn varies with the tenure of the customer. . The initial assumption can be that as the customer tenure increases (or customer is availing the company&#39;s services for longer time period, the probability of churn decreases). Let&#39;s check this assumption. . We can create a copy of our data and replace the classes in &quot;Churn&quot; column with binary variables: 0= No Churn, 1= Churn. The mean of this field would provide the churn rate. The scatter plot of &quot;Tenure vs Churn&quot; would help in visualization of how churn is affected by tenure. . copy = df_data.copy() copy.loc[copy.Churn==&#39;No&#39;,&#39;Churn&#39;] = 0 copy.loc[copy.Churn==&#39;Yes&#39;,&#39;Churn&#39;] = 1 . plot1 = copy.groupby(&#39;tenure&#39;).Churn.mean().reset_index() plot1.plot(kind=&quot;scatter&quot;, x=&#39;tenure&#39;, y=&#39;Churn&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2332ab27cc8&gt; . The initial assumption is verified. Churn in-fact decreases as the tenure increases as seen from the downward trend in the above scatter plot. . However, we will be able to get better insights by clubbing the tenure months into groups. . def tenure_groups(df): if df.tenure &lt;= 12: return &quot;less_than_1&quot; elif (df.tenure &gt; 12) &amp; (df.tenure &lt;= 24): return &quot;less_than_2&quot; elif (df.tenure &gt; 24) &amp; (df.tenure &lt;= 36): return &quot;less_than_3&quot; elif (df.tenure &gt; 36) &amp; (df.tenure &lt;= 48): return &quot;less_than_4&quot; elif (df.tenure &gt; 48) &amp; (df.tenure &lt;= 60): return &quot;less_than_5&quot; else: return &quot;greater_than_5&quot; df_data[&#39;grouped_tenure&#39;] = df_data.apply(tenure_groups, axis=1) df_data.head(5) . customerID gender SeniorCitizen Partner Dependents tenure PhoneService MultipleLines InternetService OnlineSecurity ... TechSupport StreamingTV StreamingMovies Contract PaperlessBilling PaymentMethod MonthlyCharges TotalCharges Churn grouped_tenure . 0 | 7590-VHVEG | Female | 0 | Yes | No | 1 | No | No phone service | DSL | No | ... | No | No | No | Month-to-month | Yes | Electronic check | 29.85 | 29.85 | No | less_than_1 | . 1 | 5575-GNVDE | Male | 0 | No | No | 34 | Yes | No | DSL | Yes | ... | No | No | No | One year | No | Mailed check | 56.95 | 1889.50 | No | less_than_3 | . 2 | 3668-QPYBK | Male | 0 | No | No | 2 | Yes | No | DSL | Yes | ... | No | No | No | Month-to-month | Yes | Mailed check | 53.85 | 108.15 | Yes | less_than_1 | . 3 | 7795-CFOCW | Male | 0 | No | No | 45 | No | No phone service | DSL | Yes | ... | Yes | No | No | One year | No | Bank transfer (automatic) | 42.30 | 1840.75 | No | less_than_4 | . 4 | 9237-HQITU | Female | 0 | No | No | 2 | Yes | No | Fiber optic | No | ... | No | No | No | Month-to-month | Yes | Electronic check | 70.70 | 151.65 | Yes | less_than_1 | . 5 rows × 22 columns . Let&#39;s save this data file for further use. . df_data.to_csv(&#39;Telco-Customer-Churn-Final.csv&#39;, index=False) . We can see the count of customers who have churned and not churned for each of these tenure groups. . def tenure_group_counts(df): plt.figure(figsize = (10,5)) t = sns.countplot(data = df, x = &#39;grouped_tenure&#39;, hue = &#39;Churn&#39;, order = [&#39;less_than_1&#39;, &#39;less_than_2&#39;, &#39;less_than_3&#39;, &#39;less_than_4&#39;, &#39;less_than_5&#39;, &#39;greater_than_5&#39;]) t.set_title(&#39;Churn Count by Tenure Groups&#39;, fontsize = 20, fontweight = &#39;bold&#39;) t.set_xlabel(&#39;Tenure Groups&#39;,fontsize = 10, fontweight = &#39;bold&#39;, labelpad = 1.5) t.set_ylabel(&#39;Count&#39;, fontsize = 10, fontweight = &#39;bold&#39;) t.legend(loc = &#39;upper left&#39;, fontsize = 10, labels = [&#39;No Churn&#39;, &#39;Churn&#39;], edgecolor = &#39;black&#39;) plt.tight_layout() tenure_group_counts(df_data) . We can see that most of the customers fall in tenure group of &quot;less_than_1&quot; or &quot;greater_than_5&quot; years and proportion of customers churning is higher in lower tenure groups. . We can also look at the plot of average monthly and total charges for different tenure groups for churned and not churned customers. . def plot_numerical_averages(df, feature): fig = plt.figure(figsize = (10, 5)) b = sns.barplot(data = df, x = &#39;grouped_tenure&#39;, y = feature, hue = &#39;Churn&#39;, order = [&#39;less_than_1&#39;, &#39;less_than_2&#39;, &#39;less_than_3&#39;, &#39;less_than_4&#39;, &#39;less_than_5&#39;, &#39;greater_than_5&#39;]) b.set_xlabel(&#39;Tenure Groups&#39;, fontsize = 10, fontweight = &#39;bold&#39;) b.set_ylabel(f&#39;{feature} ($)&#39;, fontsize = 10, fontweight = &#39;bold&#39;) b.set_title(f&#39;Average {feature} by Tenure Group&#39;, fontsize = 20, fontweight = &#39;bold&#39;) b.legend(fontsize = 10, loc = &#39;upper left&#39;, edgecolor = &#39;black&#39;) plt.tight_layout() plot_numerical_averages(df_data,&#39;MonthlyCharges&#39;) plot_numerical_averages(df_data,&#39;TotalCharges&#39;) . Interesting insights from the above plots: . Customers who churn are mostly likely to have monthly charges greater than $60. | Generally speaking, the likelihood of a customer churning increases as monthly charges increase. | From the total charges plot, we can infer that total charges paid by customers increases with their tenure whihc is pretty normal.. Hence, monthly charges plot is more relevant to understand Churn. | Categorical Features . We have the following 16 categorical features in our dataset: . Gender — M/F | SeniorCitizen — Whether the customer is a senior citizen or not (1, 0) | Partner — Whether customer has a partner or not (Yes, No) | Dependents — Whether customer has dependents or not (Yes, No) | PhoneService — Whether the customer has a phone service or not (Yes, No) | MulitpleLines — Whether the customer has multiple lines or not (Yes, No, No Phone Service) | InternetService — Customer’s internet service type (DSL, Fiber Optic, None) | OnlineSecurity — Whether the customer has Online Security add-on (Yes, No, No Internet Service) | OnlineBackup — Whether the customer has Online Backup add-on (Yes, No, No Internet Service) | DeviceProtection — Whether the customer has Device Protection add-on (Yes, No, No Internet Service) | TechSupport — Whether the customer has Tech Support add-on (Yes, No, No Internet Service) | StreamingTV — Whether the customer has streaming TV or not (Yes, No, No Internet Service) | StreamingMovies — Whether the customer has streaming movies or not (Yes, No, No Internet Service) | Contract — Term of the customer’s contract (Monthly, 1-Year, 2-Year) | PaperlessBilling — Whether the customer has paperless billing or not (Yes, No) | PaymentMethod — The customer’s payment method (E-Check, Mailed Check, Bank Transfer (Auto), Credit Card (Auto)) | Demographic Features . There are four variables related to demographics of the customer - Gender, Age, Partner, Dependents. Let&#39;s look at their distribution in both classes - Churn vs No Churn. . fig = plt.figure(figsize = (30,10)) def plot_gender_dist(df): plt.subplot(141) g = df.copy() g = g.groupby(&#39;gender&#39;)[&#39;Churn&#39;].value_counts().to_frame() g = g.rename({&#39;Churn&#39;:&#39;pct_total&#39;}, axis = 1).reset_index() g[&#39;pct_total&#39;] = (g[&#39;pct_total&#39;]/len(df)) * 100 t = sns.barplot(&#39;gender&#39;, y = &#39;pct_total&#39;, hue = &#39;Churn&#39;, data = g) t.set_title(&#39;Churn % by Gender&#39;, fontsize = 30, fontweight = &#39;bold&#39;) t.set_xlabel(&#39;&#39;) t.set_ylabel(&#39;% of Customers&#39;, fontsize = 20, fontweight = &#39;bold&#39;) t.set_xticklabels(labels = [&#39;Female&#39;, &#39;Male&#39;], fontweight = &#39;bold&#39;, fontsize = 20) plt.tight_layout() def plot_age_dist(df): plt.subplot(142) g = df.copy() g = g.groupby(&#39;SeniorCitizen&#39;)[&#39;Churn&#39;].value_counts().to_frame() g = g.rename({&#39;Churn&#39;:&#39;pct_total&#39;}, axis = 1).reset_index() g[&#39;pct_total&#39;] = (g[&#39;pct_total&#39;]/len(df)) * 100 t = sns.barplot(&#39;SeniorCitizen&#39;, y = &#39;pct_total&#39;, hue = &#39;Churn&#39;, data = g) t.set_title(&#39;Churn % by Age&#39;, fontsize = 30, fontweight = &#39;bold&#39;) t.set_xlabel(&#39;&#39;) t.set_ylabel(&#39;% of Customers&#39;, fontsize = 20, fontweight = &#39;bold&#39;) t.set_xticklabels(labels = [&#39;Non-Senior Citizen&#39;, &#39;Senior Citizen&#39;], fontweight = &#39;bold&#39;, fontsize = 20) plt.tight_layout() def plot_partner_dist(df): plt.subplot(143) g = df.copy() g = g.groupby(&#39;Partner&#39;)[&#39;Churn&#39;].value_counts().to_frame() g = g.rename({&#39;Churn&#39;:&#39;pct_total&#39;}, axis = 1).reset_index() g[&#39;pct_total&#39;] = (g[&#39;pct_total&#39;]/len(df)) * 100 t = sns.barplot(&#39;Partner&#39;, y = &#39;pct_total&#39;, hue = &#39;Churn&#39;, data = g) t.set_title(&#39;Churn % by Partner&#39;, fontsize = 30, fontweight = &#39;bold&#39;) t.set_xlabel(&#39;&#39;) t.set_ylabel(&#39;% of Customers&#39;, fontsize = 20, fontweight = &#39;bold&#39;) t.set_xticklabels(labels = [&#39;No Partner&#39;, &#39;Partner&#39;], fontweight = &#39;bold&#39;, fontsize = 20) plt.tight_layout() def plot_dependents_dist(df): plt.subplot(144) g = df.copy() g = g.groupby(&#39;Dependents&#39;)[&#39;Churn&#39;].value_counts().to_frame() g = g.rename({&#39;Churn&#39;:&#39;pct_total&#39;}, axis = 1).reset_index() g[&#39;pct_total&#39;] = (g[&#39;pct_total&#39;]/len(df)) * 100 t = sns.barplot(&#39;Dependents&#39;, y = &#39;pct_total&#39;, hue = &#39;Churn&#39;, data = g) t.set_title(&#39;Churn % by Dependents&#39;, fontsize = 30, fontweight = &#39;bold&#39;) t.set_xlabel(&#39;&#39;) t.set_ylabel(&#39;% of Customers&#39;, fontsize = 20, fontweight = &#39;bold&#39;) t.set_xticklabels(labels = [&#39;No Dependents&#39;, &#39;Dependents&#39;], fontweight = &#39;bold&#39;, fontsize = 20) plt.tight_layout() plot_gender_dist(df_data) plot_age_dist(df_data) plot_partner_dist(df_data) plot_dependents_dist(df_data) . We can draw following conclusions from the above plots: . Our dataset has similar numbers of Male and Female customers. Also, proportion of customers churned is almost equal in both genders. | Our dataset has less senior citizens and non-senior citizens. Overall, a higher proportion of senior citizens will churn than non-senior citizens. | Customers without partners churn slightly more than those with partners. | Customers withot dependents churn slightly more than those with dependents. | Main Services . There are three categorical features which tells the type of services subscribed by the customer - PhoneService, MultipleLines and InternetService. . fig = plt.figure(figsize = (30,10)) def plot_phoneservice_dist(df): plt.subplot(131) g = df.copy() g = g.groupby(&#39;PhoneService&#39;)[&#39;Churn&#39;].value_counts().to_frame() g = g.rename({&#39;Churn&#39;:&#39;pct_total&#39;}, axis = 1).reset_index() g[&#39;pct_total&#39;] = (g[&#39;pct_total&#39;]/len(df)) * 100 t = sns.barplot(&#39;PhoneService&#39;, y = &#39;pct_total&#39;, hue = &#39;Churn&#39;, data = g) t.set_title(&#39;Churn % by Phone Service&#39;, fontsize = 30, fontweight = &#39;bold&#39;) t.set_xlabel(&#39;&#39;) t.set_ylabel(&#39;% of Customers&#39;, fontsize = 20, fontweight = &#39;bold&#39;) t.set_xticklabels(labels = [&#39;No Phone&#39;, &#39;Phone&#39;], fontweight = &#39;bold&#39;, fontsize = 20) plt.tight_layout() def plot_phonelinequantity_dist(df): plt.subplot(132) g = df.copy() g = g.groupby(&#39;MultipleLines&#39;)[&#39;Churn&#39;].value_counts().to_frame() g = g.rename({&#39;Churn&#39;:&#39;pct_total&#39;}, axis = 1).reset_index() g[&#39;pct_total&#39;] = (g[&#39;pct_total&#39;]/len(df)) * 100 t = sns.barplot(&#39;MultipleLines&#39;, y = &#39;pct_total&#39;, hue = &#39;Churn&#39;, data = g) t.set_title(&#39;Churn % by Phone Line Quantity&#39;, fontsize = 30, fontweight = &#39;bold&#39;) t.set_xlabel(&#39;&#39;) t.set_ylabel(&#39;% of Customers&#39;, fontsize = 20, fontweight = &#39;bold&#39;) t.set_xticklabels(labels = [&#39;Singular Line&#39;,&#39;No Phone Service&#39;, &#39;Multiple Lines&#39;], fontweight = &#39;bold&#39;, fontsize = 20) plt.tight_layout() def plot_internetservice_dist(df): plt.subplot(133) g = df.copy() g = g.groupby(&#39;InternetService&#39;)[&#39;Churn&#39;].value_counts().to_frame() g = g.rename({&#39;Churn&#39;:&#39;pct_total&#39;}, axis = 1).reset_index() g[&#39;pct_total&#39;] = (g[&#39;pct_total&#39;]/len(df)) * 100 t = sns.barplot(&#39;InternetService&#39;, y = &#39;pct_total&#39;, hue = &#39;Churn&#39;, data = g) t.set_title(&#39;Churn % by Internet Service&#39;, fontsize = 30, fontweight = &#39;bold&#39;) t.set_xlabel(&#39;&#39;) t.set_ylabel(&#39;% of Customers&#39;, fontsize = 20, fontweight = &#39;bold&#39;) t.set_xticklabels(labels = [&#39;DSL&#39;,&#39;Fiber Optic&#39;, &#39;No Internet Service&#39;], fontweight = &#39;bold&#39;, fontsize = 20) plt.tight_layout() plot_phoneservice_dist(df_data) plot_phonelinequantity_dist(df_data) plot_internetservice_dist(df_data) . We can draw following conclusions from the above plots: . Customers with phone service churn ~25% of the time. | Of customers with phone service, customers with multiple lines have a slightly higher churn rate than those with a singular line. | Customers with only phone service (and no internet service) have lower churn rate than those having internet service. Higher number of customers have fiber optic internet service. | Also, fiber optic internet customers churn at significantly higher proportions than DSL or No Internet customers. | Add-On Services . Customers have subscribed various add-on services along with phone and internet services. Let&#39;s look at the churn rate for customers with add-on services. . def plot_services(df): copy = df[df.InternetService != &#39;No&#39;] fig = plt.figure(figsize = (25, 10)) plt.subplot(231) copy1 = copy[copy.OnlineSecurity == &#39;Yes&#39;] plt.pie(copy1.Churn.value_counts(), labels = [&#39;No Churn&#39;, &#39;Churn&#39;], autopct = &#39;%.1f%%&#39;, textprops = {&#39;fontsize&#39;:20, &#39;fontweight&#39;:&#39;bold&#39;}) plt.title(&#39;Online Security - Churn %&#39;, fontsize = 25, fontweight = &#39;bold&#39;) plt.subplot(232) copy2 = copy[copy.OnlineBackup == &#39;Yes&#39;] plt.pie(copy2.Churn.value_counts(), labels = [&#39;No Churn&#39;, &#39;Churn&#39;], autopct = &#39;%.1f%%&#39;, textprops = {&#39;fontsize&#39;:20, &#39;fontweight&#39;:&#39;bold&#39;}) plt.title(&#39;Online Backup - Churn %&#39;, fontsize = 25, fontweight = &#39;bold&#39;) plt.subplot(233) copy3 = copy[copy.DeviceProtection == &#39;Yes&#39;] plt.pie(copy3.Churn.value_counts(), labels = [&#39;No Churn&#39;, &#39;Churn&#39;], autopct = &#39;%.1f%%&#39;, textprops = {&#39;fontsize&#39;:20, &#39;fontweight&#39;:&#39;bold&#39;}) plt.title(&#39;Device Protection - Churn %&#39;, fontsize = 25, fontweight = &#39;bold&#39;) plt.subplot(234) copy4 = copy[copy.TechSupport == &#39;Yes&#39;] plt.pie(copy4.Churn.value_counts(), labels = [&#39;No Churn&#39;, &#39;Churn&#39;], autopct = &#39;%.1f%%&#39;, textprops = {&#39;fontsize&#39;:20, &#39;fontweight&#39;:&#39;bold&#39;}) plt.title(&#39;Tech Support - Churn %&#39;, fontsize = 25, fontweight = &#39;bold&#39;) plt.subplot(235) copy5 = copy[copy.StreamingTV == &#39;Yes&#39;] plt.pie(copy5.Churn.value_counts(), labels = [&#39;No Churn&#39;, &#39;Churn&#39;], autopct = &#39;%.1f%%&#39;, textprops = {&#39;fontsize&#39;:20, &#39;fontweight&#39;:&#39;bold&#39;}) plt.title(&#39;Streaming TV - Churn %&#39;, fontsize = 25, fontweight = &#39;bold&#39;) plt.subplot(236) copy6 = copy[copy.StreamingMovies == &#39;Yes&#39;] plt.pie(copy6.Churn.value_counts(), labels = [&#39;No Churn&#39;, &#39;Churn&#39;], autopct = &#39;%.1f%%&#39;, textprops = {&#39;fontsize&#39;:20, &#39;fontweight&#39;:&#39;bold&#39;}) plt.title(&#39;Streaming Movies - Churn %&#39;, fontsize = 25, fontweight = &#39;bold&#39;) plt.tight_layout() plot_services(df_data) . Customers with TV streaming and/or Movie Streaming services churn more than all other add-on services. . Contracts . def plot_contracts(df): copy = df.copy() plt.figure(figsize = (30, 10)) plt.subplot(121) plt.pie(copy.Contract.value_counts(), labels = [&#39;Monthly&#39;, &#39;1-Year&#39;, &#39;2-Year&#39;], autopct = &#39;%.1f%%&#39;, textprops = {&#39;fontweight&#39;:&#39;bold&#39;, &#39;fontsize&#39;: 20}) plt.title(&#39;Customer Contract Composition&#39;, fontweight = &#39;bold&#39;, fontsize = 30) plt.subplot(122) plt.title(&#39;Churn % by Contract Type&#39;, fontsize = 30, fontweight = &#39;bold&#39;) copy = copy.groupby(&#39;Contract&#39;)[&#39;Churn&#39;].value_counts().to_frame() copy = copy.rename({&#39;Churn&#39;:&#39;pct_total&#39;}, axis = 1).reset_index() copy[&#39;pct_total&#39;] = (copy[&#39;pct_total&#39;]/len(df)) * 100 a = sns.barplot(&#39;Contract&#39;, y = &#39;pct_total&#39;, hue = &#39;Churn&#39;, data = copy) a.set_title(&#39;Contract Type - Churn %&#39;, fontsize = 30, fontweight = &#39;bold&#39;) a.set(xticklabels = [&#39;Monthly&#39;, &#39;1-Year&#39;, &#39;2-Year&#39;]) a.set_xlabel(&#39;&#39;) a.set_ylabel(&#39;% of Customers&#39;, fontweight = &#39;bold&#39;) a.set_xticklabels(a.get_xticklabels(), fontsize = 20, rotation = 45) plt.tight_layout() plot_contracts(df_data) . We can draw the following conclusions from above plots: . More than half of the customers use a monthly payment option. | Significantly more customers churn on monthly plans. | The longer the plan, the lower the churn rate. | Paperless Billing . def plot_paperless(df): copy = df.copy() plt.figure(figsize = (30, 10)) plt.subplot(121) plt.pie(copy.PaperlessBilling.value_counts(), labels=[&#39;Paperless&#39;,&#39;Not Paperless&#39;], autopct = &#39;%.1f%%&#39;, textprops = {&#39;fontweight&#39;:&#39;bold&#39;, &#39;fontsize&#39;: 20}) plt.title(&#39;Customer Paperless Billing Composition&#39;, fontweight = &#39;bold&#39;, fontsize = 30) plt.subplot(122) plt.title(&#39;Churn % by Billing Type&#39;, fontsize = 30, fontweight = &#39;bold&#39;) copy = copy.groupby(&#39;PaperlessBilling&#39;)[&#39;Churn&#39;].value_counts().to_frame() copy = copy.rename({&#39;Churn&#39;:&#39;pct_total&#39;}, axis = 1).reset_index() copy[&#39;pct_total&#39;] = (copy[&#39;pct_total&#39;]/len(df)) * 100 a = sns.barplot(&#39;PaperlessBilling&#39;, y = &#39;pct_total&#39;, hue = &#39;Churn&#39;, data = copy) a.set_title(&#39;Paperless Billing - Churn %&#39;, fontsize = 30, fontweight = &#39;bold&#39;) a.set_xticklabels(a.get_xticklabels(), fontsize = 20) a.set_xlabel(&#39;&#39;) a.set_ylabel(&#39;% of Customers&#39;, fontsize = 20, fontweight = &#39;bold&#39;) plt.tight_layout() plot_paperless (df_data) . About 60% of the customers have paperless billing. Also, customers with paperless have a significantly higher churn rate than customers with non-paperless billing. . Payment Methods . def plot_pay_methods(df): copy = df.copy() plt.figure(figsize = (30, 10)) plt.subplot(121) plt.pie(copy.PaymentMethod.value_counts(), labels = [&#39;E-Check&#39;, &#39;Mail Check&#39; , &#39;Bank Transfer (Auto)&#39;, &#39;Credit Card (Auto)&#39;], autopct = &#39;%.1f%%&#39;, textprops = {&#39;fontsize&#39;:20, &#39;fontweight&#39;:&#39;bold&#39;}, startangle = -90) plt.title(&#39;Customer Payment Method Composition&#39;, fontsize = 30, fontweight = &#39;bold&#39;) plt.subplot(122) copy = copy.groupby(&#39;PaymentMethod&#39;)[&#39;Churn&#39;].value_counts().to_frame() copy = copy.rename({&#39;Churn&#39;:&#39;pct_total&#39;}, axis = 1).reset_index() copy[&#39;pct_total&#39;] = (copy[&#39;pct_total&#39;]/len(df))*100 a = sns.barplot(&#39;PaymentMethod&#39;, &#39;pct_total&#39;, &#39;Churn&#39;, data = copy) a.set_title(&#39;Payment Methods - Churn %&#39;, fontsize = 30, fontweight = &#39;bold&#39;) a.set_xlabel(&#39;&#39;) a.set_ylabel(&#39;% of Customers&#39;, fontsize = 20, fontweight = &#39;bold&#39;) a.set_xticklabels(a.get_xticklabels(), fontsize = 20, rotation = 45) plt.tight_layout() plot_pay_methods(df_data) . We can see that customers with Automatic payment methods have significantly lower churn than other payment methods. Customers with e-check have higher churn than other payment methods. . Conclusion . We have look various ways to understand the features in our dataset and how churn is affected due to each feature. The data can be looked at numerous ways and the above visualizations are only few ways to understand the data. Before building the classification models, it is necessary to have understanding of business domain and the data structure. For example, we have found the overall churn of 27% which can be good or bad depending on the telecom industry churn rates benchmarks. . In the next blog, I will show predictive classificative models using two algorithms - Logistic Regression and Decision Tree - which can be used to predict whether customer will churn or not and take preventive actions for the customers with higher likelihood of churning. . If you have any comments or suggestions please comment below or reach out to me at - Twitter or LinkedIn .",
            "url": "https://rahuls0959.github.io/ds-blog/python/churn/retention/classification%20modeling/exploratory%20data%20analysis/matplotlib/seaborn/2020/06/06/_Customer-Churn-Analysis_Exploratory-Data-Analysis.html",
            "relUrl": "/python/churn/retention/classification%20modeling/exploratory%20data%20analysis/matplotlib/seaborn/2020/06/06/_Customer-Churn-Analysis_Exploratory-Data-Analysis.html",
            "date": " • Jun 6, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Topic Modeling of Product Reviews on Amazon Scraped using Selenium in Python",
            "content": "Introduction . The blog covers the step-by-step process to scrap product reviews from Amazon webpage and analysing main topics from the extracted data. We will scrap 1000 reviews from the Amazon for Apple iPhone 11 64GB. With this data, we will convert each review doc into bag-of words for applying the topic modeling algorithm. We will be using Latent Dirichlet Allocation (LDA) algorithm in this tutorial. The main python libraries used are: . selenium: Selenium is a portable framework for testing web applications. We will be using this to interact with the browser and open URLs (https://pypi.org/project/selenium/) | gensim: Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning (https://pypi.org/project/gensim/) | . Web Scraping . Web scraping is a technique for extracting information from the internet automatically using a software that simulates human web surfing. Web scraping helps us extract large volumes of data about customers, products, people, stock markets, etc. It is usually difficult to get this kind of information on a large scale using traditional data collection methods. We can utilize the data collected from a website such as e-commerce portal, social media channels to understand customer behaviors and sentiments, buying patterns, and brand attribute associations which are critical insights for any business. . The first and foremost thing while scraping a website is to understand the structure of the website. We will be scraping the reviews for Apple iPhone 11 64GB on Amazon.in website. We will scrape 1000 reviews from different users across multiple pages. We will scrape user name, date of review and review and export it into a .csv file for any further analysis. . Import Packages . Instll selenium package (if not already worked with before) using command &#39;!pip install selenium&#39; | Import webdriver from selenium in the notebook which we use to open an instance of Chrome browser | The executable file for launching Chrome &#39;chromedriver.exe&#39; should be in the same folder as the notebook | . #Importing packages from selenium import webdriver import pandas as pd . Script for Scraping . The below code opens the new chrome browser window and open our website with the url link provided. By the way, chrome knows that you are accessing it through an automated software! . driver = webdriver.Chrome(&#39;chromedriver.exe&#39;) url = &#39;https://www.amazon.in/Apple-iPhone-11-64GB-White/product-reviews/B07XVMCLP7/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&amp;reviewerType=all_reviews&amp;pageNumber1&#39; driver.get(url) . Woha! We just opened an url from python notebook. . . We will inspect 3 items (user id, date and comment) on our web page and understand how we can extract them. . Xpath for User id: Inspecting the userid, we can see the highlighted text represents the XML code for user id.The XML path (XPath)for the userid is shown below: . //*[@id=&quot;customer_review-RBOIMRTKIYBBR&quot;]/div[1]/a/div[2]/span . | . . There is an interesting thing to note here that the XML path contains a review id, which uniquely denotes each review on the website. This will be very helpful as we try to recursively scrape multiple comments. . Xpath for Date &amp; review: Similarily, we will find the XPaths for date and review. | Selenium has a function called “find_elements_by_xpath”. We will pass our XPath into this function and get a selenium element. Once we have the element, we can extract the text inside our XPath using the ‘text’ function. | We will recursively run the code for different review id and extract user id, date and review for each review id. Also, we will recursively go to next pages by simply changing the page numbers in the url to extract more comments until we get the desired number of comments. | . driver = webdriver.Chrome(&#39;chromedriver.exe&#39;) #Creating empty data frame to store user_id, dates and comments from ~5K users. data = pd.DataFrame(columns = [&#39;date&#39;,&#39;username&#39;,&#39;review&#39;]) j = 1 while (j&lt;=130): # Running while loop only till we get 1K reviews if (len(data)&lt;1000): url = &#39;https://www.amazon.in/Apple-iPhone-11-64GB-White/product-reviews/B07XVMCLP7/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&amp;reviewerType=all_reviews&amp;pageNumber=&#39; + str(j) driver.get(url) ids = driver.find_elements_by_xpath(&quot;//*[contains(@id,&#39;customer_review-&#39;)]&quot;) review_ids = [] for i in ids: review_ids.append(i.get_attribute(&#39;id&#39;)) for x in review_ids: #Extract dates from for each user on a page date_element = driver.find_elements_by_xpath(&#39;//*[@id=&quot;&#39; + x +&#39;&quot;]/span&#39;)[0] date = date_element.text #Extract user ids from each user on a page username_element = driver.find_elements_by_xpath(&#39;//*[@id=&quot;&#39; + x +&#39;&quot;]/div[1]/a/div[2]/span&#39;)[0] username = username_element.text #Extract Message for each user on a page review_element = driver.find_elements_by_xpath(&#39;//*[@id=&quot;&#39; + x +&#39;&quot;]/div[4]&#39;)[0] review = review_element.text #Adding date, userid and comment for each user in a dataframe data.loc[len(data)] = [date,username,review] j=j+1 else: break . Data Cleaning . We perform few data cleaning operations such as replacing line breaks with a space and copy the data into .csv file which can be used for further analysis. | . import copy data = copy.deepcopy(data) def remove_space(s): return s.replace(&quot; n&quot;,&quot; &quot;) data[&#39;review&#39;] = data[&#39;review&#39;].apply(remove_space) data.to_csv(&#39;amazon_reviews.csv&#39;, header=True, sep=&#39;,&#39;) . data = pd.read_csv(&#39;amazon_reviews.csv&#39;,index_col=[0]) data . date username review . 0 | Reviewed in India on 20 October 2019 | Suman Biswas | May be my first negative review about the prod... | . 1 | Reviewed in India on 17 September 2019 | Kaushik Bajaj | It&#39;s very expensive but the quality you get is... | . 2 | Reviewed in India on 29 September 2019 | Sunny Kumar | The iPhone design is good and the camera quali... | . 3 | Reviewed in India on 30 September 2019 | shanu Kumar | Awesome Phone. Nice upgrade from iPhone 6s to ... | . 4 | Reviewed in India on 14 October 2019 | Amazon Customer | My Phone is Producing Too Much Heat Even Didn’... | . ... | ... | ... | ... | . 995 | Reviewed in India on 4 March 2020 | Md.Imran | Rt | . 996 | Reviewed in India on 1 March 2020 | Amazon Customer | ❤️ | . 997 | Reviewed in India on 9 March 2020 | Chirag Patel | Ok | . 998 | Reviewed in India on 11 March 2020 | chintu | Excellent | . 999 | Reviewed in India on 8 March 2020 | Amazon Customer | Excellent | . 1000 rows × 3 columns . Since the goal of further analysis is to perform topic modeling, we will solely focus on the review text, and drop other metadata columns i.e. date and user name. | . # Remove the columns data = data.drop(columns=[&#39;date&#39;, &#39;username&#39;], axis=1) # Print out the data data . review . 0 | May be my first negative review about the prod... | . 1 | It&#39;s very expensive but the quality you get is... | . 2 | The iPhone design is good and the camera quali... | . 3 | Awesome Phone. Nice upgrade from iPhone 6s to ... | . 4 | My Phone is Producing Too Much Heat Even Didn’... | . ... | ... | . 995 | Rt | . 996 | ❤️ | . 997 | Ok | . 998 | Excellent | . 999 | Excellent | . 1000 rows × 1 columns . Topic Modeling using LDA . Topic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic. LDA is a generative probabilistic model that assumes each topic is a mixture over an underlying set of words, and each document is a mixture of over a set of topic probabilities. . Illustration of LDA input/output workflow (Credit: http://chdoig.github.io/pytexas2015-topic-modeling/#/3/4) . . Data Pre-processing . We will preprocess the review data using gensim library. Few of the actions performed by preprocess_string as follows: . Tokenization: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation. | All stopwords are removed. | Words are lemmatized: words in third person are changed to first person and verbs in past and future tenses are changed into present. | Words are stemmed: words are reduced to their root form. | . Please see below the output after pre-processing one of the reviews. . import gensim from gensim.parsing.preprocessing import preprocess_string # print unprocessed text print(data.review[1]) # print processed text print(preprocess_string(data.review[1])) . It&#39;s very expensive but the quality you get is osum [&#39;expens&#39;, &#39;qualiti&#39;, &#39;osum&#39;] . processed_data = data[&#39;review&#39;].map(preprocess_string) processed_data . 0 [neg, review, product, amazon, elat, receiv, i... 1 [expens, qualiti, osum] 2 [iphon, design, good, camera, qualiti, awesom,... 3 [awesom, phone, nice, upgrad, iphon, iphon, lo... 4 [phone, produc, heat, didn’t, sim, half, hour,... ... 995 [] 996 [] 997 [] 998 [excel] 999 [excel] Name: review, Length: 1000, dtype: object . Preparign Document-Term-Matrix . Gensim requires that tokens be converted to a dictionary. In this instance a dictionary is a mapping between words and their integer IDs. We then create a Document-Term-Matrix where we use Bag-of-Words approach returning the vector of word and its frequency (number of occurences in the document) for each document. . # Importing Gensim import gensim from gensim import corpora # Creating the term dictionary of our list of documents (corpus), where every unique term is assigned an index. dictionary = corpora.Dictionary(processed_data) # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. doc_term_matrix = [dictionary.doc2bow(doc) for doc in processed_data] . Running LDA Model . We will now run the LDA Model. The number of topics you give is largely a guess/arbitrary. The model assumes the document contains that many topics. However, finding the number of topics explaining the data is a optimisation problem and can be found by &#39;Coherence Model&#39;. . Here, we have used number of topics = 3 . #RUN THE MODEL # Creating the object for LDA model using gensim library Lda = gensim.models.ldamodel.LdaModel # Running and Trainign LDA model on the document term matrix. TOPIC_CNT= 3 ldamodel = Lda(doc_term_matrix, num_topics=TOPIC_CNT, id2word = dictionary, passes=50) . We can then see the weights of top 20 words in each topic, which can help us to explain the topic. . #Results topics= ldamodel.print_topics(num_topics=TOPIC_CNT, num_words=20) topics . [(0, &#39;0.050*&#34;phone&#34; + 0.031*&#34;iphon&#34; + 0.019*&#34;best&#34; + 0.015*&#34;amazon&#34; + 0.015*&#34;nice&#34; + 0.014*&#34;great&#34; + 0.013*&#34;camera&#34; + 0.012*&#34;time&#34; + 0.012*&#34;appl&#34; + 0.010*&#34;charger&#34; + 0.009*&#34;product&#34; + 0.008*&#34;bui&#34; + 0.008*&#34;charg&#34; + 0.007*&#34;qualiti&#34; + 0.007*&#34;us&#34; + 0.006*&#34;good&#34; + 0.006*&#34;deliveri&#34; + 0.006*&#34;perfect&#34; + 0.006*&#34;look&#34; + 0.006*&#34;batteri&#34;&#39;), (1, &#39;0.032*&#34;phone&#34; + 0.026*&#34;awesom&#34; + 0.024*&#34;iphon&#34; + 0.015*&#34;love&#34; + 0.011*&#34;great&#34; + 0.010*&#34;io&#34; + 0.010*&#34;dai&#34; + 0.010*&#34;camera&#34; + 0.009*&#34;android&#34; + 0.008*&#34;displai&#34; + 0.008*&#34;it’&#34; + 0.008*&#34;batteri&#34; + 0.007*&#34;screen&#34; + 0.007*&#34;appl&#34; + 0.007*&#34;review&#34; + 0.006*&#34;pro&#34; + 0.006*&#34;best&#34; + 0.006*&#34;experi&#34; + 0.006*&#34;feel&#34; + 0.006*&#34;word&#34;&#39;), (2, &#39;0.066*&#34;good&#34; + 0.041*&#34;batteri&#34; + 0.036*&#34;camera&#34; + 0.035*&#34;phone&#34; + 0.032*&#34;product&#34; + 0.025*&#34;life&#34; + 0.023*&#34;qualiti&#34; + 0.023*&#34;appl&#34; + 0.018*&#34;iphon&#34; + 0.018*&#34;best&#34; + 0.014*&#34;awesom&#34; + 0.014*&#34;bui&#34; + 0.014*&#34;amaz&#34; + 0.013*&#34;monei&#34; + 0.011*&#34;excel&#34; + 0.011*&#34;valu&#34; + 0.011*&#34;perform&#34; + 0.010*&#34;mobil&#34; + 0.009*&#34;great&#34; + 0.009*&#34;worth&#34;&#39;)] . Extracting Topics . We can identify the follow topics emerging out of reviews of Amazon iPhone 11 64GB: . Topic #1: There seems to discussion of heat/ charging issue with the product. | Topic #2: The discussion on iPhone&#39;s features such as camera, display, battery. | Topic #3: iPhone being value for money and discussion on Amazon delivery service. | . word_dict = {}; for i in range(TOPIC_CNT): words = ldamodel.show_topic(i, topn = 20) word_dict[&#39;Topic #&#39; + &#39;{:2d}&#39;.format(i+1)] = [i[0] for i in words] pd.DataFrame(word_dict) . Topic # 1 Topic # 2 Topic # 3 . 0 | phone | phone | good | . 1 | iphon | awesom | batteri | . 2 | best | iphon | camera | . 3 | amazon | love | phone | . 4 | nice | great | product | . 5 | great | io | life | . 6 | camera | dai | qualiti | . 7 | time | camera | appl | . 8 | appl | android | iphon | . 9 | charger | displai | best | . 10 | product | it’ | awesom | . 11 | bui | batteri | bui | . 12 | charg | screen | amaz | . 13 | qualiti | appl | monei | . 14 | us | review | excel | . 15 | good | pro | valu | . 16 | deliveri | best | perform | . 17 | perfect | experi | mobil | . 18 | look | feel | great | . 19 | batteri | word | worth | . The below code provide the % of topic a document is about. This helps to find the dominant topic in each review. . doc_to_topic = [] for i in range(len(doc_term_matrix)): top_topics = ldamodel.get_document_topics(doc_term_matrix[i], minimum_probability=0.0) topic_vec = [top_topics[j][1] for j in range(TOPIC_CNT)] doc_to_topic.append(topic_vec) . #Dataframe of topic document_topics = pd.DataFrame(doc_to_topic) document_topics = document_topics.rename(columns=lambda x: x + 1) document_topics.columns = document_topics.columns.astype(str) document_topics = document_topics.rename(columns=lambda x: &#39;Topic #&#39; + x) . #Dataframe of review and topics data_new = pd.concat([data,document_topics],axis=1,join=&#39;inner&#39;) data_new.head() . review Topic #1 Topic #2 Topic #3 . 0 | May be my first negative review about the prod... | 0.990397 | 0.004789 | 0.004815 | . 1 | It&#39;s very expensive but the quality you get is... | 0.084920 | 0.089119 | 0.825960 | . 2 | The iPhone design is good and the camera quali... | 0.989838 | 0.004876 | 0.005286 | . 3 | Awesome Phone. Nice upgrade from iPhone 6s to ... | 0.455081 | 0.503810 | 0.041108 | . 4 | My Phone is Producing Too Much Heat Even Didn’... | 0.978437 | 0.010337 | 0.011225 | . Endnotes . I hope this blog helps in understanding how powerful Topic Modeling is in understanding unstructured textual data. Feel free to play around with the code by opening in Colab or cloning the repo in github. . If you have any comments or suggestions please comment below or reach out to me at - Twitter or LinkedIn .",
            "url": "https://rahuls0959.github.io/ds-blog/python/text%20analytics/web%20scraping/topic%20modeling/selenium/gensim/nlp/2020/04/21/Topic_Modeling_Amazon_Reviews_new.html",
            "relUrl": "/python/text%20analytics/web%20scraping/topic%20modeling/selenium/gensim/nlp/2020/04/21/Topic_Modeling_Amazon_Reviews_new.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Basic Text Analytics Tutorial on Sentiment Analysis of Tweets using Python",
            "content": "Introduction . This blog covers the basics of Text Analytics. We will be using Natural Language Processing and Machine Learning alogorithm to analyse the tweets on the Twitter platform.We will analyse the sentiments of 5000 tweets with certain hashtags and label them as Positive, Negative or Neutral. With this data, we will build a ML model using Naive Bayes Classifier which can be deployed to predict the sentiment of new tweets.We will be using the following python libraries for this analysis: . tweepy: Library for accessing the Twitter API | pandas: Library to perform various operations on the data | nltk: Library to perform Natural Language Processing for English text | sklearn: Library to perform machine learning algorithms | . Imports . tweepy: http://docs.tweepy.org/en/latest/ | pandas: https://pandas.pydata.org/docs/user_guide/index.html | . import tweepy as tw #library for accessing Twitter API import pandas as pd #data analysis API import json . Scrapping Tweets from Twitter using Tweepy . You have to first create a Twitter Developer account to gain credentials for Tweepy. This does require that you have a Twitter account. The application will ask various questions about what sort of work you want to do. Don’t fret, these details don’t have to be extensive, and the process is relatively easy. Link- https://developer.twitter.com/en | . After finishing the application, the approval process is relatively quick and shouldn’t take longer than a couple of days. Upon being approved you will need to log in and set up a dev environment in the developer dashboard and view that app’s details to retrieve your developer credentials as shown in the below picture. Unless you specifically have requested access to the other API’s offered, you will now be able to use the standard Tweepy API. | . Before getting started on Tweepy you will have to authorize that you have the credentials to utilize its API. The code snippet given is how one authorizes themself. | The search parameters I focused on are q(text query), lang(language of tweets), since(starting date o tweets), include_rts(whether to include retweets or not), items(count of tweets to be extracted). In the below code, I scrape the 5000 of tweets since 1st April 2020 that have following text &quot;#coronavirus, #COVID19, #CoronavirusOutbreak&quot;. | If you want to further customize your search you can view the rest of the search parameters available in the api.search method here- https://tweepy.readthedocs.io/en/latest/api.html#API.search | One of the advantages of querying with Tweepy is the amount of information contained in the tweet object. I have queried only tweet txt in the below code. If you’re interested in grabbing other information you can view the full list of information available in Tweepy’s tweet object here- https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object | . #Twitter App Auth consumer_key = &#39;41zvYrOHdiIKgSq7Xf5tbTyrp&#39; consumer_secret = &#39;3rOqNnBRjjqkEpGfQcmyD9kh6WgGALmTmiI6IJUgILee0Z0Uad&#39; access_key = &#39;1224090842106757120-gD25mu7R2pNTCQCozu5o9SCSpE8XHG&#39; access_secret = &#39;D63jeYIxtByfzbMvwr2SSXycTSyR4Hdm1xCxcAt3mNfY5&#39; # Initialize API auth = tw.OAuthHandler(consumer_key, consumer_secret) auth.set_access_token(access_key, access_secret) api = tw.API(auth, wait_on_rate_limit=True) # Search terms search_words = [&quot;#coronavirus&quot;, &quot;#COVID19&quot;, &quot;#CoronavirusOutbreak&quot;] date_since = &quot;2020-04-01&quot; # Collect tweets tweets = tw.Cursor(api.search, q=search_words, lang=&quot;en&quot;, since=date_since, tweet_mode=&#39;extended&#39;, include_rts=True).items(5000) tweets_arr = [] # Iterate and print tweets for tweet in tweets: tweets_arr.append(tweet.full_text) print(&quot;Done&quot;) #Creating data frame of tweets df_tweets = pd.DataFrame(tweets_arr) df_tweets . Done . 0 . 0 | RT @chidambara09: #Coronavirus: n n#European P... | . 1 | China denies cover up, but abruptly raises COV... | . 2 | RT @chidambara09: #Coronavirus: n n#European P... | . 3 | RT @pcraindia: With necessary precautions and ... | . 4 | RT @chidambara09: #Coronavirus: n n#European P... | . ... | ... | . 4995 | Vietnam sets up &#39;Rice ATMs&#39; to provide free ri... | . 4996 | Covid19 austerity: New Zealand PM sets example... | . 4997 | RT @admediainsider: &quot;This phase has made India... | . 4998 | RT @pennewstweet: Masks are mandatory in Singa... | . 4999 | RT @QuickTake: Young people are falling seriou... | . 5000 rows × 1 columns . Sentiment Analysis of the scrapped tweets . Import nltk libraries for doing sentiment analysis: https://www.nltk.org/ | . # NLP libraries import nltk from nltk.tokenize import sent_tokenize #sentence tokenization:break text into sentences from nltk.tokenize import word_tokenize #word tokenization:break sentences into words from nltk.corpus import stopwords #removal of stop words from nltk.stem import PorterStemmer #lexicon Normalisation/Stemming: retain only root form of the word . Install required packages in nltk with inbuilt sentiment analyser. VADER belongs to a type of sentiment analysis that is based on lexicons of sentiment-related words. In this approach, each of the words in the lexicon is rated as to whether it is positive or negative, and in many cases, how positive or negative. | . #Using NLTK package to conduct sentiment analysis nltk.download(&#39;vader_lexicon&#39;) . [nltk_data] Downloading package vader_lexicon to [nltk_data] C: Users 61920959 AppData Roaming nltk_data... [nltk_data] Package vader_lexicon is already up-to-date! . True . The polarity scores for each tweet are calculated using Sentiment Intensity Analyser. | The Positive, Negative and Neutral scores represent the proportion of text that falls in these categories. Hence all these should add up to 1. | The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1(most extreme negative) and +1 (most extreme positive). | . #Sentiment Scores from nltk.sentiment.vader import SentimentIntensityAnalyzer sid = SentimentIntensityAnalyzer() scores = [] for tweet in tweets_arr: score = sid.polarity_scores(tweet) scores.append(score) #Dataframe for sentiment scores df_sentiments = pd.DataFrame(scores) df_sentiments dataset = pd.concat([df_tweets, df_sentiments], axis=1, join=&#39;inner&#39;) dataset . 0 neg neu pos compound . 0 | RT @chidambara09: #Coronavirus: n n#European P... | 0.000 | 1.000 | 0.000 | 0.0000 | . 1 | China denies cover up, but abruptly raises COV... | 0.257 | 0.743 | 0.000 | -0.8047 | . 2 | RT @chidambara09: #Coronavirus: n n#European P... | 0.000 | 1.000 | 0.000 | 0.0000 | . 3 | RT @pcraindia: With necessary precautions and ... | 0.115 | 0.885 | 0.000 | -0.3818 | . 4 | RT @chidambara09: #Coronavirus: n n#European P... | 0.000 | 1.000 | 0.000 | 0.0000 | . ... | ... | ... | ... | ... | ... | . 4995 | Vietnam sets up &#39;Rice ATMs&#39; to provide free ri... | 0.000 | 0.883 | 0.117 | 0.5106 | . 4996 | Covid19 austerity: New Zealand PM sets example... | 0.059 | 0.941 | 0.000 | -0.1280 | . 4997 | RT @admediainsider: &quot;This phase has made India... | 0.174 | 0.826 | 0.000 | -0.4588 | . 4998 | RT @pennewstweet: Masks are mandatory in Singa... | 0.000 | 0.909 | 0.091 | 0.0772 | . 4999 | RT @QuickTake: Young people are falling seriou... | 0.243 | 0.757 | 0.000 | -0.6249 | . 5000 rows × 5 columns . The overall sentiment of the tweet is computed using the following: positive sentiment : (compound score &gt;= 0.05) | neutral sentiment : (compound score &gt; -0.05) and (compound score &lt; 0.05) | negative sentiment : (compound score &lt;= -0.05) | . | As we can see, sentiment of around 50% of the tweets is neutral, 27% negative and 23% positive | . # Generate overall_sentiment using pandas overall_sentiment = [] for value in dataset[&quot;compound&quot;]: if value &gt;= 0.05: overall_sentiment.append(&quot;Positive&quot;) elif value &lt;= -0.05: overall_sentiment.append(&quot;Negative&quot;) else: overall_sentiment.append(&quot;Neutral&quot;) dataset[&quot;overall_sentiment&quot;] = overall_sentiment #dropping the scores columns data = dataset.drop(columns ={&quot;neg&quot;,&quot;pos&quot;,&quot;neu&quot;,&quot;compound&quot;}) # changing column name with rename() data = data.rename(columns = {0: &quot;text&quot;}) print(data) data.groupby(&#39;overall_sentiment&#39;).size() . text overall_sentiment 0 RT @chidambara09: #Coronavirus: n n#European P... Neutral 1 China denies cover up, but abruptly raises COV... Negative 2 RT @chidambara09: #Coronavirus: n n#European P... Neutral 3 RT @pcraindia: With necessary precautions and ... Negative 4 RT @chidambara09: #Coronavirus: n n#European P... Neutral ... ... ... 4995 Vietnam sets up &#39;Rice ATMs&#39; to provide free ri... Positive 4996 Covid19 austerity: New Zealand PM sets example... Negative 4997 RT @admediainsider: &#34;This phase has made India... Negative 4998 RT @pennewstweet: Masks are mandatory in Singa... Positive 4999 RT @QuickTake: Young people are falling seriou... Negative [5000 rows x 2 columns] . overall_sentiment Negative 1356 Neutral 2475 Positive 1169 dtype: int64 . Wordcloud of the Tweet Data . Wordcloud is a powerful visualisation tool to understand what are the main words in the content or what most people are tweeting about. In the below code we have removed the following words from appearing in wordcloud: . Words starting with @ (tweet author&#39;s name) | Words starting with # (we have extracted tweets with hashtags related to coronavirus) | Words such as RT (which is coming in case it is a re-tweet) | Few interesting observations from the word clouds: . Positive wordcloud: United States,deaths reported, new cases - United States is mostly mentioned in tweets with overall positive tone. | Negative wordcloud: Europe, seriously ill, young people - Tweets are showing the illness caused by virus and young people might be most affected by this disease. Europe is mostly mentioned in tweets with overall negative tone. | Neutral wordcloud: new case, total deaths, total confirmed - These might be general tweets providing coronaviris stats. | #visualizing word clouds for positive and negative tweets from wordcloud import WordCloud,STOPWORDS import matplotlib.pyplot as plt %matplotlib inline data_pos = data[ data[&#39;overall_sentiment&#39;] == &#39;Positive&#39;] data_pos = data_pos[&#39;text&#39;] data_neg = data[ data[&#39;overall_sentiment&#39;] == &#39;Negative&#39;] data_neg = data_neg[&#39;text&#39;] data_neu = data[ data[&#39;overall_sentiment&#39;] == &#39;Neutral&#39;] data_neu = data_neu[&#39;text&#39;] def wordcloud_draw(data, color = &#39;black&#39;): words = &#39; &#39;.join(data) cleaned_word = &quot; &quot;.join([word for word in words.split() if &#39;http&#39; not in word and not word.startswith(&#39;@&#39;) and not word.startswith(&#39;#&#39;) and word != &#39;RT&#39; ]) wordcloud = WordCloud(stopwords=STOPWORDS, background_color=color, width=2500, height=2000 ).generate(cleaned_word) plt.figure(1,figsize=(9, 9)) plt.imshow(wordcloud) plt.axis(&#39;off&#39;) plt.show() print(&quot;Positive words&quot;) wordcloud_draw(data_pos,&#39;white&#39;) print(&quot;Negative words&quot;) wordcloud_draw(data_neg) print(&quot;Neutral words&quot;) wordcloud_draw(data_neu,&#39;white&#39;) . Positive words . Negative words . Neutral words . Naive Bayes Classification Model . Import sklearn library for building the classification model: https://scikit-learn.org/stable/ | . #import machine learning libraries import time from sklearn.pipeline import Pipeline from sklearn.multiclass import OneVsRestClassifier from sklearn import model_selection, naive_bayes, metrics, svm from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfTransformer . Split the 5000 tweet data into training (70%) and validation sets (30%) | . #Splitting the data in train and test split t1 = time.time() X_train, X_test, y_train, y_test = train_test_split(data[&#39;text&#39;], data[&#39;overall_sentiment&#39;], test_size=0.3,random_state = 0) t2= time.time() . We will be using CountVectorizer for performing the following data preprocessing tasks: Representing the tweet text as bag-of-words for feature extraction | Text preprocessing, tokenizing and filtering of stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors. | | . The output would be the list of words/ tokens that appear in the tweet corpus and number of ocuurences of words in each tweet. . However, this would only give the occurences of word in a document which might be not be an ideal metric since larger tweets will have more words. We will solve this by normalising using TF(Term Frequency) - IDF (Inverse Document Frequency) metric explained in this image. TfidfTransform performs this function in Python. | . count_vect = CountVectorizer(lowercase=True,stop_words=&quot;english&quot;,min_df=10) count_vect.fit(X_train) X_train_counts = count_vect.transform(X_train) X_test_counts = count_vect.transform(X_test) # Create the tf-idf representation using the bag-of-words matrix tfidf_transformer = TfidfTransformer(norm=None) tfidf_transformer.fit(X_train_counts) X_train_tfid =tfidf_transformer.transform(X_train_counts) X_test_tfid = tfidf_transformer.transform(X_test_counts) . X_train_counts.shape . (3500, 624) . X_test_counts.shape . (1500, 624) . We will be building classification model using Naive Bayes algorithm. It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature (a big assumption indeed, that is why model is called Naive). We will be using multinomial naive bayes since there are more than 2 classes (positive, negative, neutral). . | Explained simply, naive bayes algoorithm works by calculating the probabilities using following formula and giving output as the class having highest probability. . | Our model accuracy on training data is 79% and 76% on test data (which is quite good). . | . #Applying Naive Bayes Classifier from sklearn.naive_bayes import MultinomialNB nb = MultinomialNB() #fit data to NB model nb.fit(X_train_tfid, y_train) # train naive bayes on count print(&quot;Train Accuracy: &quot;, round(nb.score(X_train_tfid,y_train), 3)) print(&quot;Test Accuracy: &quot;, round(nb.score(X_test_tfid,y_test), 3)) . Train Accuracy: 0.793 Test Accuracy: 0.76 . Using this model, we can predict the sentiment of any new tweet. As given in example below, the tone of the tweet is negative and model performed a well job in predicting the same. Voila! | . #predicting the sentiment of a new tweet docs_new = [&#39;Balancing working from home and shouldering the bulk of domestic tasks leaves many women stretched to capacity, meaning less quality time with their families and for themselves.&#39;] X_new_counts = count_vect.transform(docs_new) X_new_tfidf = tfidf_transformer.transform(X_new_counts) clf = nb.fit(X_train_tfid, y_train) predicted = clf.predict(X_new_tfidf) print(predicted) . [&#39;Neutral&#39;] . If you have any comments or suggestions please comment below or reach out to me at - Twitter or LinkedIn .",
            "url": "https://rahuls0959.github.io/ds-blog/python/text%20analytics/sentiment%20analysis/wordcloud/tweepy/nlp/sklearn/2020/04/18/twitter_sentiment_analysis.html",
            "relUrl": "/python/text%20analytics/sentiment%20analysis/wordcloud/tweepy/nlp/sklearn/2020/04/18/twitter_sentiment_analysis.html",
            "date": " • Apr 18, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there! I am Rahul. I have completed my engineering from IIT Kanpur in 2015 and MBA from ISB in 2020. I believe in the power of data analytics in solving challenging business problems. . . If you want to chat about Tech, Products, or Analytics feel free to reach out to me on Twitter or Linkedin .",
          "url": "https://rahuls0959.github.io/ds-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rahuls0959.github.io/ds-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}