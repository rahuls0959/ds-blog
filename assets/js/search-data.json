{
  
    
        "post0": {
            "title": "Topic Modeling of Product Reviews on Amazon Scraped using Selenium in Python",
            "content": "Introduction . The blog covers the step-by-step process to scrap product reviews from Amazon webpage and analysing main topics from the extracted data. We will scrap 1000 reviews from the Amazon for Apple iPhone 11 64GB. With this data, we will convert each review doc into bag-of words for applying the topic modeling algorithm. We will be using Latent Dirichlet Allocation (LDA) algorithm in this tutorial. The main python libraries used are: . selenium: Selenium is a portable framework for testing web applications. We will be using this to interact with the browser and open URLs. | gensim: Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning. | . Web Scraping . Web scraping is a technique for extracting information from the internet automatically using a software that simulates human web surfing.Web scraping helps us extract large volumes of data about customers, products, people, stock markets, etc. It is usually difficult to get this kind of information on a large scale using traditional data collection methods. We can utilize the data collected from a website such as e-commerce portal, social media channels to understand customer behaviors and sentiments, buying patterns, and brand attribute associations which are critical insights for any business. . The first and foremost thing while scraping a website is to understand the structure of the website. We will be scraping the reviews for Apple iPhone 11 64GB on Amazon.in website. We will scrape 1000 reviews from different users across multiple pages. We will scrape user name, date of review and review and export it into a csv file for any further analysis. . Import Packages . Install selenium package (if not already worked with before) using command &#39;!pip install selenium&#39; | Import webdriver from selenium in the notebook which we use to open an instance of Chrome browser. | The executable file for launching Chrome &#39;chromedriver.exe&#39; should be in the same folder as the notebook. | . #Importing packages from selenium import webdriver import pandas as pd . Script for Scraping . The below code opens the new chrome browser window and open our website with the url link provided. By the way, chrome knows that you are accessing it through an automated software! . driver = webdriver.Chrome(&#39;chromedriver.exe&#39;) url = &#39;https://www.amazon.in/Apple-iPhone-11-64GB-White/product-reviews/B07XVMCLP7/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&amp;reviewerType=all_reviews&amp;pageNumber1&#39; driver.get(url) . We will inspect 3 items (user id, date and comment) on our web page and understand how we can extract them. . Xpath for User id: Inspecting the userid, we can see the highlighted text represents the XML code for user id.The XML path (XPath)for the userid is shown below: . //*[@id=&quot;customer_review-RBOIMRTKIYBBR&quot;]/div[1]/a/div[2]/span . | . . There is an interesting thing to note here that the XML path contains a review id, which uniquely denotes each review on the website. This will be very helpful as we try to recursively scrape multiple comments. . Xpath for Date &amp; review: Similarily, we will find the XPaths for date and review. | Selenium has a function called “find_elements_by_xpath”. We will pass our XPath into this function and get a selenium element. Once we have the element, we can extract the text inside our XPath using the ‘text’ function. | We will recursively run the code for different review id and extract user id, date and review for each review id. Also, we will recursively go to next pages by simply changing the page numbers in the url to extract more comments until we get the desired number of comments. | . driver = webdriver.Chrome(&#39;chromedriver.exe&#39;) #Creating empty data frame to store user_id, dates and comments from ~5K users. data = pd.DataFrame(columns = [&#39;date&#39;,&#39;username&#39;,&#39;review&#39;]) j = 1 while (j&lt;=130): # Running while loop only till we get 1K reviews if (len(data)&lt;1000): url = &#39;https://www.amazon.in/Apple-iPhone-11-64GB-White/product-reviews/B07XVMCLP7/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&amp;reviewerType=all_reviews&amp;pageNumber=&#39; + str(j) driver.get(url) ids = driver.find_elements_by_xpath(&quot;//*[contains(@id,&#39;customer_review-&#39;)]&quot;) review_ids = [] for i in ids: review_ids.append(i.get_attribute(&#39;id&#39;)) for x in review_ids: #Extract dates from for each user on a page date_element = driver.find_elements_by_xpath(&#39;//*[@id=&quot;&#39; + x +&#39;&quot;]/span&#39;)[0] date = date_element.text #Extract user ids from each user on a page username_element = driver.find_elements_by_xpath(&#39;//*[@id=&quot;&#39; + x +&#39;&quot;]/div[1]/a/div[2]/span&#39;)[0] username = username_element.text #Extract Message for each user on a page review_element = driver.find_elements_by_xpath(&#39;//*[@id=&quot;&#39; + x +&#39;&quot;]/div[4]&#39;)[0] review = review_element.text #Adding date, userid and comment for each user in a dataframe data.loc[len(data)] = [date,username,review] j=j+1 else: break . Data Cleaning . We perform few data cleaning operations such as replacing line breaks with a space and copy the data into .csv file which can be used for further analysis. | . import copy data = copy.deepcopy(data) def remove_space(s): return s.replace(&quot; n&quot;,&quot; &quot;) data[&#39;review&#39;] = data[&#39;review&#39;].apply(remove_space) data.to_csv(&#39;amazon_reviews.csv&#39;, header=True, sep=&#39;,&#39;) . data = pd.read_csv(&#39;amazon_reviews.csv&#39;,index_col=[0]) data . date username review . 0 | Reviewed in India on 20 October 2019 | Suman Biswas | May be my first negative review about the prod... | . 1 | Reviewed in India on 17 September 2019 | Kaushik Bajaj | It&#39;s very expensive but the quality you get is... | . 2 | Reviewed in India on 29 September 2019 | Sunny Kumar | The iPhone design is good and the camera quali... | . 3 | Reviewed in India on 30 September 2019 | shanu Kumar | Awesome Phone. Nice upgrade from iPhone 6s to ... | . 4 | Reviewed in India on 14 October 2019 | Amazon Customer | My Phone is Producing Too Much Heat Even Didn’... | . ... | ... | ... | ... | . 995 | Reviewed in India on 1 March 2020 | Amazon Customer | ❤️ | . 996 | Reviewed in India on 9 March 2020 | Chirag Patel | Ok | . 997 | Reviewed in India on 11 March 2020 | chintu | Excellent | . 998 | Reviewed in India on 8 March 2020 | Amazon Customer | Excellent | . 999 | Reviewed in India on 8 March 2020 | Dheeraj v. | Flawless! | . 1000 rows × 3 columns . Since the goal of further analysis is to perform topic modeling, we will solely focus on the review text, and drop other metadata columns i.e. date and user name. | . # Remove the columns data = data.drop(columns=[&#39;date&#39;, &#39;username&#39;], axis=1) # Print out the data data . review . 0 | May be my first negative review about the prod... | . 1 | It&#39;s very expensive but the quality you get is... | . 2 | The iPhone design is good and the camera quali... | . 3 | Awesome Phone. Nice upgrade from iPhone 6s to ... | . 4 | My Phone is Producing Too Much Heat Even Didn’... | . ... | ... | . 995 | ❤️ | . 996 | Ok | . 997 | Excellent | . 998 | Excellent | . 999 | Flawless! | . 1000 rows × 1 columns . Topic Modeling using LDA . Topic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic. LDA is a generative probabilistic model that assumes each topic is a mixture over an underlying set of words, and each document is a mixture of over a set of topic probabilities. . Illustration of LDA input/output workflow (Credit: http://chdoig.github.io/pytexas2015-topic-modeling/#/3/4) . Data Pre-processing . We will preprocess the review data using gensim library. Few of the actions performed by preprocess_string as follows: . Tokenization: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation. | All stopwords are removed. | Words are lemmatized: words in third person are changed to first person and verbs in past and future tenses are changed into present. | Words are stemmed: words are reduced to their root form. | . Please see below the output after pre-processing for one of the reviews. . import gensim from gensim.parsing.preprocessing import preprocess_string # print unprocessed text print(data.review[0]) # print processed text print(preprocess_string(data.review[0])) . May be my first negative review about the product &amp; Amazon both. I was much elated to receive the iPhone 11 so fast, next day of dispatch i.e. 28/09/19, but the thing I got started heating up every now and then. Contacted Applecare, just to be consoled that it&#39;s quite normal. As it continued, tried to return the product by speaking to Amazon customer support but in vain. Some body called me back to convey that only Apple will decide which one to take back. Why is then Amazon took up the sacred duty of selling such an item which they can&#39;t exchange/ have no control ? The product developed new issues like proximity sensor malfunction and last but most importantly loosing mobile network every other minute(even had two software updates). It was handed over to the Apple ASP as the return window closed on 10/10/19 (what use it was for??) and diagnosed as having issues and has further been sent to Apple repair facility at Bengaluru. So I&#39;m here w/out my first iPhone after using it(suffering for??) just a little over 2 weeks and the CREDIT GOES TO AMAZON !! Bravo, keep it up Amazon. [&#39;neg&#39;, &#39;review&#39;, &#39;product&#39;, &#39;amazon&#39;, &#39;elat&#39;, &#39;receiv&#39;, &#39;iphon&#39;, &#39;fast&#39;, &#39;dai&#39;, &#39;dispatch&#39;, &#39;thing&#39;, &#39;got&#39;, &#39;start&#39;, &#39;heat&#39;, &#39;contact&#39;, &#39;applecar&#39;, &#39;consol&#39;, &#39;normal&#39;, &#39;continu&#39;, &#39;tri&#39;, &#39;return&#39;, &#39;product&#39;, &#39;speak&#39;, &#39;amazon&#39;, &#39;custom&#39;, &#39;support&#39;, &#39;vain&#39;, &#39;bodi&#39;, &#39;call&#39;, &#39;convei&#39;, &#39;appl&#39;, &#39;decid&#39;, &#39;amazon&#39;, &#39;took&#39;, &#39;sacr&#39;, &#39;duti&#39;, &#39;sell&#39;, &#39;item&#39;, &#39;exchang&#39;, &#39;control&#39;, &#39;product&#39;, &#39;develop&#39;, &#39;new&#39;, &#39;issu&#39;, &#39;like&#39;, &#39;proxim&#39;, &#39;sensor&#39;, &#39;malfunct&#39;, &#39;importantli&#39;, &#39;loos&#39;, &#39;mobil&#39;, &#39;network&#39;, &#39;minut&#39;, &#39;softwar&#39;, &#39;updat&#39;, &#39;hand&#39;, &#39;appl&#39;, &#39;asp&#39;, &#39;return&#39;, &#39;window&#39;, &#39;close&#39;, &#39;us&#39;, &#39;diagnos&#39;, &#39;have&#39;, &#39;issu&#39;, &#39;sent&#39;, &#39;appl&#39;, &#39;repair&#39;, &#39;facil&#39;, &#39;bengaluru&#39;, &#39;iphon&#39;, &#39;suffer&#39;, &#39;littl&#39;, &#39;week&#39;, &#39;credit&#39;, &#39;goe&#39;, &#39;amazon&#39;, &#39;bravo&#39;, &#39;amazon&#39;] . processed_data = data[&#39;review&#39;].map(preprocess_string) processed_data . 0 [neg, review, product, amazon, elat, receiv, i... 1 [expens, qualiti, osum] 2 [iphon, design, good, camera, qualiti, awesom,... 3 [awesom, phone, nice, upgrad, iphon, iphon, lo... 4 [phone, produc, heat, didn’t, sim, half, hour,... ... 995 [] 996 [] 997 [excel] 998 [excel] 999 [flawless] Name: review, Length: 1000, dtype: object . Preparign Document-Term-Matrix . Gensim requires that tokens be converted to a dictionary. In this instance a dictionary is a mapping between words and their integer IDs. We then create a Document-Term-Matrix where we use Bag-of-Words approach returning the vector of word and its frequency (number of occurences in the document) for each document. . # Importing Gensim import gensim from gensim import corpora # Creating the term dictionary of our list of documents (corpus), where every unique term is assigned an index. dictionary = corpora.Dictionary(processed_data) # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. doc_term_matrix = [dictionary.doc2bow(doc) for doc in processed_data] . Running LDA Model . We will now run the LDA Model. The number of topics you give is largely a guess/arbitrary. The model assumes the document contains that many topics. However, finding the number of topics explaining the data is a optimisation problem and can be found by &#39;Coherence Model&#39;. . Here, we have used number of topics = 3 . #RUN THE MODEL # Creating the object for LDA model using gensim library Lda = gensim.models.ldamodel.LdaModel # Running and Trainign LDA model on the document term matrix. TOPIC_CNT= 3 ldamodel = Lda(doc_term_matrix, num_topics=TOPIC_CNT, id2word = dictionary, passes=50) . We can then see the weights of top 20 words in each topic, which can help us to explain the topic. . #Results topics= ldamodel.print_topics(num_topics=TOPIC_CNT, num_words=20) topics . [(0, &#39;0.066*&#34;phone&#34; + 0.047*&#34;good&#34; + 0.017*&#34;appl&#34; + 0.015*&#34;best&#34; + 0.014*&#34;charger&#34; + 0.010*&#34;time&#34; + 0.010*&#34;work&#34; + 0.010*&#34;issu&#34; + 0.009*&#34;charg&#34; + 0.008*&#34;iphon&#34; + 0.008*&#34;camera&#34; + 0.007*&#34;screen&#34; + 0.007*&#34;get&#34; + 0.007*&#34;purchas&#34; + 0.006*&#34;mobil&#34; + 0.006*&#34;android&#34; + 0.006*&#34;batteri&#34; + 0.006*&#34;heat&#34; + 0.006*&#34;us&#34; + 0.006*&#34;like&#34;&#39;), (1, &#39;0.043*&#34;iphon&#34; + 0.038*&#34;batteri&#34; + 0.037*&#34;camera&#34; + 0.036*&#34;phone&#34; + 0.026*&#34;awesom&#34; + 0.026*&#34;good&#34; + 0.024*&#34;best&#34; + 0.024*&#34;qualiti&#34; + 0.024*&#34;life&#34; + 0.023*&#34;great&#34; + 0.011*&#34;displai&#34; + 0.010*&#34;amaz&#34; + 0.010*&#34;upgrad&#34; + 0.009*&#34;perform&#34; + 0.009*&#34;love&#34; + 0.008*&#34;superb&#34; + 0.008*&#34;bui&#34; + 0.008*&#34;dai&#34; + 0.007*&#34;face&#34; + 0.006*&#34;better&#34;&#39;), (2, &#39;0.064*&#34;product&#34; + 0.030*&#34;amazon&#34; + 0.028*&#34;appl&#34; + 0.023*&#34;nice&#34; + 0.015*&#34;phone&#34; + 0.015*&#34;bui&#34; + 0.015*&#34;monei&#34; + 0.013*&#34;excel&#34; + 0.011*&#34;deliveri&#34; + 0.010*&#34;got&#34; + 0.010*&#34;time&#34; + 0.010*&#34;iphon&#34; + 0.009*&#34;devic&#34; + 0.009*&#34;perfect&#34; + 0.009*&#34;worth&#34; + 0.009*&#34;valu&#34; + 0.009*&#34;thank&#34; + 0.009*&#34;happi&#34; + 0.008*&#34;receiv&#34; + 0.007*&#34;good&#34;&#39;)] . Extracting Topics . We can identify the follow topics emerging out of reviews of Amazon iPhone 11 64GB on Amazon: . Topic #1: There seems to discussion of heat/ charging issue with the product. | Topic #2: The discussion on iPhone&#39;s features such as camera, display, battery. | Topic #3: iPhone being value for money and discussuin on Amazon delivery. | . word_dict = {}; for i in range(TOPIC_CNT): words = ldamodel.show_topic(i, topn = 20) word_dict[&#39;Topic #&#39; + &#39;{:2d}&#39;.format(i+1)] = [i[0] for i in words] pd.DataFrame(word_dict) . Topic # 1 Topic # 2 Topic # 3 . 0 | phone | iphon | product | . 1 | good | batteri | amazon | . 2 | appl | camera | appl | . 3 | best | phone | nice | . 4 | charger | awesom | phone | . 5 | time | good | bui | . 6 | work | best | monei | . 7 | issu | qualiti | excel | . 8 | charg | life | deliveri | . 9 | iphon | great | got | . 10 | camera | displai | time | . 11 | screen | amaz | iphon | . 12 | get | upgrad | devic | . 13 | purchas | perform | perfect | . 14 | mobil | love | worth | . 15 | android | superb | valu | . 16 | batteri | bui | thank | . 17 | heat | dai | happi | . 18 | us | face | receiv | . 19 | like | better | good | . The below code provide the % of topics a document is about. This helps to find the dominant topic in each review. . doc_to_topic = [] for i in range(len(doc_term_matrix)): top_topics = ldamodel.get_document_topics(doc_term_matrix[i], minimum_probability=0.0) topic_vec = [top_topics[j][1] for j in range(TOPIC_CNT)] doc_to_topic.append(topic_vec) . #Dataframe of topic document_topics = pd.DataFrame(doc_to_topic) document_topics = document_topics.rename(columns=lambda x: x + 1) document_topics.columns = document_topics.columns.astype(str) document_topics = document_topics.rename(columns=lambda x: &#39;Topic #&#39; + x) . #Dataframe of review and topics data_new = pd.concat([data,document_topics],axis=1,join=&#39;inner&#39;) data_new.head() . review Topic #1 Topic #2 Topic #3 . 0 | May be my first negative review about the prod... | 0.004879 | 0.004542 | 0.990579 | . 1 | It&#39;s very expensive but the quality you get is... | 0.084246 | 0.831019 | 0.084735 | . 2 | The iPhone design is good and the camera quali... | 0.696215 | 0.133451 | 0.170334 | . 3 | Awesome Phone. Nice upgrade from iPhone 6s to ... | 0.036537 | 0.802536 | 0.160927 | . 4 | My Phone is Producing Too Much Heat Even Didn’... | 0.567121 | 0.010595 | 0.422285 | . Visualization of LDA Model . There is a nice way to visualize the LDA mode using the package pyLDAvis. This visualization allows us to compare topics on two reduced dimensions and observe the distribution of words in topics. The size of the bubble measures the importance of the topics, relative to the data. . import pyLDAvis.gensim lda_display = pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary, sort_topics=False) pyLDAvis.display(lda_display) . C: Users 61920959 AppData Local Continuum anaconda3 lib site-packages pyLDAvis _prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version of pandas will change to not sort by default. To accept the future behavior, pass &#39;sort=False&#39;. To retain the current behavior and silence the warning, pass &#39;sort=True&#39;. return pd.concat([default_term_info] + list(topic_dfs)) . Endnotes . I hope this blog helps in understanding how powerful Topic Modeling is in understanding unstructured textual data. Feel free to play around with the code by opening in Colab or cloning the repo in github. . If you have any comments or suggestions please comment below or reach out to me at - Twitter or LinkedIn .",
            "url": "https://rahuls0959.github.io/ds-blog/python/text%20analytics/web%20scraping/topic%20modeling/selenium/gensim/nlp/2020/04/19/_Topic_Modeling_Amazon_Reviews.html",
            "relUrl": "/python/text%20analytics/web%20scraping/topic%20modeling/selenium/gensim/nlp/2020/04/19/_Topic_Modeling_Amazon_Reviews.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Basic Text Analytics Tutorial on Sentiment Analysis of Tweets using Python",
            "content": "Introduction . This blog covers the basics of Text Analytics. We will be using Natural Language Processing and Machine Learning alogorithm to analyse the tweets on the Twitter platform.We will analyse the sentiments of 5000 tweets with certain hashtags and label them as Positive, Negative or Neutral. With this data, we will build a ML model using Naive Bayes Classifier which can be deployed to predict the sentiment of new tweets.We will be using the following python libraries for this analysis: . tweepy: Library for accessing the Twitter API | pandas: Library to perform various operations on the data | nltk: Library to perform Natural Language Processing for English text | sklearn: Library to perform machine learning algorithms | . Imports . tweepy: http://docs.tweepy.org/en/latest/ | pandas: https://pandas.pydata.org/docs/user_guide/index.html | . import tweepy as tw #library for accessing Twitter API import pandas as pd #data analysis API import json . Scrapping Tweets from Twitter using Tweepy . You have to first create a Twitter Developer account to gain credentials for Tweepy. This does require that you have a Twitter account. The application will ask various questions about what sort of work you want to do. Don’t fret, these details don’t have to be extensive, and the process is relatively easy. Link- https://developer.twitter.com/en | . After finishing the application, the approval process is relatively quick and shouldn’t take longer than a couple of days. Upon being approved you will need to log in and set up a dev environment in the developer dashboard and view that app’s details to retrieve your developer credentials as shown in the below picture. Unless you specifically have requested access to the other API’s offered, you will now be able to use the standard Tweepy API. | . Before getting started on Tweepy you will have to authorize that you have the credentials to utilize its API. The code snippet given is how one authorizes themself. | The search parameters I focused on are q(text query), lang(language of tweets), since(starting date o tweets), include_rts(whether to include retweets or not), items(count of tweets to be extracted). In the below code, I scrape the 5000 of tweets since 1st April 2020 that have following text &quot;#coronavirus, #COVID19, #CoronavirusOutbreak&quot;. | If you want to further customize your search you can view the rest of the search parameters available in the api.search method here- https://tweepy.readthedocs.io/en/latest/api.html#API.search | One of the advantages of querying with Tweepy is the amount of information contained in the tweet object. I have queried only tweet txt in the below code. If you’re interested in grabbing other information you can view the full list of information available in Tweepy’s tweet object here- https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object | . #Twitter App Auth consumer_key = &#39;41zvYrOHdiIKgSq7Xf5tbTyrp&#39; consumer_secret = &#39;3rOqNnBRjjqkEpGfQcmyD9kh6WgGALmTmiI6IJUgILee0Z0Uad&#39; access_key = &#39;1224090842106757120-gD25mu7R2pNTCQCozu5o9SCSpE8XHG&#39; access_secret = &#39;D63jeYIxtByfzbMvwr2SSXycTSyR4Hdm1xCxcAt3mNfY5&#39; # Initialize API auth = tw.OAuthHandler(consumer_key, consumer_secret) auth.set_access_token(access_key, access_secret) api = tw.API(auth, wait_on_rate_limit=True) # Search terms search_words = [&quot;#coronavirus&quot;, &quot;#COVID19&quot;, &quot;#CoronavirusOutbreak&quot;] date_since = &quot;2020-04-01&quot; # Collect tweets tweets = tw.Cursor(api.search, q=search_words, lang=&quot;en&quot;, since=date_since, tweet_mode=&#39;extended&#39;, include_rts=True).items(5000) tweets_arr = [] # Iterate and print tweets for tweet in tweets: tweets_arr.append(tweet.full_text) print(&quot;Done&quot;) #Creating data frame of tweets df_tweets = pd.DataFrame(tweets_arr) df_tweets . Done . 0 . 0 | RT @chidambara09: #Coronavirus: n n#European P... | . 1 | China denies cover up, but abruptly raises COV... | . 2 | RT @chidambara09: #Coronavirus: n n#European P... | . 3 | RT @pcraindia: With necessary precautions and ... | . 4 | RT @chidambara09: #Coronavirus: n n#European P... | . ... | ... | . 4995 | Vietnam sets up &#39;Rice ATMs&#39; to provide free ri... | . 4996 | Covid19 austerity: New Zealand PM sets example... | . 4997 | RT @admediainsider: &quot;This phase has made India... | . 4998 | RT @pennewstweet: Masks are mandatory in Singa... | . 4999 | RT @QuickTake: Young people are falling seriou... | . 5000 rows × 1 columns . Sentiment Analysis of the scrapped tweets . Import nltk libraries for doing sentiment analysis: https://www.nltk.org/ | . # NLP libraries import nltk from nltk.tokenize import sent_tokenize #sentence tokenization:break text into sentences from nltk.tokenize import word_tokenize #word tokenization:break sentences into words from nltk.corpus import stopwords #removal of stop words from nltk.stem import PorterStemmer #lexicon Normalisation/Stemming: retain only root form of the word . Install required packages in nltk with inbuilt sentiment analyser. VADER belongs to a type of sentiment analysis that is based on lexicons of sentiment-related words. In this approach, each of the words in the lexicon is rated as to whether it is positive or negative, and in many cases, how positive or negative. | . #Using NLTK package to conduct sentiment analysis nltk.download(&#39;vader_lexicon&#39;) . [nltk_data] Downloading package vader_lexicon to [nltk_data] C: Users 61920959 AppData Roaming nltk_data... [nltk_data] Package vader_lexicon is already up-to-date! . True . The polarity scores for each tweet are calculated using Sentiment Intensity Analyser. | The Positive, Negative and Neutral scores represent the proportion of text that falls in these categories. Hence all these should add up to 1. | The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1(most extreme negative) and +1 (most extreme positive). | . #Sentiment Scores from nltk.sentiment.vader import SentimentIntensityAnalyzer sid = SentimentIntensityAnalyzer() scores = [] for tweet in tweets_arr: score = sid.polarity_scores(tweet) scores.append(score) #Dataframe for sentiment scores df_sentiments = pd.DataFrame(scores) df_sentiments dataset = pd.concat([df_tweets, df_sentiments], axis=1, join=&#39;inner&#39;) dataset . 0 neg neu pos compound . 0 | RT @chidambara09: #Coronavirus: n n#European P... | 0.000 | 1.000 | 0.000 | 0.0000 | . 1 | China denies cover up, but abruptly raises COV... | 0.257 | 0.743 | 0.000 | -0.8047 | . 2 | RT @chidambara09: #Coronavirus: n n#European P... | 0.000 | 1.000 | 0.000 | 0.0000 | . 3 | RT @pcraindia: With necessary precautions and ... | 0.115 | 0.885 | 0.000 | -0.3818 | . 4 | RT @chidambara09: #Coronavirus: n n#European P... | 0.000 | 1.000 | 0.000 | 0.0000 | . ... | ... | ... | ... | ... | ... | . 4995 | Vietnam sets up &#39;Rice ATMs&#39; to provide free ri... | 0.000 | 0.883 | 0.117 | 0.5106 | . 4996 | Covid19 austerity: New Zealand PM sets example... | 0.059 | 0.941 | 0.000 | -0.1280 | . 4997 | RT @admediainsider: &quot;This phase has made India... | 0.174 | 0.826 | 0.000 | -0.4588 | . 4998 | RT @pennewstweet: Masks are mandatory in Singa... | 0.000 | 0.909 | 0.091 | 0.0772 | . 4999 | RT @QuickTake: Young people are falling seriou... | 0.243 | 0.757 | 0.000 | -0.6249 | . 5000 rows × 5 columns . The overall sentiment of the tweet is computed using the following: positive sentiment : (compound score &gt;= 0.05) | neutral sentiment : (compound score &gt; -0.05) and (compound score &lt; 0.05) | negative sentiment : (compound score &lt;= -0.05) | . | As we can see, sentiment of around 50% of the tweets is neutral, 27% negative and 23% positive | . # Generate overall_sentiment using pandas overall_sentiment = [] for value in dataset[&quot;compound&quot;]: if value &gt;= 0.05: overall_sentiment.append(&quot;Positive&quot;) elif value &lt;= -0.05: overall_sentiment.append(&quot;Negative&quot;) else: overall_sentiment.append(&quot;Neutral&quot;) dataset[&quot;overall_sentiment&quot;] = overall_sentiment #dropping the scores columns data = dataset.drop(columns ={&quot;neg&quot;,&quot;pos&quot;,&quot;neu&quot;,&quot;compound&quot;}) # changing column name with rename() data = data.rename(columns = {0: &quot;text&quot;}) print(data) data.groupby(&#39;overall_sentiment&#39;).size() . text overall_sentiment 0 RT @chidambara09: #Coronavirus: n n#European P... Neutral 1 China denies cover up, but abruptly raises COV... Negative 2 RT @chidambara09: #Coronavirus: n n#European P... Neutral 3 RT @pcraindia: With necessary precautions and ... Negative 4 RT @chidambara09: #Coronavirus: n n#European P... Neutral ... ... ... 4995 Vietnam sets up &#39;Rice ATMs&#39; to provide free ri... Positive 4996 Covid19 austerity: New Zealand PM sets example... Negative 4997 RT @admediainsider: &#34;This phase has made India... Negative 4998 RT @pennewstweet: Masks are mandatory in Singa... Positive 4999 RT @QuickTake: Young people are falling seriou... Negative [5000 rows x 2 columns] . overall_sentiment Negative 1356 Neutral 2475 Positive 1169 dtype: int64 . Wordcloud of the Tweet Data . Wordcloud is a powerful visualisation tool to understand what are the main words in the content or what most people are tweeting about. In the below code we have removed the following words from appearing in wordcloud: . Words starting with @ (tweet author&#39;s name) | Words starting with # (we have extracted tweets with hashtags related to coronavirus) | Words such as RT (which is coming in case it is a re-tweet) | Few interesting observations from the word clouds: . Positive wordcloud: United States,deaths reported, new cases - United States is mostly mentioned in tweets with overall positive tone. | Negative wordcloud: Europe, seriously ill, young people - Tweets are showing the illness caused by virus and young people might be most affected by this disease. Europe is mostly mentioned in tweets with overall negative tone. | Neutral wordcloud: new case, total deaths, total confirmed - These might be general tweets providing coronaviris stats. | #visualizing word clouds for positive and negative tweets from wordcloud import WordCloud,STOPWORDS import matplotlib.pyplot as plt %matplotlib inline data_pos = data[ data[&#39;overall_sentiment&#39;] == &#39;Positive&#39;] data_pos = data_pos[&#39;text&#39;] data_neg = data[ data[&#39;overall_sentiment&#39;] == &#39;Negative&#39;] data_neg = data_neg[&#39;text&#39;] data_neu = data[ data[&#39;overall_sentiment&#39;] == &#39;Neutral&#39;] data_neu = data_neu[&#39;text&#39;] def wordcloud_draw(data, color = &#39;black&#39;): words = &#39; &#39;.join(data) cleaned_word = &quot; &quot;.join([word for word in words.split() if &#39;http&#39; not in word and not word.startswith(&#39;@&#39;) and not word.startswith(&#39;#&#39;) and word != &#39;RT&#39; ]) wordcloud = WordCloud(stopwords=STOPWORDS, background_color=color, width=2500, height=2000 ).generate(cleaned_word) plt.figure(1,figsize=(9, 9)) plt.imshow(wordcloud) plt.axis(&#39;off&#39;) plt.show() print(&quot;Positive words&quot;) wordcloud_draw(data_pos,&#39;white&#39;) print(&quot;Negative words&quot;) wordcloud_draw(data_neg) print(&quot;Neutral words&quot;) wordcloud_draw(data_neu,&#39;white&#39;) . Positive words . Negative words . Neutral words . Naive Bayes Classification Model . Import sklearn library for building the classification model: https://scikit-learn.org/stable/ | . #import machine learning libraries import time from sklearn.pipeline import Pipeline from sklearn.multiclass import OneVsRestClassifier from sklearn import model_selection, naive_bayes, metrics, svm from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfTransformer . Split the 5000 tweet data into training (70%) and validation sets (30%) | . #Splitting the data in train and test split t1 = time.time() X_train, X_test, y_train, y_test = train_test_split(data[&#39;text&#39;], data[&#39;overall_sentiment&#39;], test_size=0.3,random_state = 0) t2= time.time() . We will be using CountVectorizer for performing the following data preprocessing tasks: Representing the tweet text as bag-of-words for feature extraction | Text preprocessing, tokenizing and filtering of stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors. | | . The output would be the list of words/ tokens that appear in the tweet corpus and number of ocuurences of words in each tweet. . However, this would only give the occurences of word in a document which might be not be an ideal metric since larger tweets will have more words. We will solve this by normalising using TF(Term Frequency) - IDF (Inverse Document Frequency) metric explained in this image. TfidfTransform performs this function in Python. | . count_vect = CountVectorizer(lowercase=True,stop_words=&quot;english&quot;,min_df=10) count_vect.fit(X_train) X_train_counts = count_vect.transform(X_train) X_test_counts = count_vect.transform(X_test) # Create the tf-idf representation using the bag-of-words matrix tfidf_transformer = TfidfTransformer(norm=None) tfidf_transformer.fit(X_train_counts) X_train_tfid =tfidf_transformer.transform(X_train_counts) X_test_tfid = tfidf_transformer.transform(X_test_counts) . X_train_counts.shape . (3500, 624) . X_test_counts.shape . (1500, 624) . We will be building classification model using Naive Bayes algorithm. It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature (a big assumption indeed, that is why model is called Naive). We will be using multinomial naive bayes since there are more than 2 classes (positive, negative, neutral). . | Explained simply, naive bayes algoorithm works by calculating the probabilities using following formula and giving output as the class having highest probability. . | Our model accuracy on training data is 79% and 76% on test data (which is quite good). . | . #Applying Naive Bayes Classifier from sklearn.naive_bayes import MultinomialNB nb = MultinomialNB() #fit data to NB model nb.fit(X_train_tfid, y_train) # train naive bayes on count print(&quot;Train Accuracy: &quot;, round(nb.score(X_train_tfid,y_train), 3)) print(&quot;Test Accuracy: &quot;, round(nb.score(X_test_tfid,y_test), 3)) . Train Accuracy: 0.793 Test Accuracy: 0.76 . Using this model, we can predict the sentiment of any new tweet. As given in example below, the tone of the tweet is negative and model performed a well job in predicting the same. Voila! | . #predicting the sentiment of a new tweet docs_new = [&#39;Balancing working from home and shouldering the bulk of domestic tasks leaves many women stretched to capacity, meaning less quality time with their families and for themselves.&#39;] X_new_counts = count_vect.transform(docs_new) X_new_tfidf = tfidf_transformer.transform(X_new_counts) clf = nb.fit(X_train_tfid, y_train) predicted = clf.predict(X_new_tfidf) print(predicted) . [&#39;Neutral&#39;] . If you have any comments or suggestions please comment below or reach out to me at - Twitter or LinkedIn .",
            "url": "https://rahuls0959.github.io/ds-blog/python/text%20analytics/sentiment%20analysis/wordcloud/tweepy/nlp/sklearn/2020/04/18/twitter_sentiment_analysis.html",
            "relUrl": "/python/text%20analytics/sentiment%20analysis/wordcloud/tweepy/nlp/sklearn/2020/04/18/twitter_sentiment_analysis.html",
            "date": " • Apr 18, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there! I am Rahul. I have completed my engineering from IIT Kanpur in 2015 and MBA from ISB in 2020. I believe in the power of data analytics in solving challenging business problems. . . If you want to chat about Tech, Products, or Analytics feel free to reach out to me on Twitter or Linkedin .",
          "url": "https://rahuls0959.github.io/ds-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rahuls0959.github.io/ds-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}